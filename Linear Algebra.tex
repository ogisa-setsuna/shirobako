\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[hidelinks,CJKbookmarks]{hyperref}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\graphicspath{{C:/Users/18967/Desktop/latex/figure/}}
\title{Linear Algebra}
\author{haruki}
\date{\today}
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Linear Equations}\label{1}
\subsection{What is Fields? What is Vector Spaces?}
\noindent In short, a Field $F$ should satisfy these properties:
\begin{enumerate}
	\item[(A)] Axioms for addition
	\begin{enumerate}
		\item[(A1)] if $x$,$y$ in $F$, then $x+y$ in $F$
	    \item[(A2)] Addition is commutative:\[x+y=y+x\]for all $x$ and $y$ in $F$
	    \item[(A3)] Addition is associative:\[x+(y+z)=(x+y)+z\]for all $x$,$y$ and $z$ in $F$
	    \item[(A4)] There is an element $0$ in $F$ such that $x+0=x$, for all $x$ in $F$
	    \item[(A5)] To every $x$ in $F$,there exists an element $-x$ in $F$ such that \[(-x)+x=0\]
	\end{enumerate}
	\item[(M)] Axioms for multiplication
	\begin{enumerate}
		\item[(M1)] if $x$,$y$ in $F$, then $xy$ in $F$
		\item[(M2)] Multiplication is commutative:\[xy=yx\]for all $x$ and $y$ in $F$
		\item[(M3)] Multiplication is associative:\[x(yz)=(xy)z\]for all $x$,$y$ and $z$ in $F$
		\item[(M4)] There is an element $1\neq 0$ in $F$ such that $1x=x$, for all $x$ in $F$
		\item[(M5)] To every $x\neq 0$ in $F$, there exists an element $\dfrac{1}{x}$ in $F$ such that \[(\frac{1}{x})x=1\]
	\end{enumerate}
	\item[(D)] The distributive law\[x(y+z)=xy+xz\]for all $x$,$y$ and $z$ in $F$
\end{enumerate}
\indent Some familiar propositions like cancellation law and the uniqueness of $0$ and $1$ is omitted here. See the comprehensive proofs in Baby Rudin.\\
\indent In the following context, we assume the field $F$ as a subfield of the complex numbers. Otherwise, it could be possible that $1+1+\dots =0$! You must not want to see that, which arises from the nonzero characteristic of the field. So we assume the field $F$ we discuss is a field of characteristic zero.\\
\indent Our preparation work for vector spaces has done. Let's define it!
\theoremstyle{defination}\newtheorem{dde}{Defination}[section]
\begin{dde}
	A vector space consists of the following:
	\begin{enumerate}
		\item a field $F$ for scalars
		\item a set $V$ of objects, called vectors
		\item an operation called vector addition,which satisfies the axioms for additions (A)
		\item an operation called scalar multiplication, which satisfies:
		\begin{enumerate}
			\item[(a)] $c\alpha$ is in $V$ for every $\alpha$ in $V$
			\item[(b)] $1\alpha=\alpha$
			\item[(c)] $(c_1c_2)\alpha=c_1(c_2\alpha)$
			\item[(d)] $c(\alpha+\beta)=c\alpha+c\beta$
			\item[(e)] $(c_1+c_2)\alpha=c_1\alpha+c_2\alpha$
		\end{enumerate}
	\end{enumerate}
\end{dde}
The scalar multiplication has familiar properties just like the ordinary multiplication. We will dig into this concept more deeply in the following sections.
\subsection{Systems of Linear Equations}\label{systems of le}
\noindent Of Course, the numbers we involved are the elements of a field $F$.\\
A system of linear equations is defined as:
\begin{align}
	A_{11}x_1+A_{12}x_2+\ldots+A_{1n}x_n&=y_1\notag\\
	A_{21}x_1+A_{22}x_2+\ldots+A_{2n}x_n&=y_2\notag\\
	\vdots\:\,\quad\quad\quad\vdots\quad\quad\quad\qquad\quad\vdots\quad&=\,\:\vdots\notag\\
	A_{m1}x_1+A_{m2}x_2+\ldots+A_{mn}x_n&=y_m\notag
\end{align}
\indent Obviously, there's no doubt that all the linear combinations of the equations are still linear equations, and every solution of the previous system of linear equations is also a solution of the new one.\\
\indent However, we want to transform the original system of linear equations to the simplest form, which requires that the solutions should be the same. Therefore, if the original equations are also the linear combinations of the new one, the solutions must be the same, and we've reached our purpose. The two systems of linear equations are called \textbf{equivalent}, if each equation in each system is a linear combination of the equations in the other system. And we have the following apparent theorem.\\
\theoremstyle{plain}\newtheorem{thm}{Theorem}[section]
\begin{thm}
	Equivalent systems of linear equations have exactly the same solutions.
\end{thm}
Then all we have to do is to find the suitable way of linear combinations.
\subsection{Matrices and Elementary Row Operations}
\noindent To simplify our notations, we introduce a concept:
\[A=\begin{bmatrix}
	A_{11}&A_{12}&$\ldots$&A_{1n}\\
	A_{21}&A_{22}&$\ldots$&A_{2n}\\
	\vdots&\vdots&\ddots&\vdots\\
	A_{n1}&A_{n2}&$\ldots$&A_{nn}
\end{bmatrix}\]
We called $A$ a matrix. Sometimes we use the notation $\{A_{ij}\}$ to denote it.\\
\indent By now the matrix is only a pile of numbers, so we endow it some operations:
\begin{enumerate}
	\item matrix addition:$(A+B)_{ij}=A_{ij}+B_{ij}$
	\item scalar multiplication:$(cA)_{ij}=cA_{ij}$
	\item matrix multiplication:$(AB)_{ij}=\sum\limits_{r=1}^pA_{ir}B_{rj}$,where $AB$ is a $m\times n$ matrix, and $A,B$ are $m\times p,p\times n$ matrices respectively. 
\end{enumerate}
The $n\times n$ identity matrix is denoted by $I_n$ with entries $I_{ij}$ that:
\[I_{ij}=
\begin{cases}
	1&\text{if }i=j\\
	0&\text{if }i\neq j
\end{cases}\]
which has a significant property:$IA=AI=A$.\\
\indent The first and second operations make the matrices space become a vector space, and the third operation make the matrices space become a algebra, which will simplify our notations greatly.\\
\indent An important property of matrix multiplication should be pointed out:
\theoremstyle{plain}\newtheorem{pro}{Proposition}[section]
\begin{pro}
	The matrix multiplication is associative.
\end{pro}
\begin{proof}
	The proof is direct. If $A,B,C$ are $m\times p,p\times q,q\times n$ matrices, then
	\begin{align}
		((AB)C)_{ij}&=\sum\limits_{t=1}^q(\sum\limits_{r=1}^pA_{ir}B_{rt})C_{tj}=\sum\limits_{t=1}^q\sum\limits_{r=1}^pA_{ir}B_{rt}C_{tj}\notag\\
		&=\sum\limits_{r=1}^pA_{ir}(\sum\limits_{t=1}^qB_{rt}C_{tj})=(A(BC))_{ij}\notag
	\end{align}
\end{proof}
But it isn't commutative and you can prove it directly as well.\\
\indent We have possessed the matrix, so let's come back to the system of linear equations. You could find an amazing thing we expect:
\[AX=Y\]and where
\[A=\begin{bmatrix}
	A_{11}&A_{12}&$\ldots$&A_{1n}\\
	A_{21}&A_{22}&$\ldots$&A_{2n}\\
	\vdots&\vdots&\ddots&\vdots\\
	A_{n1}&A_{n2}&$\ldots$&A_{nn}
\end{bmatrix}\]
\[X=\begin{bmatrix}
	x_1\\
	x_2\\
	\vdots\\
	x_n\\
\end{bmatrix},
Y=\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n\\
\end{bmatrix}\]This is just the same thing we discussed above!So the objects concerned will be matrices from now on.\\
\indent Don't forget the target we said at the end of the subsection \ref{systems of le}!I will show you that the way we're looking for can be presented as a convenient matrix.\\
\indent We define three elementary raw operations of matrix as following:
\begin{enumerate}
	\item multiplication of one row of $A$ by a nonzero scalar $c$
	\item replacement of the $r$th row of $A$ by row $r$ plus $c$ times row $s$, $c$ is any scalar and $r\neq s$
	\item interchange of two rows of $A$
\end{enumerate}
The elementary matrices come after:
\begin{dde}
	An elementary matrix $E_n$ is a matrix which is the consequence of an elementary raw operation on the identity matrix $I_n$. 
\end{dde}
A simple but important conclusion reached now:
\begin{pro}
	The consequence of an elementary raw operation on $m\times n$ matrix $A$ is just $e_mA$.
\end{pro}
Whose proof is direct and I'm lazy to type.\\
\indent By the definition of elementary raw operations, it's apparent that each operation has a \textbf{unique} inverse operation to cancel the effect it cause. 
\indent For a further convenience, we introduce the inverse of a matrix:
\begin{dde}
	Let $A$ be a $n\times n$ matrix. If $BA=I$,we call $B$ a \textbf{left inverse} of $A$. If $AC=I$,we call $C$ a \textbf{right inverse} of $A$. If $BA=AB=I$, we call $B$ a \textbf{two-side inverse} of $A$. You may find that:\[B=BI=BAC=IC=C\]So we use $A^{-1}$ to denote the \textbf{inverse} of $A$, and $A$ is called invertible.
\end{dde}
It's simple to prove:
\begin{thm}
\begin{enumerate}
	\item If $A$ is invertible, then $(A^{-1})^{-1}=A$
	\item If $A,B$ are invertible, then $AB$ is invertible and the inverse is $B^{-1}A^{-1}$ 
\end{enumerate}
\end{thm}
Then we can formalize the statement above:
\begin{thm}
	The elementary matrix is invertible, and its inverse is just the matrix which corresponds to the inverse operation.
\end{thm}
Let's come back to the topic again. Why we choose the elementary raw operations?Because we have the following theorem:
\begin{thm}
	Let $A^*$ be $(A,Y)$ for a system of linear equations which we call the \textbf{augmented matrix} of the system. $B^*,A^*$ are called \textbf{raw-equivalent} if they are connected by a series of elementary raw operations, then the two system are equivalent as a result.
\end{thm}
We can write $A^*,B^*$ in an equation:\[B^*=E_1E_2\dots E_nA^*\footnote{the foot scripts here don't mean the dimension}\]
and an inverse one:\[A^*=E_n^{-1}E_{n-1}^{-1}\dots E_1^{-1}B^*\]
\indent A more convenient concept comes now:
\begin{dde}
	An $m\times n$ echelon matrix $R$ is called row-reduced if:
	\begin{enumerate}
		\item[(a)] the first nonzero entry in each nonzero row of $R$ is equal to $1$
		\item[(b)] each column of $R$ which contains the leading nonzero entry of some row has all its other entries $0$.
		\item[(c)] if rows$1,2,\dots,r$ are the nonzero rows of $R$, and if the leading nonzero entry of row $i$ occurs in column $k_i$,$i=1,2,\dots,r$,then $k_1<k_2<\dots<k_r$
	\end{enumerate}
\end{dde}
For example, $I_n$ is just a row-reduced echelon matrix.\\
\indent We have a theorem below, whose proof is apparent:
\begin{thm}
	Every $m\times n$ matrix over the field $F$ is row-equivalent to a row-reduced echelon matrix.
\end{thm}
And we have done the most of all things of systems of linear equations. You may find that if we use the elementary raw operations on the augmented matrix $A^*$ to make it become row-reduced echelon matrix $R$, then we get a explicit solution,because from the definition, we have $x_{k_1},x_{k_2},\dots\footnote{the foot scripts label the first nonzero entry of each nonzero row}$ as linear combinations of the rest $x_m$, which by arbitrary values of $x_m$ becomes a solution.\\
\indent Of course we forgot the situation of no solutions, which can be simply demonstrated by the statement:
\begin{pro}
	Let $R$ be the raw-reduced echelon augmented matrix of a system of linear equations, then:
	\begin{itemize}
		\item If there's a first nonzero entry in the last column, there's no solutions
		\item If not the above condition, the solution is unique if except the last column $R$ is $I_n$,otherwise there are infinite number of solutions. 
	\end{itemize} 
\end{pro} 
If you are smart enough, you would find that the uniqueness of solutions means the coefficient matrix $A$ is an invertible matrix,because:
\[E_1E_2\dots E_nA=I_n\]
implies the inverse of $A$ is:
\[A^{-1}=E_n^{-1}E_{n-1}^{-1}\dots E_1^{-1}I_n\]
and the procedure can be taken by hand if we write them together $(A,I_n)$,and use the elementary raw operations till $A$ becomes $I_n$, then the right is just $A^{-1}$.We can go further with this, if you notice that for an invertible matrix $A$, the solution of the systems of linear equations with coefficient matrix $A$ is unique. Then we have a theorem
\begin{thm}\label{determinantandinvertible}
	A square matrix $A$ is invertible if and only if the solution of the systems of linear equations with coefficient matrix $A$ is unique. Moreover, if there exits a square matrix $B$ such that $BA=I$ or $AB=I$, then $A$ is invertible and $B=A^{-1}$. 
\end{thm}
\begin{proof}
	We shall prove the last statement. If $BA=I$, then $AX=Y$ means $X=BY$ that has a unique solution, which implies $B=A^{-1}$.\\Similarly, $AB=I$ means $A=B^{-1}$, and then $B=A^{-1}$.
\end{proof}
\indent The last thing about systems of linear equations you need to know is that the solutions of a system of linear equations is a vector space. Let the rest $x_m$ be a series of values:\[(1,0,\dots,0),(0,1,\dots,0),\dots(0,0,\dots,1)\]and a series of solutions come after. You may notice that each solutions of the system is a linear combination of the $n-r$ solutions, which implies the solutions of the system is a vector space that has a dimension of $n-r$, which we will detail in the next section.\\
\indent We finish this section by some properties of matrices:
\begin{thm}
	\begin{enumerate}
		\item Write $B$ as $(\beta_1,\beta_2,\dots,\beta_n)$, then $AB=(A\beta_1,A\beta_2,\dots,A\beta_n)$
		\item $A$ is called upper triangle matrix if it only have nonzero entries above the diagonal(contain the diagonal), and the product of two upper triangle matrices is still a upper triangle matrix
		\item It will be useful if you notice that $AE_n$ corresponds to an elementary column operation, which is just a copy version of the row one. 
	\end{enumerate}
\end{thm}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{reimu}
\end{figure}
\newpage
\section{Vector Spaces}\label{2}
\subsection{Subspaces,Basis and Dimension}
For a further discussion about linear algebra,we will introduce some useful concepts:
\begin{dde}
	Let $V$ be a vector space over the field $F$.$W$ is a subset of $V$.$W$ is a subspace of $V$ if $W$ is a vector space with vector addition and scalar multiplication on $V$.
\end{dde}
A simple method to check $W$ is make an arbitrary linear combination by two arbitrary vectors in $W$,which means:
\begin{thm}
	If $\alpha$ and $\beta$ are vectors in $W$,then $W$ is a subspace if and only if $c\alpha+\beta$ in $W$ for each pair of $\alpha,\beta$ and each scalar $c$.
\end{thm}
We will introduce the concept ``span" in a new way:
\begin{thm}
	The intersection of any collections of subspaces is also a subspaces.
\end{thm}
The proof is easy,so let's skip it.
\begin{dde}
	Let $S$ be a set of vectors of $V$.The \textbf{subspace spanned} by $S$ is the intersection $W$ of all subspaces that contains $S$.If $S$ is a finite set,then we call $W$ the \textbf{subspace spanned by the vectors} $\{\alpha_1,\alpha_2,\dots,\alpha_n\}$. 
\end{dde}
A more usual form comes form this:
\begin{thm}
	The subspace spanned by a non-empty subset $S$ of a vector space $V$ is the set of all linear combinations of vectors in $S$. 
\end{thm}
There's an operation of addition defined on the subsets:
\begin{dde}
	If $S_1,S_2,\dots,S_k$ is subsets of a vector space $V$,the set of all sums
	\[\alpha_1,\alpha_2,\dots,\alpha_k\]of vectors $\alpha_i$ in $S_i$ is called the \textbf{sum} of the subsets $S_1,S_2,\dots,S_k$ and is denoted by
	\[S_1+S_2+\dots+S_k\]If $S_1,S_2,\dots,S_k$ are subspaces,then the sum of them is also a subspace.
\end{dde}
A further exploration need the concept ``linear dependent":
\begin{dde}
	A subset $S$ of a vector space $V$ is called \textbf{linearly dependent} if there exits distinct vectors $\alpha_1,\alpha_2,\dots,\alpha_n$ in $S$ and scalars $c_1,c_2,\dots,c_n$ in F,not all of which are $0$,such that
	\[c_1\alpha_1+c_2\alpha_2+\dots+c_n\alpha_n=0\]
	A set which is not linear dependent is called \textbf{linearly independent}.If $S$ is only a finite set of vectors,we call these vectors \textbf{dependent} or \textbf{independent} rather than call $S$.
\end{dde}
An important remark is that if $S$ is an infinite set,the concept above didn't mean \textbf{any} infinite sum of vectors.We will see this in the following concept.
\begin{dde}
	A \textbf{basis} for $V$ is a linearly independent set of vectors in $V$,which span the $V$.The \textbf{dimension} of $V$ is the number of elements in the basis.It's easy to see that the linear combination of each vector is unique
\end{dde}
You may notice a theorem is a must to make the definition legitimate.
\theoremstyle{plain}\newtheorem{lem}{Lemma}[section]
\begin{lem}\label{lemma1}
	Let $\alpha_1,\alpha_2,\dots,\alpha_k$ be independent vectors.A vector $\beta$ is a linear combination of $\alpha_1,\alpha_2,\dots,\alpha_k$ if and only if $\beta,\alpha_1,\alpha_2,\dots,\alpha_k$ are dependent vectors.
\end{lem}
\begin{lem}\label{lemma2}
	If a nonzero vector $\beta$ is a linear combination of independent vectors $\alpha_1,\alpha_2,\dots,\alpha_k$,there exits $i$ such that $\beta,\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$ are independent vectors.
\end{lem}
\begin{proof}
	Because $\beta$ is nonzero,there exits $i$ such that the coefficient of $\alpha_i$ is nonzero,which means $\alpha_i$ is a linear combination of $\beta,\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$.\\
	If $\beta,\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$ are dependent,the coefficient of $\beta$ should be nonzero otherwise $\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$ are dependent,which means $\alpha_i$ is a linear combination of $\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$,and by the Lemma \ref{lemma1},they are dependent.\\Therefore the hypothesis is incorrect.
\end{proof}
\begin{thm}\label{repthm}
	If $\beta_1,\beta_2,\dots,\beta_m$ are independent vectors so as $\alpha_1,\alpha_2,\dots,\alpha_n$,and each of them is a linear combination of $\alpha_1,\alpha_2,\dots,\alpha_n$,then they can replace some of the $\alpha$ to form a new independent vector set.
\end{thm}
\begin{proof}
	We notice that repeat the procedure in Lemma \ref{lemma2} we can change the original set by the new set,and by the independence of $\beta$ we assure the nonzero coefficients happen on $\alpha$,then the proof is over.
\end{proof}
From this theorem,We can prove lots of corollaries.
\theoremstyle{plain}\newtheorem{coro}{Corollary}[section]
\begin{coro}
	If every element of a finite set of independent vectors is a linear combination of the other,then the number of elements of the former $m$ is no more than the one of the latter $n$.  
\end{coro}
\begin{coro}
	The number of every finite basis is equal.
\end{coro}
\begin{coro}
	If there exits an infinite basis,then there's no finite basis.
\end{coro}
Hence definition of dimension is legitimate now.\\
\indent A simple example of basis is below.
Let $V$ be $F^n$\footnote{This means n-triples on field $F$,whose operations defined as usual.},then
\[e_i=(0,0,\dots,1,\dots,0)\qquad\text{The $i$th value is $1$}\]
is a basis.It's natural to popularize this to infinite-dimensional space $F^{\infty}$,but it's wrong.Do you remember the remark above?Yes,the \textbf{``span"} should be \textbf{``finite"} as the \textbf{``finite"} requirement appears in the definition of linear combination!So it's impossible to express $(1,1,\dots)$ by the method above.To ``span" this vector space,we need \textbf{orthogonal basis} rather than usual basis,which appears in functional analysis.\\
\indent How can we form a basis?A useful way to do that is extend a basis of a subspace $W$.The feasibility of this method is assured by Theorem \ref{repthm},which says every basis of $V$ can be replaced partly by a basis of $W$.There's a further theorem:
\begin{thm}
	Let $W_1,W_2$ be finite-dimensional subspaces of $V$,then
	\[\dim W_1+\dim W_2 =\dim(W_1\cap W_2)+\dim(W_1+W_2)\]
\end{thm} 
\begin{proof}
	We can extend the basis of $W_1\cap W_2$ in $W_1,W_2$ respectively,which forms a basis of $W_1,W_2$ and can be proved easily.
\end{proof}
Possessing these theorems,we can learn the topics about ``coordinate" and ``rank",which will be explained in the next subsection.
\subsection{Coordinate and Rank}
\noindent For convenience,the vector space we discuss below is finite-dimensional.
\indent To describe the ``coordinate",we need a basis ordered.having an ordered basis we can define ``coordinate" as following:
\begin{dde}
	A vector $\alpha$ in $V$ can be uniquely presented as a linear combination of an ordered basis $\{\alpha_i\}$,and the coefficient $x_i$ of each $\alpha_i$ is called the $i$th \textbf{coordinate} of $\alpha$ related to the ordered basis $\{\alpha_i\}$.We can write the coordinates in a matrix called \textbf{coordinate matrix}
	\[X=\begin{bmatrix}
		x_1\\
		\vdots\\
		x_n
	\end{bmatrix}\]   
\end{dde}
To do something about coordinate further,we need a theorem about matrix.You may notice we develop the properties of matrix at the time we need it.
\begin{thm}\label{invertindepend}
	Write the square matrix $A$ as column vectors $(\alpha_1,\alpha_2,\dots,\alpha_n)$.$A$ is invertible if and only if $\alpha_1,\alpha_2,\dots,\alpha_n$ are linearly independent. 
\end{thm}
\begin{proof}
	The proof is easy if you notice the fact that a linear combination of $\alpha_1,\alpha_2,\dots,\alpha_n$ is just a product of two matrices corresponded to a system of linear equations:
	\[c_1\alpha_1+c_2\alpha_2+\dots+c_n\alpha_n=A\begin{bmatrix}
		c_1\\
		c_2\\
		\vdots\\
		c_n
	\end{bmatrix}\]
	and immediately we finish the proof.
\end{proof}
Let's come back to the topic.One thing you must notice is the term ``relate",which tells us the arbitrariness of the coordinate because of the arbitrary choice of basis.Therefore a natural question is how to change the coordinate when the basis changed.
\begin{thm}
	Let $\{\alpha_i\},\{\alpha'_i\}$ be two bases.$P$ is a matrix whose entries is
	\[\alpha_i=\sum\limits_{j=1}^nP_{ji}\alpha'_j\]Then the two coordinates are related by
	\[X'=PX\]or\[X=P^{-1}X'\]The invertibility of $P$ is guaranteed by Theorem \ref{invertindepend}.\\
	Inversely,an invertible matrix $P$ relate $\{\alpha_i\}$ to an unique basis $\{\alpha'_i\}$ that 
	\[X'=PX\]and\[X=P^{-1}X\].
\end{thm}
The proof is direct.\\
\indent The rest properties of coordinate are left to the next section.Therefore let's discuss the term ``rank".
\begin{dde}
	The \textbf{raw rank} of a matrix $A$ is the dimension of the vector space\footnote{Obviously a subspace of $F^n$} spanned by the row vectors of $A$.The \textbf{column rank} is defined similarly.
\end{dde}
\begin{thm}
	Row-equivalent matrices have the same raw space. 
\end{thm}
\begin{proof}
	Use Theorem \ref{repthm}.
\end{proof}
The similar conclusion about column-equivalent is established as well.
\begin{thm}
	The nonzero raws of raw-reduced echelon matrix $R$ is a basis of the raw space.So as column-reduced echelon matrix.
\end{thm}
If a matrix $R$ is both a raw-reduced echelon matrix and a column-reduced echelon matrix,it's direct to draw the conclusion:
\begin{thm}
	The raw rank is equal to the column rank.Hence we call them the \textbf{rank} of matrix $A$.
\end{thm}
\begin{proof}
	All we have to do is to prove that elementary raw operations do not change the column rank,which is obviously correct when we analyze the linearly independent column vectors.
\end{proof}
We can also call the finite dimension of a space spanned by a vector set the \textbf{rank} of the vector set.\\
\indent Why we introduce the rank of a matrix?You will find the answer in the next section.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{siji}
\end{figure}
\newpage
\section{Linear Transformations and Forms}\label{3}
\subsection{Linear Transformations and Matrices}
\begin{dde}
	Let $V,W$ be vector spaces over $F$.\textbf{A linear transformation} from $V$ to $W$ is a function $T$ from $V$ to $W$ such that
	\[T(c\alpha+\beta)=c(T\alpha)+T\beta\]
	for all $\alpha,\beta$ in $V$ and $c$ in $F$.
\end{dde}
Obviously,the image 
\begin{dde}
	The \textbf{null space} of $T$ is the set of all vectors $\alpha$ in $V$ such that $T\alpha=0$.\\
	If $V$ is finite-dimensional,the \textbf{rank} of $T$ is the dimension of the range of $T$,and the \textbf{nullity} of $T$ is the dimension of the null space of $T$.
\end{dde}
For convenience,we confined ourselves to finite-dimensional vector spaces.We will have an important theorem.
\begin{thm}\label{dimthm}
	rank($T$)+nullity($T$)=$\dim(T)$
\end{thm}
\begin{proof}
	Let $\{\alpha_i\}$ be an ordered basis of the null space $W$ with dimension $m$.Use theorem \ref{repthm} we can expend it to an ordered basis $\{\alpha_{i}\}$ of $V$.Now we prove $T\alpha_i,i=m+1,\dots,n$ is an ordered basis of the range of $V$.
	\[T\alpha=T(x_1\alpha_1+x_2\alpha_2+\dots+x_n\alpha_n)=x_{m+1}T\alpha_{m+1}+\dots+x_nT\alpha_n\]
	Hence they span the range.
	\[c_1T\alpha_{m+1}+\dots+c_{n-m}T\alpha_n=0\]
	\[T(c_1\alpha_{m+1}+\dots+c_{n-m}\alpha_n)=0\rightarrow c_1\alpha_{m+1}+\dots+c_{n-m}\alpha_n\in W\]
	\[c_1=c_2=\cdots=c_{n-m}=0\]
	Hence they are linearly independent.We have finished the proof.
\end{proof}
We have seen the rank in the previous section.We combine them by the theorem below. 
\begin{thm}\label{matrixrepthm}
	A linear transformation is determined uniquely by its action on an ordered basis of $V$,which is connected to a unique matrix when an ordered basis of $W$ is chosen.
\end{thm}
\begin{proof}
	Let $\{\alpha_i\}$ be an ordered basis of $V$.Let $\{\gamma_i\}$ in $W$ satisfy
	\[T\alpha_i=\gamma_i\]
	Obviously,for all $\alpha$ in $V$,we have
	\[T\alpha=\sum\limits_{i=1}^nx_i\beta_i\]
	where $\{x_i\}$ is the coordinate of $\alpha$.This determines $T$ uniquely.\\
	Let $\{\beta_j\}$ be an ordered basis of $W$,we have
	\[T\alpha=\sum\limits_{i=1}^nx_i\sum\limits_{j=1}^mA_{ji}\beta_j=\sum\limits_{j=1}^m\beta_j\sum\limits_{i=1}^nA_{ji}x_i\]
	Let $\{y_j\}$ be the coordinate of $\beta=T\alpha$,we have
	\[y_j=\sum\limits_{i=1}^nA_{ji}x_i\rightarrow Y=AX\]
	which implies that $T$ is connected to the $m\times n$ matrix $A$.
\end{proof}
Apparently,the rank of matrix is just the rank of the linear transformation,because the nullity of $T$ is just the solution space of $AX=0$ with a connection between coordinate $X$ and $\alpha$.\\
For a further discussion,we should focus on the algebra of linear transformations.
\begin{thm}
	Let $V,W,Z$ be vector spaces.$U,T$ is linear transformations from $W$ to $Z$ and from $V$ to $W$ respectively.Then the function $UT$ is a linear transformation from $V$ to $Z$. If $V$ is invertible,the inverse $T^{-1}$ is also a linear transformation from $W$ to $V$.
\end{thm}
Because the matrices representation of linear transformations,we can define the addition and scalar multiplication on the linear transformations naturally to make a vector space of linear transformations from $V$ to $W$ called $\mathcal{L}(V,W)$,whose dimension is $mn$.Obviously,the addition,scalar multiplication and function composition(a sense of multiplication) of linear transformation are corresponded to the ones of matrix.The linear transformations from $V$ to $V$ is denoted by $\mathcal{L}(V)$.We call $T$ non-singular if $T\alpha=0$ implies $\alpha=0$,which implies that the nullity of $T$ is $0$ and $T$ is one-to-one.
\begin{thm}
	$T$ is non-singular if and only if T carries each linearly independent subset of $W$ onto a linearly independent subset of $W$. 
\end{thm}
The proof is one-step.
\begin{thm}\label{invertiblelineartransformation}
	Let $\dim V=\dim W$.$T$ is invertible if and only if $T$ is non-singular or $T$ is onto.
\end{thm}
Use Theorem \ref{dimthm} and the proof is done.
\begin{dde}
	$V$ is isomorphic to $W$ if there exists an one-to-one linear transformation $T$ of $V$ onto $W$,which is called isomorphism.
\end{dde}
Obviously,isomorphism is an equivalence relation on the class of vector spaces.
\begin{thm}
	$V$ is isomorphic to $W$ if and only if they have the same dimension.
\end{thm}
\begin{proof}
	Let a linear transformation connect the ordered bases of $V,W$,then we have done the proof.
\end{proof}
This theorem tells that the matrix of an invertible $T$ is a square matrix.
\begin{thm}
	The linear transformation is invertible if and only if a matrix of it is invertible,and the matrix of it related to the same basis is just the inverse one.
\end{thm}
The proof is direct.
It's important to notice that the $A$ depends on the choice of ordered bases.You may have found that the $P$ matrix of basis transformation from $\{\alpha_i\}$ to $\{\alpha'_i\}$ is just the matrix of a linear transformation $C$ related to an ordered basis of $V$ such that
\[\alpha'_i=C\alpha_i=\sum\limits_{i=1}^nP^{-1}_{ji}\alpha_j\]
We want to use $C$ on $\alpha$.
\[C\alpha=\sum\limits_{i=1}^nx_i\alpha'_i=\sum\limits_{j=1}^n\sum\limits_{i=1}^nP^{-1}_{ji}x_i\alpha_j\rightarrow X''=P^{-1}X\]
If you look it carefully enough,you may find some difference compared to the previous statement.This denotes a different view of basis transformation.The view before is that basis is changed and vectors is unchanged,which is called passive view.The view above is that vectors is changed and basis is unchanged,which is called active view.We will use passive view below.\\
\begin{thm}
	Let $\{\alpha_i\},\{\alpha'_i\}$ be ordered bases of $V$,$T\in\mathcal{L}(V)$.$A,B$ is the matrices of $T$ related to them respectively.Then we have
	\[B=P^{-1}AP\]
	where $X=PX',\alpha'_i=C\alpha_i=\sum\limits_{i=1}^nP_{ji}\alpha_j$
\end{thm}
\begin{proof}
	\[AX=Y\]
	\[P^{-1}APP^{-1}X=P^{-1}Y\]
	\[P^{-1}APX'=BX'=Y'\]
\end{proof}
\begin{dde}
	Let $A,B$ be square matrices.$B$ is \textbf{similar} to $A$ if there exists an invertible matrix $P$ such that
	\[B=P^{-1}AP\]
\end{dde}
We have
\[C=Q^{-1}BQ=Q^{-1}P^{-1}APQ=(PQ)^{-1}A(PQ)\]
Thus,similarity is an equivalent relation on the set of $n\times n$ square matrices,which implies that $B,A$ are corresponded to the same linear transformation related to different ordered bases.
\subsection{Linear Functionals and Forms}
A linear transformation $f$ from $V$ to $F$ is called \textbf{linear functional}.We use $V^*$ to denote $\mathcal{L(V,F)}$.Obviously we have
\[\dim V^*=\dim V\]
If $\{\alpha_i\}$ is an ordered basis of $V$,we have a matrix representation of $f$
\[\begin{bmatrix}
	a_1&a_2&\dots&a_n
\end{bmatrix}\]
where $a_i=f(\alpha_i)$.\\
\indent A natural basis of $ V^*$ comes after,if we define $f_i(\alpha_j)=\delta_{ij}$,which is guaranteed by Theorem \ref{matrixrepthm}.Then we have the theorem below.
\begin{thm}
	If $V$ is a finite-dimensional vector space and has an ordered basis $\{\alpha_i\}$,then there is a unique \textbf{dual basis} $\{f_i\}$ for $V^*$ such that $f_i(\alpha_j)=\delta_{ij}$.For each linear functional $f$ on $V$ we have
	\[f=\sum\limits_{i=1}^nf(\alpha_i)f_i\]
	and for each vector $\alpha$ in $V$ we have
	\[\alpha=\sum\limits_{i=1}^nf_i(\alpha)\alpha_i\]
\end{thm}
The proof is direct.
\begin{dde}
	If $V$ is a vector space over the field $F$ and $S$ is a subset of $V$,the \textbf{annihilator} of $S$ is the set $S^0$ of linear functionals $f$ on $V$ such that $f(\alpha)=0$ for every $\alpha$ in $S$.
\end{dde}
It's clear that $S^0$ is a subspace of $V^*$.
\begin{thm}\label{dimfunc}
	Let $V$ be a finite-dimensional vector space and let $W$ be a subspace of $V$,we have
	\[\dim W+\dim W^0=\dim V\]
\end{thm}
\begin{proof}
	Let $\{\alpha_i\},i=1,2,\dots,k$ be an ordered basis of $W$ and extend it to an ordered basis of $V$.A linear functional $f$ can be written as a linear combination of the dual basis related to $\{\alpha_i\}$
	\[f=\sum\limits_{i=1}^nf(\alpha_i)f_i\]
	For the functionals in $W^0$ we have
	\[\forall \alpha \in W,f(\alpha)=0\rightarrow\sum\limits_{i=1}^kc_if(\alpha_i)=0\]
	Because $c_i$ is arbitrary,we have
	\[f(\alpha_i)=0,i=1,2,\dots,k\]
	Therefore $\{f_i\},i=k+1,k+2,\dots,n$ is an ordered basis of $W^0$ and we have
	\[\dim W+\dim W^0=\dim V\]
\end{proof}
In a vector space of dimension $n$, a subspace of dimension $n-1$ is called a \textbf{hyperspace}.Obviously a hyperspace is a null space of a linear functional.Use the theorem above we have
\begin{coro}\label{hyperinter}
	If $W$ is a $k$-dimensional subspace of an $n$-dimensional vector space $V$,then $W$ is the intersection of $n-k$ hyperspaces in $V$ 
\end{coro}
\begin{proof}
	From the proof above we learn that $W$ is the intersection of null spaces of $f_i,i=k+1,k+2,\dots,n$,then we done the proof.  
\end{proof}
\begin{coro}\label{uniqueW}
	If $W_1,W_2$ are subspaces of finite-dimensional vector space $V$,then $W_1=W_2$ if and only if $W_1^0=W_2^0$.
\end{coro} 
\begin{proof}
	Use Corollary \ref{hyperinter}
\end{proof}
Another proof of Corollary \ref{hyperinter} is that consider $f(\alpha)=0$ as a system of linear equations,the solution space is $n-k$-dimensional with rank of $k$.\\
You may ask if there exits a basis of $V$ dual to a basis of $V^*$.The answer is to consider $V^{**}$.Let $L_\alpha$ be a linear functional on $V^*$ such that
\[L_\alpha(f)=f(\alpha),\alpha\in V\]
It seems to be a connection to the vector space $V$.
\begin{thm}
	Let $V$ be a finite-dimensional vector space over the field $F$.For each vector $\alpha$ in $V$ define
	\[L_\alpha=f(\alpha),f\in V^*\]
	The mapping $\alpha\rightarrow L_\alpha$ is then an isomorphism of $V$ onto $V^{**}$
\end{thm}
\begin{proof}
	It's easy to show that the mapping is a linear transformation and non-singular.Use Theorem \ref{invertiblelineartransformation}.
\end{proof}
\begin{coro}\label{DualDual}
	Let $V$ be a finite-dimensional vector space over the field $F$.If $L$ is a linear functional on the dual space $V^*$ of $V$, then there is a unique vector $\alpha$ in $V$ such that 
	\[L(f)=f(\alpha)\]
	for every $f$ in $V^*$.
\end{coro}
\begin{coro}
	Let $V$ be a finite-dimensional vector space over the field $F$.Each basis for $V^*$ is the dual of some basis for $V$. 
\end{coro}
\begin{proof}
	Each basis for $V^*$ has a dual basis for $V^{**}$.Use Corollary \ref{DualDual},there exists a basis for $V$ such that
	\[L_i(f_j)=f_j(\alpha_i)=\delta_{ij}\]
	whose dual basis is exactly $\{f_i\}$.
\end{proof}
We usually identify $\alpha$ with $L_\alpha$ and say that $V$ is the dual space of $V^*$
\begin{thm}
	If $S$ is a subset of a finite-dimensional vector space $V$,then $(S^0)^0$ is the subspace spanned by $S$.
\end{thm}
\begin{proof}
	Let $W$ be the subspace spanned by $S$.Clearly $W^0=S^0$.$W$ is uniquely determined by $W^0$ according to Corollary \ref{uniqueW}.You may notice that for $\alpha$ in $(S^0)^0$ we have
	\[L_\alpha(f)=f(\alpha)=0\]
	which means $(W^0)^0=W$,according to Theorem \ref{dimfunc}.
\end{proof}
As for infinite-dimensional vector space,we omitted the proof.\\
\indent It's clear that our definition of hyperspace fails when $V$ is infinite-dimensional,so we need another definition.
\begin{dde}
	If $V$ is a vector space,a \textbf{hyperspace} in $V$ is a \textbf{maximal proper subspace} of $V$,which satisfies
	\begin{enumerate}
		\item $N$ is a proper subspace of $V$.
		\item if $W$ is a subspace of $V$ which contains $N$,then either $W=N$ or $W=V$.
	\end{enumerate}
\end{dde}
\begin{thm}
	If $f$ is a non-zero linear functional on the vector space $V$,then the null space of $f$ is a hyperspace of in $V$.Conversely,every hyperspace in $V$ is the null space of a (not unique) nonzero linear functional on $V$.
\end{thm}
\begin{proof}
	Clearly $N_f$ is a proper subspace of $V$.Let $W$ be a subspace of $V$ which contains $N_f$ and $W\neq N_f$.Choose the vector $\alpha\in W\setminus N_f$ and a vector $\beta$ in $V$ such that $f(\alpha),f(\beta)\neq0$,then we define
	\[c=\frac{f(\beta)}{f(\alpha)}\]
	let $\gamma=\beta-c\alpha$,we have
	\[f(\gamma)=f(\beta)-cf(\alpha)=0\]
	which means $\gamma\in N_f$.Therefore $\beta=\gamma+c\alpha\in W$,$W=V$.\\
	Let $N$ be a hyperspace in $V$.Choose a vector $\alpha\in V\setminus N$,we have
	\[\beta=\gamma+c\alpha,\gamma\in N,\beta\in V\]
	The $\gamma$ and $\alpha$ is determined uniquely by $\beta$.If we have
	\[\beta=\gamma'+c'\alpha\]
	then
	\[(c-c')\alpha=\gamma'-\gamma\]
	If $c-c'\neq0$,then $\alpha\in N$.Hence $c=c',\gamma=\gamma'$.We can define $g(\beta)=c$,which is a linear functional on $V$,and then $N$ is the null space of $g$. 
\end{proof}
\begin{lem}
	If $f,g$ are linear functionals on $V$,then $g=cf,c\in F$ if and only if $N_f\subseteq N_g$.
\end{lem}
\begin{proof}
	The `only if' part is trivial.If $f=0$,then $g=0$.Thus we assume $f\neq0$.The null space $N_f$ is a hyperspace of $V$.Choose $\alpha\in V\setminus N_f$ and define
	\[c=\frac{g(\alpha)}{f(\alpha)}\]
	\[h=g-cf\]
	If $\beta\in V$,similar to the proof above we have
	\[h(\beta)=g(\gamma+c'\alpha)-cf(\gamma+c'\alpha)=c'(g(\alpha)-cf(\alpha))=0\]
	Hence $g=cf$
\end{proof}
\begin{thm}
	Let $g,f_1,f_2,\dots,f_r$ be linear functionals on a vector space $V$ with respective null space $N,N_1,N_2,\dots,N_r$.Then $g$ is a linear combination of $f_1,\dots,f_r$ if and only if $N_1\cap N_2\cap\cdots\cap N_r\subseteq N$.
\end{thm}
\begin{proof}
	The `only if' part is trivial.Suppose the theorem holds for $r=k-1$.If we restrict $g,f_1,\dots,f_{k-1}$ on $N_k$ to be $g',f_1',\dots,f_{k-1}'$,it's clear that $N_i'=N_i\cap N_k,N'=N\cap N_k$,we have
	\[N_1\cap\dots\cap N_k=N_1'\cap\dots\cap N_{k-1}'\subseteq N'\]
	then use induction hypothesis
	\[g'=\sum\limits_{i=1}^{k-1}c_if_i'\]
	Now define $h$ such that
	\[h=g-\sum\limits_{i=1}^{k-1}f_i\]
	Then $h(\alpha)=0$ for every $\alpha\in N_k$.By the proceeding lemma we have $h=c_kf_k$,then
	\[g=\sum\limits_{i=1}^kf_i\] 
\end{proof}
Before we turn to discuss multilinear forms,let's talk something about the transpose of a linear transformation.It's easy to define the transpose of a matrix
\[A^T_{ij}=A_{ji}\]
and obviously has the properties
\begin{enumerate}
	\item $(A^T)^T=A$
	\item $(AB)^T=B^TA^T$
\end{enumerate}
\indent To introduce the transpose of a linear transformation,we need the help of dual space.
\begin{thm}
	Let $V,W$ be vector spaces over the field $F$.For each linear transformation $T$ from $V$ into $W$,there is a unique linear transformation $T^T$ from $W^*$ into $V^*$ such that
	\[(T^Tg)\alpha=g(T\alpha)\]
	for every $g$ in $W^*$ and $\alpha$ in $V$. 
\end{thm}
\begin{proof}
	\[(T^T(cg_1+g_2))(\alpha)=(cg_1+g_2)(T\alpha)=cg_1(T\alpha)+g_2(T\alpha)\]
	\[T^T(cg_1+g_2)=cT^Tg_1+T^Tg_2\]
	The uniqueness is easy to prove.
\end{proof}
We call $T^T$ be the \textbf{transpose} of $T$.$T^T$ is also called the adjoint of $T$.
\begin{thm}
	Let $V,W$ be vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$.The null space of $T^T$ is the annihilator of the range of $T$.If $V,W$ is finite-dimensional,then
	\begin{enumerate}
		\item[\romannumeral1] $rank(T^T)=rank(T)$
		\item[\romannumeral2] the range of $T^T$ is the annihilator of the null space of $T$.
	\end{enumerate}
\end{thm}
\begin{proof}
	Use the definition
	\[(T^Tg)\alpha=g(T\alpha)\]
	It's clear that the null space of $T^T$ is precisely the annihilator of the range of $T$.If $V,W$ are finite-dimensional,use Theorem \ref{dimthm} and Theorem \ref{dimfunc} we have
	\[rank(T^T)=\dim W^*-nullity(T^T)\]
	\[nullity(T^T)=\dim W-rank(T)\]
	\[\dim W=\dim W^*\rightarrow rank(T^T)=rank(T)\] 
	If $\alpha\in N_T$ then we have
	\[(T^Tg)\alpha=g(T\alpha)=0\]
	implies that the range of $T^T$ is a subset of the annihilator of $N_T$.We have
	\[rank(T^T)=rank(T)=\dim V-nullity(T)=\dim N_T^0\]
	Thus the range of $T^T$ is precisely the annihilator of the null space of $T$.
\end{proof}
We know the relation between matrices and linear transformations,thus we have
\begin{thm}
	Let $V,W$ be vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$.Let $\{\alpha_i\},\{\beta_i\}$ be ordered bases for $V,W$ and let $\{f_i\},\{g_i\}$ be the dual bases of them.Let $A$ be the matrix of $T$ relative to $\{\alpha_i\},\{\beta_i\}$ and let $B$ be the matrix of $T^T$ relative to $\{f_i\},\{g_i\}$.We have
	\[B^T=A\] 
\end{thm}
\begin{proof}
	\[B_{ji}=(T^Tg_i)\alpha_j=g_i(T\alpha_j)=A_{ij}\]
\end{proof}
Use this we can prove a theorem which we have proven by a different method before.
\begin{thm}
	The raw rank is equal to the column rank.
\end{thm}
Let's turn to the topic.
\begin{dde}
	A \textbf{multilinear function} (tensor) is a function from $V^r=V\times V\times\cdots\times V$\footnote{You can define the natural addition and scalar multiplication on $V^r$ like $R^n$} into $F$,which satisfies
	\[L(\alpha_1,\dots,c\alpha_i+\beta_i,\dots,\alpha_r)=cL(\alpha_1,\dots,\alpha_i,\dots,\alpha_r)+L(\alpha_1,\dots,\beta_i,\dots,\alpha_r)\]
	for each $i$ and every $\alpha\in V$.We have the natural definition of addition and scalar multiplication
	\[(L+M)(\alpha_1,\dots,\alpha_r)=L(\alpha_1,\dots,\alpha_r)+M(\alpha_1,\dots,\alpha_r)\]
	\[(cL)(\alpha_1,\dots,\alpha_r)=cL(\alpha_1,\dots,\alpha_r)\]
	Then the collection of all multilinear functions on $V$ is a vector space denoted by $\mathcal{M}^r(V)$. 
\end{dde}
We want to construct an ordered basis of $\mathcal{M}^r(V)$,we need introduce a concept.
\begin{dde}
	The tensor product of $L,M$ is a multilinear function on $V^{r+s}$ such that
	\[L\otimes M(\alpha_1,\dots,\alpha_{r+s})=L(\alpha_1,\dots,\alpha_r)M(\alpha_{r+1},\dots,\alpha_{r+s})\]
\end{dde}
\begin{thm}
	For $L\in\mathcal{M}^r(V),M\in\mathcal{M}^s(V)$,we have
	\begin{enumerate}
		\item $(cL_1+L_2)\otimes M=cL_1\otimes M+L_2\otimes M$
		\item $L\otimes(cM_1+M_2)=cL\otimes M_1+L\otimes M_2$
		\item $L\otimes(M\otimes N)=(L\otimes M)\otimes N$
	\end{enumerate}
\end{thm}
\begin{thm}
	Let $V$ be a finite-dimensional vector space and let $\{\alpha_i\}$ be an ordered basis for $V$.Let $\{f_i\}$ be the dual basis for $V^*$.Then
	\[f_{j_1}\otimes f_{j_2}\otimes\dots\otimes f_{j_r},1\le j_1\le n,\dots,1\le j_r\le n\]
	form a basis for $\mathcal{M}^r(V)$.
\end{thm}
\begin{proof}
	\[L(\alpha)=\sum\limits_{j_1,j_2,\dots,j_r=1}^nA_{1j_1}A_{2j_2}\cdots A_{rj_r}L(\alpha_{j_1},\alpha_{j_2},\dots,\alpha_{j_r})\]
	We shall prove
	\[L=\sum\limits_{j_1,j_2,\dots,j_r=1}^nL(\alpha_{j_1},\alpha_{j_2},\dots,\alpha_{j_r})f_{j_1}\otimes f_{j_2}\otimes\dots\otimes f_{j_r}\]
	which is apparently correct.The linearly independence is easy to prove.
\end{proof}
\begin{dde}
	Let $L$ be a multilinear function on $V^r$.We say that $L$ is alternating if $L(\alpha_1,\dots,\alpha_r)=0$ whenever $\alpha_i=\alpha_j,|i-j|=1$.
\end{dde}
\begin{thm}
	An alternating multilinear function has the properties:
	\begin{enumerate}
		\item $L(\alpha_1,\dots,\alpha_r)=0$ whenever $\alpha_i=\alpha_j,i\neq j$
		\item $L(\alpha_1,\dots,\alpha_i,\dots,\alpha_j,\dots,\alpha_r)=-L(\alpha_1,\dots,\alpha_j,\dots,\alpha_i,\dots,\alpha_r)$
	\end{enumerate}
\end{thm}
\begin{proof}
	Let $i<j$.If $j=i+1$,we have
	\begin{align}
		L(\alpha_1,\dots,\alpha_i+\alpha_j,\alpha_i+\alpha_j,\dots,\alpha_r)&=L(\alpha_1,\dots,\alpha_i,\alpha_i,\dots,\alpha_r)+L(\alpha_1,\dots,\alpha_i,\alpha_j,\dots,\alpha_r)\notag\\
		&+L(\alpha_1,\dots,\alpha_j,\alpha_i,\dots,\alpha_r)+L(\alpha_1,\dots,\alpha_j,\alpha_j,\dots,\alpha_r)=0\notag
	\end{align}
	\[L(\alpha_1,\dots,\alpha_i,\alpha_j,\dots,\alpha_r)=-L(\alpha_1,\dots,\alpha_j,\alpha_i,\dots,\alpha_r)\]
	If $j>i+1$,we need commute one-by-one.Suppose $k=j-i$,then we need commute $2k-1$ times.Thus we have
	\begin{align}
		L(\alpha_1,\dots,\alpha_i,\dots,\alpha_j,\dots,\alpha_r)&=(-1)^{2k-1}L(\alpha_1,\dots,\alpha_j,\dots,\alpha_i,\dots,\alpha_r)\notag\\
		&=-L(\alpha_1,\dots,\alpha_j,\dots,\alpha_i,\dots,\alpha_r)\notag
	\end{align}
	If $\alpha_i=\alpha_j,i\neq j$,we commute to make them adjacent,then we have \[L(A)=-L(B)=0\]
\end{proof}
\begin{dde}
	For an ordered $n$-tuple $a_1a_2\cdots a_n$,each pair of number forms an ordered pair.Let $j=i+1$,if $a_i>a_i$,then we call this pair an \textbf{inversion}.The number of all inversion is called the \textbf{inversion number} of the $n$-tuple.
\end{dde}
\begin{dde}
	A permutation is called an \textbf{odd permutation} if its inversion number is odd,and is called an \textbf{even permutation} if its inversion number is even.
\end{dde}
\begin{thm}
	Transposition change the parity of a permutation.
\end{thm}
The proof is easy so we omitted it.
\begin{thm}
	A permutation is a product of transpositions.
\end{thm}
\begin{proof}
	We can use transposition to change a permutation to $123\cdots n$,and then the inverse is also a product of transpositions. 
\end{proof}
Thus,The parity of the number of transpositions you need to change a permutation to $123\cdots n$ is unique,because the parity of a permutation is unique.Therefore we can define the signal of permutations.
\begin{dde}
	The signal of a permutation is $1$,if it's an even permutation,and is $-1$,if it's an odd permutation.
\end{dde}
This also tells us that $L(\alpha_{\sigma(1)},\alpha_{\sigma(2)},\dots,\alpha_{\sigma(r)})=sgn(\sigma)L(\alpha_1,\dots,\alpha_r)$,and the definition about alternating is legitimate because of the uniqueness of the parity of the number of transpositions the permutation has.\\
\begin{dde}
	A multilinear function $L$ on $V^r$ is called a linear $r$-form if it is alternating.
\end{dde}
The collection of all $r$-forms is obviously a subspace of $\mathcal{M}^r(V)$,we denote it by $\bigwedge^r(V)$.
\begin{thm}
	If $L$ is a non-zero $r$-form,then $r\le n$.The dimension of $\bigwedge^r(V),1\le r\le n$ is $\dbinom{n}{r}$.
\end{thm}
\begin{proof}
	There exist $\dbinom{n}{r}$ $r$-shuffle,which is a $r$-tuple $j_1j_2\cdots j_r$ with $j_1<j_2<\cdots<j_r$.For the coordinate of $f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}$\footnote{We only consider permutations because the alternating property},we have
	\[L(\alpha_{j_{\sigma(1)}},\dots,\alpha_{j_{\sigma(r)}})=sgn(\sigma)L(\alpha_{j_1},\dots,\alpha_{j_r})\]
	and therefore we have
	\[L=\sum\limits_{shuffles-J}L(\alpha_{j_1},\dots,\alpha_{j_r})D_J\]
	\[D_J=\sum\limits_\sigma sgn(\sigma)f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\]
	$D_J$ are obviously linearly independent,hence forms a basis of $\bigwedge^r(V)$.We have
	\[L=\sum\limits_Jc_JD_J\]
\end{proof}
\begin{dde}
	$\pi_r$ is a linear transformation from $\mathcal{M}^r(V)$ into $\bigwedge^r(V)$ such that
	\[\pi_r(L)=\sum\limits_\sigma sgn(\sigma)L_\sigma\]
	\[L_\sigma(\alpha_1,\dots,\alpha_r)=L(\alpha_{\sigma(1)},\dots,\alpha_{\sigma(r)})\]
	If $L\in\bigwedge^r(V)$,$\pi_r(L)=r!L$
\end{dde}
We can write $D_J$ as $\pi_r(f_{j_1}\otimes\cdots\otimes f_{j_r})$\footnote{You may notice that the existence of alternating multilinear functions is proven when we construct the $D_J$}.The proof is easy.
\begin{coro}
	If $V$ is a $n$-dimensional vector space,then $\bigwedge^n(V)$ is a vector space with dimension $1$.If $T$ is a linear transformation on $V$,then there exists a unique $c$ such that
	\[L(T\alpha_1,\dots,T\alpha_n)=cL(\alpha_1,\dots,\alpha_n)\]
	for all $L\in\bigwedge^n(V)$
\end{coro}
\begin{proof}
	Apparently,define $L_T(\alpha_1,\dots,\alpha_n)=L(T\alpha_1,\dots,T\alpha_n)$ and $L_T$ is a $n$-form.For a basis $M$,we have $L=aM$ and $M_T=cM$,then
	\[L_T=(aM)_T=aM_T=c(aM)=cL\]
	You may notice that $D_J$ here is just $\pi_n(f_{1}\otimes\cdots\otimes f_{n})$,and we have
	\[c=\sum\limits_\sigma sgn(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)}\]
	where $A$ is the matrix of $T$ related to the basis $\{\alpha_i\}$.We call $c$ the \textbf{determinant} of the transformation $T$ and denote it as $\det T$.It's clear that the matrix determinant comes after,and is independent of the basis.
\end{proof}
\begin{thm}
	The determinant has some important properties:
	\begin{enumerate}
		\item $\det AB=\det A\det B$
		\item The determinant of upper-triangle matrix is the product of diagonal elements $a_{11}\cdots a_{nn}$
		\item $\det A^T=\det A$
	\end{enumerate}
\end{thm}
\begin{proof}
	The last two statements are easy.We prove the first.
	\begin{align}
		\det AB&=\sum\limits_{\sigma}sgn(\sigma)(\sum\limits_{k_1=1}^na_{1k_1}b_{k_1\sigma(1)})\cdots(\sum\limits_{k_n=1}^na_{nk_n}b_{k_n\sigma(n)})\notag\\
		&=\sum\limits_{\sigma}\sum\limits_{k_1=1}^n\cdots\sum\limits_{k_n=1}^nsgn(\sigma)a_{1k_1}\cdots a_{nk_n}b_{k_1\sigma(1)}\cdots b_{k_n\sigma(n)}\notag
	\end{align}
	Every term has $k_i=k_j$ cancels because of $sgn(\sigma)$,thus $k_1,k_2,\dots,k_n=\tau(1),\dots,\tau(n)$.
	\begin{align}
		\det AB&=\sum\limits_{\sigma}\sum\limits_{k_1=1}^n\cdots\sum\limits_{k_n=1}^nsgn(\sigma)a_{1k_1}\cdots a_{nk_n}b_{k_1\sigma(1)}\cdots b_{k_n\sigma(n)}\notag\\
		&=\sum\limits_{\sigma}\sum\limits_{\tau}sgn(\sigma)a_{1\tau(1)}\cdots a_{n\tau(n)}b_{\sigma^{-1}\tau(1)1}\cdots b_{\sigma^{-1}\tau(n)n}\notag\\
		&=\sum\limits_{\sigma}\sum\limits_{\tau}sgn(\sigma^{-1}\tau)sgn(\tau)a_{1\tau(1)}\cdots a_{n\tau(n)}b_{\sigma^{-1}\tau(1)1}\cdots b_{\sigma^{-1}\tau(n)n}\notag\\
		&=\det A\det B\notag
	\end{align}
\end{proof}
\begin{thm}
	Laplace Expansion
	\[\det A=\sum\limits_{j=1}^n(-1)^{i+j}a_{ij}\det A(i|j)=\sum\limits_{i=1}^n(-1)^{i+j}a_{ij}\det A(i|j)\]
	where $(-1)^{i+j}\det A(i|j)$ is usually called the $i,j$ \textbf{cofactor} of $A$.If we set
	\[C_{ij}=(-1)^{i+j}\det A (i|j)\]
	then we have
	\[\det A=\sum\limits_{j=1}^na_{ij}C_{ij}=\sum\limits_{i=1}^na_{ij}C_{ij}\]
	where the cofactor $C_{ij}$ is $(-1)^{i+j}$ times the determinant of the $(n-1)\times(n-1)$ matrix obtained by deleting the $i$th row and $j$th column of $A$.\\
	we also have
	\[\sum\limits_{j=1}a_{ij}C_{kj}=\delta_{ik}\det A\]  
	\[\sum\limits_{i=1}a_{ij}C_{ik}=\delta_{jk}\det A\]
	If we define the classical adjoint $adj\,A$ of $A$ such that
	\[(adj\,A)_{ij}=C_{ji}\]
	then we have
	\[(adj\,A)A=A(adj\,A)=(\det A)I\]
	and an explicit expression of $A^{-1}$
	\[A^{-1}=\frac{1}{\det A}adj\,A\]
\end{thm}
\begin{proof}
	\begin{align*}
		\det A&=\sum\limits_{\sigma}sgn(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)}\\
		&=\sum\limits_{j=1}^na_{ij}\sum\limits_{\sigma'}sgn(\sigma'(1)\cdots j\cdots\sigma'(n))a_{1\sigma'(1)}\cdots\widehat{a_{ij}}\cdots a_{n\sigma'(n)}\\
		&=\sum\limits_{j=1}^n(-1)^{i-1}a_{ij}\sum\limits_{\sigma'}sgn(j\sigma'(1)\cdots\sigma'(n))a_{1\sigma'(1)}\cdots\widehat{a_{ij}}\cdots a_{n\sigma'(n)}\\
		&=\sum\limits_{j=1}^n(-1)^{i-1+j-1}a_{ij}\sum\limits_{\sigma'}sgn(\sigma')a_{1\sigma'(1)}\cdots\widehat{a_{ij}}\cdots a_{n\sigma'(n)}\\
		&=\sum\limits_{j=1}^n(-1)^{i+j}a_{ij}\det A(i|j)
	\end{align*}
	The second equation can be proven similarly.If we expand the matrix by different raw or column,it's just the same expansion of a matrix with two same raws or columns,and the determinant of it is $0$. 
\end{proof}
This theorem tells us that if and only if $\det A$ is nonzero,$A$ is invertible.In fact,we can reach this conclusion if you notice that $\det A$ is nonzero if and only if $A$ is raw-equivalent with $I$,which implies a unique solution of the system of linear equations with coefficient matrix $A$.Then use Theorem \ref{determinantandinvertible} we reach the conclusion.\\
\indent A simpler proof of the theorem will use the concept we introduce below.
\begin{dde}
	We call ``$\wedge$" wedge product,and write $D_J$ as $f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r}$,which implies that
	\[f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r}=\sum\limits_{\sigma}sgn(\sigma)f_{j_{\sigma(1)}}\otimes f_{j_{\sigma(2)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\] 
\end{dde}
Recall the properties of tensor product,we want the ``wedge product" to have similar properties,namely
\[(f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r})\wedge(f_{k_1}\wedge f_{k_2}\wedge\cdots\wedge f_{k_s})=f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r}\wedge f_{k_1}\wedge f_{k_2}\wedge\cdots\wedge f_{k_s}\]
To reach this,we first expand the RHS:
\begin{align*}
	&f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r}\wedge f_{k_1}\wedge f_{k_2}\wedge\cdots\wedge f_{k_s}\\
	&=\sum\limits_{\sigma}sgn(\sigma)f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\otimes f_{j_\sigma(r+1)}\otimes\cdots\otimes f_{j_{\sigma(r+s)}}\footnotemark[8]
\end{align*}
\footnotetext[8]{For convenience,we denote $k_1,\dots,k_s$ by $j_{r+1},\dots,j_{r+s}$}
Let's divide $\sigma$ into two situation:
\begin{enumerate}
	\item $\sigma=(\sigma_1,\sigma_2)$,where $\sigma_1,\sigma_2$ are permutations of $1,\dots,r$ and $r+1,\dots,r+s$ respectively.
	\item $\sigma$ can't be presented by a product of two permutations of $1,\dots,r$ and $r+1,\dots,r+s$.
\end{enumerate}
For the first situation,we have
\begin{align*}
	&\sum\limits_{some\;\sigma}sgn(\sigma)f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\otimes f_{j_{\sigma(r+1)}}\otimes\cdots\otimes f_{j_{\sigma(r+s)}}\\
	&=\sum\limits_{some\;\sigma}sgn(\sigma_1)sgn(\sigma_2)f_{j_{\sigma_1(1)}}\otimes\cdots\otimes f_{j_{\sigma_1(r)}}\otimes f_{j_{\sigma_2(r+1)}}\otimes\cdots\otimes f_{j_{\sigma_2(r+s)}}\\
	&=f_{j_1}\wedge\cdots\wedge f_{j_r}\otimes f_{k_1}\wedge\cdots\wedge f_{k_s}
\end{align*}
You may notice that the second situation is just the first situation with a combination of $1,2,\dots,r+s$.Thus we can write
\begin{align*}
	&\sum\limits_{\sigma}sgn(\sigma)f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\otimes f_{j_{\sigma(r+1)}}\otimes\cdots\otimes f_{j_{\sigma(r+s)}}\\
	&=\sum\limits_{shuffles-J_1,J_2}f_{t_1}\wedge\cdots\wedge f_{t_r}\otimes f_{u_1}\wedge\cdots\wedge f_{u_s}
\end{align*} 
where $t_1,\dots,t_r,u_1,\dots,u_s$ are shuffles of $j_1,\dots,j_r,k_1,\dots,k_s$ respectively.\\
Like the definition of tensor product,we can define wedge product as below:
\begin{dde}
	If $L\in\bigwedge^r(V),M\in\bigwedge^s(V)$,then the wedge product of $L,M$ is
	\begin{align*}
		L\wedge M(\alpha_1,\dots,\alpha_{r+s})&=\sum\limits_{shuffles-J_1,J_2}L(\alpha_{j_1},\dots,\alpha_{j_r})M(\alpha_{k_1},\dots,\alpha_{k_s})\\
		&=\frac{1}{r!s!}\sum\limits_{\sigma}sgn(\sigma)L(\alpha_{\sigma(1)},\dots,\alpha_{\sigma(r)})M(\alpha_{\sigma(r+1)},\dots,\alpha_{\sigma(r+s)})
	\end{align*}
\end{dde}
The second equation is derived from $\pi_r(L)=r!L$.This definition is just the above one,if you notice that
\[sgn(\sigma)f_{\sigma(1)}\otimes\cdots\otimes f_{\sigma(r)}(\alpha_1,\dots,\alpha_r)=sgn(\sigma^{-1})f_1\otimes\cdots\otimes f_r(\alpha_{\sigma^{-1}(1)},\dots,\alpha_{\sigma^{-1}(r)})\]
and use the second equation to prove.\\
\indent The wedge product has some properties,which we omitted the proof
\begin{thm}
	If $L\in\bigwedge^r(V),M\in\bigwedge^s(V)$,then
	\begin{enumerate}
		\item $L\wedge M\in\bigwedge^{r+s}(V)$
		\item $(cL_1+L_2)\wedge M=cL_1\wedge M+L_2\wedge M$
		\item $L\wedge(M\wedge N)=(L\wedge M)\wedge N$
		\item $L\wedge M=(-1)^{rs}M\wedge L$
	\end{enumerate}
\end{thm}
The last one implies that
\[f_{\sigma(1)}\wedge\cdots\wedge f_{\sigma(r)}=sgn(\sigma)f_1\wedge\cdots\wedge f_r\]
\indent Having the wedge product,we can calculate determinant in a new way.
\begin{thm}
	If $A$ is a square matrix,define $\mathcal{A}$ such that
	\[\mathcal{A}=\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n\]
	where $\alpha_i=\sum\limits_{j=1}^na_{ji}f_j$.Then we have
	\[\mathcal{A}=(\det A)f_1\wedge f_2\wedge\cdots\wedge f_n\]
\end{thm}
The proof is easy.Use this expression of determinant,we can reproof previous conclusions.\\
First we prove $\det AB=\det A\det B$.Let $\{e_i\}$ be an ordered basis for $V$.Define $D\in\mathcal(M)(V^n)$ as
\[D(\beta_1,\beta_2,\dots,\beta_n)=\det (A\beta_1,A\beta_2,\dots,A\beta_n)=\det AB\]
where $\beta_i=\sum\limits_{j=1}^nb_{ji}f_j,A\beta_i=\sum\limits_{j=1}^n\sum\limits_{k=1}^na_{jk}b_{ki}f_j$.\\
Obviously,$D\in\bigwedge^n(V)$.We have
\[D=D(e_1,e_2,\dots,e_n)f_1\wedge\cdots\wedge f_n=(\det(\alpha_1,\alpha_2,\dots,\alpha_n))f_1\wedge\cdots\wedge f_n=(\det A)f_1\wedge\cdots\wedge f_n\]
where $\alpha_i$ is the column vector of $A$.Hence
\[D(\beta_1,\dots,\beta_n)=\det AB=(\det A)f_1\wedge\cdots\wedge f_n(\beta_1,\dots,\beta_n)=\det A\det B\]
Second we prove $\det A=\sum\limits_{j=1}^n(-1)^{i+j}a_{ij}\det A(i|j)=\sum\limits_{i=1}^n(-1)^{i+j}a_{ij}\det A(i|j)$.\\
\begin{align*}
	\mathcal{A}&=\alpha_1\wedge\cdots\wedge\alpha_n\\
	&=\sum\limits_{i=1}^na_{ij}\alpha_1\wedge\cdots\wedge f_i\wedge\cdots\wedge\alpha_n\\
	&=\sum\limits_{i=1}^n(-1)^{j-1}a_{ij}f_i\wedge\alpha_1\wedge\cdots\wedge\alpha_n\\
	&=\sum\limits_{i=1}^n(-1)^{j-1}a_{ij}(\det A(i|j))f_i\wedge f_1\wedge\cdots\hat{f_i}\cdots\wedge f_n\\
	&=\sum\limits_{i=1}^n(-1)^{i+j}a_{ij}(\det A(i|j))f_1\wedge\cdots\wedge f_n\\
	&=(\det A)f_1\wedge\cdots\wedge f_n
\end{align*}
The raw expansion can be proven similarly.\\
\indent Actually,we have introduced an strong concept,which can show its strength in the differential forms.\\ 
\indent Next section we will talk about polynomials to prepare the later sections.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{Rubisama1}
\end{figure}
\newpage
\section{Polynomials}\label{4}
\subsection{The Algebra of Polynomials}
\noindent It's tempting to build the polynomials by linear algebra,thus we introduce the concept.
\begin{dde}
	Let $F$ be a field.\textbf{A linear algebra over the field} $F$ is a vector space $\mathcal{A}$ over $F$ with an additional operation called \textbf{multiplication of vectors} which associates with each pair of $\alpha,\beta$ in $\mathcal{A}$ a vector $\alpha\beta$ in $\mathcal{A}$ called the \textbf{product} of $\alpha$ and $\beta$ in such a way that
	\begin{enumerate}
		\item [(a)]multiplication is associative,
		\[\alpha(\beta\gamma)=(\alpha\beta)\gamma\]
		\item [(b)]multiplication is distributive with respect to addition,
		\[\alpha(\beta+\gamma)=\alpha\beta+\alpha\gamma\text{ and }(\alpha+\beta)\gamma=\alpha\gamma+\beta\gamma\]
		\item [(c)]for each scalar $c$ in $F$,
		\[c(\alpha\beta)=(c\alpha)\beta=\alpha(c\beta)\]
	\end{enumerate} 
	If there exists an element $1$ in $\mathcal{A}$ such that $1\alpha=\alpha1=\alpha$ for each $\alpha$ in $\mathcal{A}$,we call $\mathcal{A}$ a \textbf{linear algebra with identity over} $F$,and call $1$ the \textbf{identity} of $\mathcal{A}$.The algebra $\mathcal{A}$ is called commutative if $\alpha\beta=\beta\alpha$ for all $\alpha,\beta$ in $\mathcal{A}$. 
\end{dde}
A linear algebra we concern in this section is $F^\infty$,a vector space we have mentioned in Section \ref{2}.We can write an element of $F^\infty$ by $f=(f_0,f_1,f_2,\dots)$ and define the addition and scalar multiplication as usual
\[af+bg=(af_0+bg_0,af_1+bg_1,af_2+bg_2,\dots)\]
We define a multiplication of vectors on $F^\infty$
\[(fg)_n=\sum\limits_{i=0}^nf_{i}g_{n-i}\]
Thus
\[(gf)_n=\sum\limits_{i=0}^ng_{i}f_{n-i}=\sum\limits_{i=0}^nf_{i}g_{n-i}=(fg)_n\]
\begin{align*}
	(f(gh))_n&=\sum\limits_{j=0}^nf_j\sum\limits_{i=0}^{n-j}g_ih_{n-j-i}\\
	&=\sum\limits_{j=0}^n\sum\limits_{k=0}^{n-j}f_jg_{n-j-k}h_k\\
	&=\sum\limits_{k=0}^nh_k\sum\limits_{j=0}^{n-k}f_jg_{n-j-k}\\
	&=((fg)h)_n
\end{align*}
The distributive law is apparent.We also have
\[1=(1,0,0,\dots)\]
is the identity.Therefore,$F^\infty$ with the operations defined above is a commutative linear algebra with identity over $F$.\\
we shall denote $(0,1,0,\dots)$ as $x$,and you may find
\[x^2=(0,0,1,0,\dots),x^3=(0,0,0,1,0,\dots)\]
Thus we have $(x^k)_n=\delta_{nk}$.Obviously,the set $\{1,x,x^2,\dots\}$ is linearly independent and infinite,which implies that $F^\infty$ is infinite-dimensional.One thing you should remember is that this set is \textbf{not} a basis of $F^\infty$.\\
\indent The algebra $F^\infty$ is sometimes called the \textbf{algebra of formal power series} over $F$.We shall denote $f$ as
\[f=\sum\limits_{n=0}^\infty f_nx^n\]
which is just a formally notation without anything about ``infinite sums". 
\begin{dde}
	Let $F[x]$ be the subspace of $F^\infty$ spanned by the vectors $1,x,x^2,\dots$.An element of $F[x]$ is called a \textbf{polynomials} over $F$.
\end{dde}
Since an element in $F[x]$ is a (finite) linear combination of $1,x,x^2,\dots$,we can find the maximum of $n$ with nonzero $f_n$,and call it the \textbf{degree} of $f$.We denote it by $\deg f$,and do not assign a degree to the $0$-polynomial.We can write $f$ in the manner
\[f=f_01+f_1x+\cdots+f_nx^n,f_n\neq0\]
We call $f_i$ the \textbf{coefficients} of $f$.We shall call $c1$ \textbf{scalar polynomials},and frequently denote it $c$\footnote{We will denote $1$ as $x^0$ in context}.A nonzero $f$ of degree $n$ such that $f_n=1$ is called a \textbf{monic} polynomial.
\begin{thm}
	Let $f,g$ be nonzero polynomials over $F$,then we have
	\begin{enumerate}
		\item [(\rmnum{1})]$fg$ is a nonzero polynomial
		\item [(\rmnum{2})]$\deg (fg)=\deg f+\deg g$
		\item [(\rmnum{3})]$fg$ is a monic polynomial if both $f$ and $g$ are monic polynomials
		\item [(\rmnum{4})]$fg$ is a scalar polynomial if and only if both $f$ and $g$ are scalar polynomials
		\item [(\rmnum{5})]if $f+g\neq0$,
		\[\deg (f+g)\le\max\{\deg f,\deg g\}\]
	\end{enumerate}
\end{thm}
\begin{proof}
	Let $m,n$ be degrees of $f,g$.Obviously,we have
	\[(fg)_{m+n}=f_mg_n\]
	and
	\[(fg)_{m+n+k}=0,k>0\]
	Thus (\rmnum{1}),(\rmnum{2}),(\rmnum{3}),(\rmnum{4}) follow immediately.The proof of (\rmnum{5}) is easy.
\end{proof}
\begin{coro}
	$F[x]$ is a commutative linear algebra with identity over $F$.
\end{coro}
\begin{coro}
	If $f$ is a nonzero polynomial,then $fg=fh$ implies $g=h$.
\end{coro}
We can define the \textbf{division} of two polynomials.
\begin{dde}
	Let $g$ be a nonzero polynomials.Then we define
	\[h=\frac{f}{g}\]
	if $f=hg$.The uniqueness of $h$ is apparent.
\end{dde}
You may notice that we can also write $fg$ as 
\[fg=\sum\limits_{i,j}f_ig_jx^{i+j},0\le i\le m,0\le j\le n\]
\begin{dde}
	Let $\mathcal{A}$ be a linear algebra with identity over $F$.We shall denote the identity of $\mathcal{A}$ by $1$ and make the convention that $\alpha^0=1$ for each $\alpha$ in $\mathcal{A}$.Then to each polynomial $f=\sum\limits_{i=0}^nf_ix^i$ over $F$ and $\alpha$ in $\mathcal{A}$ we associate an element $f(\alpha)$ in $\mathcal{A}$ by the rule
	\[f(\alpha)=\sum\limits_{i=0}^nf_i\alpha^i\]
\end{dde}
\begin{thm}
	Let $F$ be a field and $\mathcal{A}$ be a linear algebra with identity over $F$.Suppose $f,g$ are polynomials over $F$,$\alpha$ is an element of $\mathcal{A}$,and $c$ is a scalar,then we have
	\begin{enumerate}
		\item [(\rmnum{1})](cf+g)($\alpha$)=cf($\alpha$)+g($\alpha$)
		\item [(\rmnum{2})](fg)($\alpha$)=f($\alpha$)g($\alpha$)
	\end{enumerate}
\end{thm}
We notice that $F$ itself is a linear algebra with identity over $F$.Let $V$ be the subspace of $F[x]$ consisting of all polynomials of degree less than or equal to $n$ (together with $0$-polynomial).Obviously,$\{1,x,\dots,x^n\}$ is a basis for $V$.Let $t_0,t_1,\dots,t_n$ be elements of $F$.We define $P_i$ such that
\[P_i=\prod_{j\neq i}\frac{x-t_j}{t_i-t_j}\]
Then we have
\[P_i(t_j)=\delta_{ij}\]
Let $f=\sum\limits_{i=0}^nc_iP_i$,then
\[f(t_j)=c_j\]
Let $f$ be $0$ then we prove the linearly independence of $\{P_i\}$.Since the dimension of $V$ is $n+1$,$\{P_i\}$ is a basis for $V$ and we have
\[f=\sum\limits_{i=0}^nf(t_i)P_i\]
We can define the polynomial function $\tilde{f}$ form $F$ to $F$ by
\[\tilde{f}(x)=f(x)\]
and define similar addition and multiplication like polynomials over $F$
\[(\tilde{f}\tilde{g})(x)=\tilde{f}(x)\tilde{g}(x)\]
Apparently,we have
\[\widetilde{fg}=\tilde{f}\tilde{g}\]
and then the polynomial functions on $F$ is a linear algebra with identity over $F$.
\begin{dde}
	Let $F$ be a field and let $\mathcal{A}$ and $\tilde{\mathcal{A}}$ be linear algebras over $F$.The algebras $\mathcal{A}$ and $\tilde{\mathcal{A}}$ are said to be \textbf{isomorphic} if there is a one-to-one mapping $\alpha\rightarrow\tilde{\alpha}$ of $\mathcal{A}$ onto $\tilde{\mathcal{A}}$ such that
	\begin{enumerate}
		\item [(a)]$\widetilde{(c\alpha+d\beta)}=c\tilde{\alpha}+d\tilde{\beta}$
		\item [(b)]$\widetilde{\alpha\beta}=\tilde{\alpha}\tilde{\beta}$
	\end{enumerate}
	for all $\alpha,\beta$ in $\mathcal{A}$ and all scalars $c,d$ in $F$.The mapping $\alpha\rightarrow\tilde{\alpha}$ is called an \textbf{isomorphism} of $\mathcal{A}$ onto $\tilde{\mathcal{A}}$.
\end{dde}
\begin{thm}
	If $F$ is a field containing an infinite number of distinct elements,the mapping $f\rightarrow\tilde{f}$ is an isomorphism of the algebra of polynomials over $F$ onto the algebra of polynomial functions over $F$. 
\end{thm}
\begin{proof}
	We just need to prove the mapping is one-to-one,which implies we just need to prove $\tilde{f}=0$ means $f=0$.Suppose $f$ is a polynomial of degree $n$ or less (or $0$-polynomial),which obviously is an element of $V$.Choose different $t_i,i=1,2,\dots,n$,and let $\{P_i\}$ be a basis for $V$,then we have
	\[f(t_i)=\tilde{f}(t_i)=0,i=1,2,\dots,n\]
	Thus $f=0$ when $\tilde{f}=0$.
\end{proof}
\subsection{Polynomial Ideals}
\begin{thm}
	Suppose $f,d$ are nonzero polynomials over $F$ such that $\deg d\le\deg f$,then there exists a unique polynomial $g$ such that
	\[f-dg=0\text{ or }\deg(f-dg)<\deg d\]
	We denote $f-dg$ by $r$,so we have
	\[f=dg+r\]
\end{thm}
\begin{proof}
	First we suppose
	\[f=a_nx^n+\sum\limits_{i=0}^{n-1}a_ix^i\]
	\[d=a_mx^m+\sum\limits_{i=0}^{m-1}a_ix^i\]
	Then let $g_1$ be $\dfrac{a_n}{a_m}x^{n-m}$ we have
	\[\deg(f-dg_1)<\deg f\]
	If $\deg(f-dg_1)<\deg d$ or just zero polynomial,we have done.If not,construct $g_2$ similarly,we have
	\[\deg(f-dg_1-d_2)<\deg(f-dg_1)\]
	Obviously this process can be repeated finitely,and we have
	\[g=\sum\limits_{i=1}^kg_i\]
	If there exit another $g',r'$ such that
	\[f=dg+r=dg'+r'\]
	then we have
	\[d(g-g')=r'-r\]
	But if $g\neq g'$,$\deg d(g-g')\ge\deg d>\deg r,\deg r'$.\\
	Hence we must have $g=g',r=r'$.
\end{proof}
\begin{coro}
	$f$ is divisible by $x-c$ if and only if $f(c)=0$.
\end{coro}
\begin{dde}
	Let $F$ be a field.An element $c$ in $F$ is said to be a \textbf{root} or a \textbf{zero} of a polynomial $f$ over $F$ if $f(c)=0$.
\end{dde}
\begin{coro}
	A polynomial $f$ of degree $n$ over $F$ has at most $n$ roots in $F$. 
\end{coro}
\begin{dde}
	Let $f$ be a polynomial over $F$ such that
	\[f=a_0+a_1x+\cdots+a_nx^n\]
	The \textbf{derivative} of the polynomial is
	\[Df=a_1+2a_2x+\cdots+na_nx^{n-1}\]
\end{dde}
\begin{thm}
	Let $F$ be a field and let $c$ be an element in $F$.Let $f$ be a polynomial over $F$,then we have
	\[f=\sum\limits_{k=0}^n\frac{D^kf(c)}{k!}(x-c)^k\]
	where $n\ge\deg f$.
\end{thm}
\begin{proof}
	We notice that
	\begin{align*}
		x^m&=(x-c+c)^m\\
		&=\sum\limits_{k=0}^m\binom{m}{k}(x-c)^kc^{m-k}\\
		&=\sum\limits_{k=0}^m\frac{m!}{(m-k)!k!}(x-c)^kc^{m-k}
	\end{align*}
	\begin{align*}
		f&=\sum\limits_{m=0}^na_mx^m\\
		&=\sum\limits_{m=0}^na_m\sum\limits_{k=0}^m\frac{m!}{(m-k)!k!}(x-c)^kc^{m-k}\\
		&=\sum\limits_{k=0}^n(x-c)^k\sum\limits_{m=k}^na_m\frac{m!}{(m-k)!k!}c^{m-k}\\
		&=\sum\limits_{k=0}^n\frac{D^kf(c)}{k!}(x-c)^k
	\end{align*}
\end{proof}
Obviously $\{(x-c)^k\}$ is linearly independent and is a basis for $F[x]$.\\ 
\indent If $c$ is a root of $f$,the \textbf{multiplicity} of $c$ is the largest positive integer $r$ such that $(x-c)^r$ divides $f$.
\begin{thm}
	Let $f$ be a polynomial over $F$.Then $c$ is a root of $f$ of multiplicity $r$ if and only if
	\[D^kf(c)=0,k=0,1,\dots,r-1\]
	\[D^{r}f(c)\neq0\]  
\end{thm}
\begin{proof}
	First we have
	\[f=(x-c)^rg,g(c)\neq0\]
	Then we expand $g$
	\[f=\sum\limits_{m=0}^{n-r}\frac{D^mg(c)}{m!}(x-c)^{m+r}\]
	which means
	\[D^kf(c)=\begin{cases}
		0&0\le k\le r-1\\[8pt]
		\dfrac{D^{k-r}g(c)}{(k-r)!}&r\le k\le n
	\end{cases}\]
	And we have $g(c)\neq0$,which means $D^rf(c)\neq0$.
\end{proof}
\begin{dde}
	Let $F$ be a field.An \textbf{ideal} in $F[x]$ is a subspace $M$ of $F[x]$ such that $fg$ belongs to $M$ whenever $f$ is in $F[x]$ and $g$ is in $M$. 
\end{dde}
Apparently,$M=dF[x]$ is an ideal of $F[x]$,which is called the \textbf{principal ideal generated by} $d$.\\
\indent Let $d_1,d_2,\dots,d_n$ be a finite number of polynomials in $F[x]$.The sum $M$ of the subspaces $d_iF[x]$ is obviously an ideal,which is called the \textbf{ideal generated by} the polynomials,$d_1,d_2,\dots,d_n$.
\begin{thm}
	Let $F$ be a field and let $M$ be any nonzero ideal in $F[x]$.Then there is a unique monic polynomial $d$ in $F[x]$ such that $M$ is the principal ideal generated by $d$.
\end{thm}
\begin{proof}
	Let $d$ be a monic polynomial in $M$ which has the smallest degree.Then let $g$ be an element in $M$ we have
	\[g=dq+r\]
	Because $M$ is a subspace,then $r=g-dq$ is in $M$.Hence $r$ must be $0$,otherwise $r$ would be the polynomial with the smallest degree.Therefore we have
	\[g=dq,\forall g\in M\]
	which means $M\subseteq dF[x]$.Obviously for all $g\in M$ we have $gF[x]\subseteq M$.Thus $M$ is the principal ideal generated by $d$.\\
	If $d'$ is another monic polynomial in $M$ such that $M=d'F[x]$,then we have
	\[d'=dq=d'pq\]
	which means $p=q=1$. 
\end{proof}
\begin{coro}
	If $p_1,p_2,\dots,p_n$ are polynomials in $F[x]$,not all of which is $0$.Then there exists a unique monic polynomial $d$ such that
	\begin{enumerate}
		\item [(a)]$d$ is in the ideal generated by $p_1,\dots,p_n$
		\item [(b)]$d$ divides each of the polynomials $p_i$
	\end{enumerate}
	Any polynomials satisfying (a) and (b) necessarily satisfies
	\begin{enumerate}
		\item [(c)]$d$ is divided by every polynomial which divides each of the polynomials $p_i$
	\end{enumerate} 
\end{coro}
\begin{proof}
	According to the theorem above,the existence of $d$ is obvious.If there exists another monic polynomial $d'$ satisfies (a)(b),then we have
	\[d'=dq=d'pq\]
	which means $p=q=1,d=d'$.(c) is obviously correct from (a). 
\end{proof}
\begin{dde}
	If $p_1,p_2,\dots,p_n$ are polynomials in $F[x]$,not all of which is $0$,the monic generator $d$ of 
	\[p_1F[x]+p_2F[x]+\cdots+p_nF[x]\]
	is called the \textbf{greatest common divisor} of $p_1,\dots,p_n$.We say $p_1,\dots,p_n$ are \textbf{relatively prime} if their greatest common divisor is $1$,or equivalently if the ideal they generate is all of $F[x]$.
\end{dde}
\begin{dde}
	Let $F$ be a field.A polynomial $f$ in $F[x]$ is called \textbf{reducible over} $F$ if there exit polynomials $g,h$ in $F[x]$ of degree$\le1$ such that $f=gh$,and if not,$f$ is called \textbf{irreducible over} $F$.A non-scalar irreducible polynomial over $F$ is called a \textbf{prime polynomial over} $F$,and we sometimes say it is a \textbf{prime in} $F[x]$.  
\end{dde}
\begin{thm}
	Let $p,f,g$ be polynomials over $F$.Suppose that $p$ is a prime polynomial and that $p$ divides the product $fg$.Then either $p$ divides $f$ or $p$ divides $g$.
\end{thm}
\begin{proof}
	We can always suppose $p$ is a monic polynomial.Then the only monic divisor of $p$ is $1$ and $p$.Therefore the greatest common divisor of $f,p$ is $1$ or $p$.If $(f,p)=1$,then we have
	\[1=ff_0+pp_0\]
	which means
	\[g=fgf_0+gpp_0\]
	and then $p$ divides $g$.
\end{proof}
\begin{coro}
	If $p$ is a prime and divides the product $f_1f_2\cdots f_n$,then $p$ divides one of the polynomials $f_1,\dots,f_n$.
\end{coro}
\begin{thm}
	If $F$ is a field,a non-scalar monic polynomial in $F[x]$ can be factored as a product of monic primes in $F[x]$ in one and,except for order,only one way. 
\end{thm}
\begin{proof}
	There is nothing to do when $\deg f=1$.Suppose $\deg f=n$,we can use induction to prove it easily.If there exits two product such that
	\[p_1\cdots p_n=q_1\cdots q_m\]
	Use the corollary above we have that there exists $q_i$ such that $p_n$ divides $q_i$.Because $p_n,q_i$ are monic primes,we have
	\[p_n=q_i\]
	Rearrange them and cancel them we have
	\[p_1\cdots p_{n-1}=q_1\cdots q_{m-1}\]
	Continue this process we have done the proof.
\end{proof}
Combine the same monic polynomials we have
\[f=p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}\]
which is called the \textbf{primary decomposition} of $f$.It is easy to see that each monic divisor of $f$ has the form
\[p_1^{m_1}p_2^{m_2}\cdots p_r^{m_r},0\le m_i\le n_i\]
\begin{thm}
	Let $f$ be a non-scalar monic polynomials over $F$ and let
	\[f=p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}\]
	\[f_i=\frac{f}{p_i^{n_i}}\]
	Then $f_1,f_2,\dots,f_r$ are relatively prime.
\end{thm}
\begin{thm}
	Let $f$ be an element in $F^\infty$,define $Df$ as
	\[Df=\sum\limits_{i=1}^\infty ia_ix^{i-1}\]
	Then we have
	\[D(fg)=(Df)g+f(Dg)\]
\end{thm}
\begin{proof}
	\begin{align*}
		(D(fg))_n&=(n+1)(fg)_{n+1}\\
		&=(n+1)\sum\limits_{k=0}^{n+1}f_kg_{n+1-k}
	\end{align*}
	\begin{align*}
		((Df)g)_n&=\sum\limits_{k=0}^n(Df)_kg_{n-k}\\
		&=\sum\limits_{k=0}^n(k+1)f_{k+1}g_{n-k}\\
		&=\sum\limits_{k=1}^{n+1}kf_kg_{n+1-k}
	\end{align*}
	\begin{align*}
		(f(Dg))_n&=\sum\limits_{k=0}^nf_{k}(Dg)_{n-k}\\
		&=\sum\limits_{k=0}^n(n+1-k)f_{k}g_{n+1-k}\\
	\end{align*}
	Obviously we have
	\[(D(fg))_n=((Df)g)_n+(f(Dg))_n\]
\end{proof}
\begin{thm}
	Let $f$ be a monic polynomial over $F$ with derivative $Df$.Then $f$ is a product of distinct irreducible polynomials over $F$ if and only if $f$ and $Df$ are relatively prime. 
\end{thm}
\begin{proof}
	If some prime polynomial $p$ is repeated in the product,then $f=p^2h$ and 
	\[Df=p^2(Dh)+2p(Dp)h\]
	which means $Df$ and $f$ are not relatively prime.\\
	If $f$ is a product of distinct irreducible polynomials,then we have
	\[Df=(Dp_1)f_1+(Dp_2)f_2+\cdots+(Dp_r)f_r\]
	If there exists $p_i$ divides $f$ and $Df$,and because $p_i$ divides $f_j$ for every $j\neq i$,we have $p_i$ divides $f_i(Dp_i)$,which means $p_i$ divides $Dp_i$.But this is impossible because the degree of $Dp_i$ is less than the degree of $p_i$.
\end{proof}
\begin{dde}
	The field $F$ is called \textbf{algebraically closed} if every prime polynomials over $F$ has degree $1$.
\end{dde}
The definition means that every polynomials in $F[x]$ can be expressed in the form
\[f=c(x-c_1)^{n_1}\cdots(x-c_r)^{n_r}\]
The field $C$ of complex numbers is algebraically closed,which is called the Fundamental Theorem of Algebra,and this also implies that every prime over $R$ has degree $1$ or $2$.\\
\indent In next section we will introduce an important concept:eigenvalue.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{channel1}
\end{figure}
\section{Elementary Canonical Forms}
\subsection{Partitioned Matrix and Direct Sum of Subspaces}
In this subsection, we will introduce some concepts and prove some theorems about partitioned matrix, preparing for the following content.
\begin{dde}
	We can write a matrix as ``block forms", which is called \textbf{partitioned matrix}.
	\[A=\begin{bmatrix}
		A_{11}&A_{12}&\cdots&A_{1n}\\
		A_{21}&A_{22}&\cdots&A_{2n}\\
		\vdots&\vdots&\ddots&\vdots\\
		A_{m1}&A_{m2}&\cdots&A_{mn}
	\end{bmatrix}\]
\end{dde}
\begin{thm}
	The product of two partitioned matrices $A,B$ satisfies the normal rule of matrix product, while replace the elements with matrices.
	\[A=\begin{bmatrix}
		A_{11}&A_{12}&\cdots&A_{1n}\\
		A_{21}&A_{22}&\cdots&A_{2n}\\
		\vdots&\vdots&\ddots&\vdots\\
		A_{m1}&A_{m2}&\cdots&A_{mn}
	\end{bmatrix},B=\begin{bmatrix}
	B_{11}&B_{12}&\cdots&B_{1k}\\
	B_{21}&B_{22}&\cdots&B_{2k}\\
	\vdots&\vdots&\ddots&\vdots\\
	B_{n1}&B_{m2}&\cdots&B_{mk}
	\end{bmatrix}\]
	\[(AB)_{ij}=\sum\limits_{k=1}^nA_{ik}B_{kj}\]
\end{thm}
Apparently, each product must be legitimate. 
\begin{proof}
	First, It is obviously possible to divide $AB$ as a partitioned matrix that each block has the same size as the sums of $A_{ik}B_{kj}$.
	\begin{align*}
		((AB)_{ij})_{pq}&=\sum\limits_{k'=1}^{n_0}a_{p_0k'}b_{k'q_0}\\
		&=\sum\limits_{k=1}^n(A_{ik}B_{kj})_{pq}\\
	\end{align*}
	Thus we have done the prove.
\end{proof}
\begin{coro}
	If $B=(\beta_1,\beta_2,\dots,\beta_n)$, we have
	\[AB=(A\beta_1,A\beta_2,\dots,A\beta_n)\]
	If $A=\begin{bmatrix}
		\alpha_1\\
		\alpha_2\\
		\vdots\\
		\alpha_m
	\end{bmatrix}$, we have
	\[AB=\begin{bmatrix}
		\alpha_1B\\
		\alpha_2B\\
		\vdots\\
		\alpha_m
	\end{bmatrix}\]
\end{coro}
\begin{dde}
	We can define the elementary raw operations and elementary column operations. However, we will replace the scalar by a matrix $P$.
	\begin{enumerate}
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
		A_{11}&A_{12}\\
		A_{21}+PA_{11}&A_{22}+PA_{12}
		\end{bmatrix}\]
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
		A_{21}&A_{22}\\
		A_{11}&A_{12}
		\end{bmatrix}\]
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
		A_{11}&A_{12}\\
		PA_{21}&PA_{22}
		\end{bmatrix}\]
		where $P$ is an invertible matrix
	\end{enumerate}
	and
	\begin{enumerate}
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
			A_{11}&A_{12}+A_{11}P\\
			A_{21}&A_{22}+A_{21}P
		\end{bmatrix}\]
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
			A_{12}&A_{11}\\
			A_{22}&A_{21}
		\end{bmatrix}\]
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
			A_{11}&A_{12}P\\
			A_{21}&A_{22}P
		\end{bmatrix}\]
		where $P$ is an invertible matrix
	\end{enumerate}
\end{dde}
We can then define the partitioned elementary matrix, which will not be used so we skip it.
\begin{dde}
	Let $W_1,W_2,\dots,W_k$ be subspaces of vector space $V$. We say that $W_1,W_2,\dots,W_k$ are independent if
	\[\alpha_1+\alpha_2+\cdots+\alpha_k=0,\alpha_i\in W_i\]
	implies each $\alpha_i=0$.
\end{dde}
\begin{thm}
	Let $V$ be a finite-dimensional vector space. Let $W_1,W_2,\dots,W_k$ be subspaces of $V$ and let $W=W_1+W_2+\cdots+W_k$. The following are equivalent.
	\begin{enumerate}
		\item [(a)]$W_1,\dots,W_k$ are independent.
		\item [(b)]For each $2\le j\le k$, we have
		\[W_j\cap(W_1+\cdots+W_{j-1})=\{0\}\]
		\item [(c)]If $\mathcal{B}_i$ is an ordered basis for $W_i$, $1\le i\le k$, then the sequence $\mathcal{B}=(\mathcal{B}_1,\mathcal{B}_2,\dots,\mathcal{B}_k)$ is an ordered basis for $W$.
	\end{enumerate}
\end{thm}
\begin{proof}
	Assume (a). Let $\alpha$ be a vector in the intersection $W_j\cap(W_1+\cdots+W_{j-1})$. Then there are $\alpha_1,\dots,\alpha_{j-1},\alpha_i\in W_i$ such that
	\[\alpha_1+\cdots+\alpha_{j-1}+(-\alpha)+0+\cdots+0=0\]
	which means $\alpha_1=\cdots=\alpha_{j-1}=\alpha=0$.\\
	Now assume (b). Let $\alpha_1,\dots,\alpha_k,\alpha_i\in W_i$
	\[\alpha_1+\cdots+\alpha_k=0\]
	If there exists a nonzero $\alpha_i$, denote the largest $i$ by $j$, we have
	\[\alpha_1+\cdots+\alpha_j=0\]
	which means
	\[\alpha_j=-\alpha_1-\cdots-\alpha_{j-1}\]
	then $\alpha_j$ is a nonzero vector in the intersection $W_j\cap(W_1+\cdots+W_{j-1})$.\\
	The equivalence of (c) and (a) is simple to prove and we omit it here.
\end{proof}
One thing you may notice is that the proof of the equivalence of (a) and (b) do not require $V$ be finite-dimensional. We can restate (c) by this:\\
\indent There is one and only one decomposition of $\alpha\in W$ such that
\[\alpha=\alpha_1+\cdots+\alpha_k,\alpha_i\in W_i\]
This also do not need $V$ be finite-dimensional.\\ 
\indent If $W_1,\dots,W_k$ are independent, we call the sum of $W_1,\dots,W_k$ \textbf{direct sum} and we write
\[W=W_1\oplus W_2\oplus\cdots\oplus W_k\] 
\begin{dde}
	If $V$ is a vector space, a \textbf{projection} of $V$ is a linear operator $E$ on $V$ such that $E^2=E$.
\end{dde}
Let $R$ be the range of $E$ and let $N$ be the null space of $E$, we have
\begin{enumerate}
	\item $\beta\in R$ if and only if $E\beta=\beta$. We shall prove the ``only if" part. We have
	\[\beta=E\alpha=E^2\alpha=E\beta\]
	\item $V=R\oplus N$
	\item $\alpha=E\alpha+(\alpha-E\alpha)$
\end{enumerate}
From (3) we know that $V=R+N$, and then
\[\alpha+\beta=0,\alpha\in R,\beta\in N\]
we have
\[E\alpha+E\beta=\alpha=0\rightarrow\beta=0\]
and we prove (2), which implies that the decomposition in (3) is unique.\\
\indent Inversely, It's easy to see that if $V=R\oplus N$, there is one and only one projection operator $E$ with range $R$ and null space $N$. We call it the \textbf{projection on} $R$ \textbf{along} $N$.
\begin{thm}\label{directsumdecomposition}
	If $V=W_1\oplus\cdots\oplus W_k$, then there exists $k$ linear operation $E_1,\dots,E_k$ on $V$ such that
	\begin{enumerate}
		\item [(\romannumeral1)]each $E_i$ is a projection
		\item [(\romannumeral2)]$E_iE_j=0$, if $i\neq j$
		\item [(\romannumeral3)]$I=E_1+\cdots+E_k$
		\item [(\romannumeral4)]the range of $E_i$ is $W_i$
	\end{enumerate}
	Conversely, if $E_1,\dots,E_k$ are linear operators on $V$ which satisfy (\romannumeral2) and (\romannumeral3), and if we let $W_i$ be the range of $E_i$, then $V=W_1\oplus\cdots\oplus W_k$.
\end{thm}
\begin{proof}
	If $V=W_1\oplus\cdots\oplus W_k$, we have
	\[\alpha=\alpha_1+\cdots+\alpha_k,\alpha_i\in W_i\]
	We define $E_i\alpha=\alpha_i$, then (\romannumeral1)(\romannumeral2)(\romannumeral3)(\romannumeral4) are obviously correct.\\
	If $E_1,\dots,E_k$ are linear operators on $V$ which satisfy (\romannumeral2) and (\romannumeral3), we have
	\[E_i=\sum\limits_{j=1}^kE_iE_j=E_i^2\]
	and
	\[\alpha=E_1\alpha+\cdots+E_k\alpha\]
	which implies $V=W_1+\cdots+W_k$. Let $\alpha_i\in W_i$ and
	\[0=\alpha_1+\cdots+\alpha_k=E_1\alpha_1+\cdots+E_k\alpha_k\]
	then
	\[0=\sum\limits_{j=1}^kE_iE_j\alpha_j=E_i^2\alpha_i=\alpha_i\]
	which implies the independence of $W_1,\dots,W_k$.
\end{proof}
\begin{dde}
	Let $V$ be a vector space and $T$ be a linear operator on $T$. If $W$ is a subspace of $V$, we say that $W$ is \textbf{invariant under} $V$ if for each $\alpha$ in $W$ we have $T\alpha$ in $W$
\end{dde}
\begin{thm}\label{Tcommuteprojection}
	Let $T$ be a linear operator on $V$, and let $W_1,\dots,W_k$ and $E_1,\dots,E_k$ be as in Theorem \ref{directsumdecomposition}, then each $W_i$ is invariant under $T$ if and only if
	\[TE_i=E_iT,i=1,\dots,k\]
	If $\mathcal{B}=(\mathcal{B}_1,dots,\mathcal{B}_k)$ is an ordered basis for $V$, then the matrix of $T$ under $\mathcal{B}$ is a partitioned diagonal matrix
	\[A=\begin{bmatrix}
		A_1&0&\cdots&0\\
		0&A_2&\cdots&0\\
		\vdots&\vdots&\ddots&\vdots\\
		0&0&\cdots&A_k
	\end{bmatrix}\]
\end{thm}
\begin{proof}
	First if $\alpha_i\in W_i$ and $TE_i=E_iT$, we have
	\[T\alpha_i=TE_i\alpha_i=E_iT\alpha_i\]
	which means $T\alpha_i\in W_i$.\\
	Conversely, if each $W_i$ is invariant under $T$, we have
	\[T\alpha=TE_1\alpha+\cdots+TE_k\alpha\]
	and we know that if $\alpha\in W_i$, then $E_j\alpha=\delta_{ij}\alpha$.
	\[E_iT\alpha=TE_i\alpha\] 
\end{proof}
\subsection{Primary Decomposition and Eigenvalue Decomposition}
We want to build the theory from general to specific, so we will introduce the annihilating polynomials first.\\
\indent The first thing we notice is that the collection of polynomials $p$ which annihilating $T$ is an ideal\footnote{One thing you must remember is that we regard the polynomial $p$ in $F[x]$ and $p(T)$ in $\mathcal{L}(V)$ as the same thing}, because
\[(qp)(T)=q(T)p(T)=0\]
\indent The existence of annihilating polynomials will be guaranteed by the finite dimension. Let $n$ be the dimension of $V$, then the dimension of $\mathcal{L}(V)$ is $n^2$, which implies that $1,T,T^2,\dots,T^{n^2}$ must bu linearly dependent, and this means 
\[\lambda_0+\lambda T_1+\cdots+\lambda_{n^2}T^{n^2}=0\]
with not all $\lambda_i$ is zero.
\begin{dde}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$ over the field $F$. The \textbf{minimal polynomial} for $T$ is the unique monic generator of the ideal of polynomials over $V$ which annihilate $T$.
\end{dde}
We can define the minimal polynomial of a $n\times n$ matrix $A$ similarly, which is the same as the minimal polynomial of $T$ if $A$ is the matrix of $T$ under some basis.\\
\indent One thing you must know is that if $F_1$ is a subfield of $F$, and $A$ is a matrix over $F_1$, the minimal polynomials over $F_1$ and $F$ are the same. This is because that the minimal polynomial means that
\[A^k+a_{k-1}A^{k-1}+\cdots+a_0I=0\]
the system of $n^2$ linear equations of $a_0,\dots,a_{k-1}$ have a unique solution. For each $k$ we can write the similar equations, however, the system has solutions over $F$ if and only if it has solutions in $F_1$, because the elementary raw operation can happen only in $F_1$, which gives the condition of whether it has solutions, as well as the uniqueness of the solution. Thus the minimal polynomial is the same.\\   
\begin{dde}
	Let $V$ be a vector space over the field $F$ and let $T$ be a linear operator on $V$. An \textbf{eigenvalue} of $V$ is a scalar $\lambda$ in $F$ such that there exists a nonzero vector $\alpha\in V$ with $T\alpha=\lambda\alpha$. If $\lambda$ is an eigenvalue of $T$, then
	\begin{enumerate}
		\item [(a)]any $\alpha$ such that $T\alpha=\lambda\alpha$ is called a \textbf{eigenvector} of $T$ associated with $\lambda$
		\item [(b)]the collection of all eigenvectors associated with $\lambda$ is called \textbf{characteristic space}, which is obviously a subspace of $V$.
	\end{enumerate}
\end{dde}
Apparently, $\lambda$ is an eigenvalue of $T$ if and only if $(T-\lambda I)\alpha=0$ has nonzero solution, which implies $\det(T-\lambda I)=0$.\\
\indent We can define the eigenvalues of an $n\times n$ matrix $A$ similarly, which is the same as eigenvalues of $T$ if $A$ is the matrix of $T$ under some basis.\\ 
\indent $\det (T-\lambda I)$ is a polynomial of $\lambda$. We often want it to be a monic polynomial, thus we call $\det(\lambda I-T)$ the \textbf{characteristic polynomial} of $A$, whose roots are obviously the eigenvalues.
\begin{thm}
	Let $T$ be a linear operator on an $n$-dimensional vector space $V$. The characteristic and minimal polynomials for $T$ have the same roots, except for multiplicities.  
\end{thm}
\begin{proof}
	Let $p$ be the minimal polynomial for $T$. If $\lambda$ is a root of $p$, we have
	\[p(T)=(T-\lambda I)q(T)\]
	Because $q$ is not the annihilating polynomial, we can find a vector $\beta$ such that $q(T)\beta=\alpha\neq0$, then we have
	\[0=(T-\lambda I)\alpha\]
	which implies that $\lambda$ is an eigenvalue of $T$, and thus a root of the characteristic polynomial.\\
	Now suppose $\lambda$ is an eigenvalue of $T$. Let $\alpha$ be an eigenvector associated with $\lambda$, then we have
	\[p(T)\alpha=p(\lambda)\alpha=0\]
	which implies that $\lambda$ is a root of $p$.
\end{proof}
\begin{thm}[Cayley-Hamilton]\label{CayleyHamilton}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$, whose matrix is $A$ under some basis. If $f$ is the characteristic polynomial for $T$, then $f(T)=0$, which implies that the minimal polynomial $p$ divides $f$.
\end{thm}
\begin{proof}
	Let $B=adj\,(\lambda I-A)$. Obviously, each element of $B$ is a polynomial of $\lambda$ with degree less than or equal to $n-1$(or zero polynomial). We have
	\[B(\lambda I-A)=\det(\lambda I-A)I\]
	and we can write $B$ as
	\[B=\lambda^{n-1}B_0+\lambda^{n-2}B_1+\cdots+B_{n-1}\]
	then
	\[B(\lambda I-A)=\lambda^nB_0+\lambda^{n-1}(B_1-B_0A)+\cdots+\lambda(B_{n-1}-B_{n-2}A)-B_{n-1}A\]
	We denote $f$ as
	\[f(\lambda)=\lambda^n+a_1\lambda^{n-1}+\cdots+a_{n}\]
	Then we have
	\begin{align*}
		B_0&=I\\
		B_1-B_0A&=a_1I\\
		\vdots\\
		B_{n-1}-B_{n-2}A&=a_{n-1}I\\
		-B_{n-1}A&=a_n
	\end{align*}
	Multiply $A^n,A^{n-1},\dots,A$, we have
	\begin{align*}
		B_0A^n&=A^n\\
		B_1A^{n-1}-B_0A^n&=a_1A^{n-1}\\
		\vdots\\
		B_{n-1}A-B_{n-2}A^2&=a_{n-1}A\\
		-B_{n-1}A&=a_n
	\end{align*}
	add them up, the left side is zero, the right side is just the characteristic polynomial $f(A)$.Thus we have
	\[f(T)=0\] 
\end{proof}
By this theorem, if we can write $f$ as $(\lambda-\lambda_1)^{d_1}\cdots(\lambda-\lambda_k)^{d_k}$, then the minimal polynomial has the form
\[p(T)=(T-\lambda_1I)^{r_1}\cdots(T-\lambda_kI)^{r_k},1\le r_i\le d_i\]
We can go further by the next theorem.
\begin{thm}[Primary Decomposition Theorem]\label{PrimeDecomposition}
	Let $T$ be a linear operator on a finite dimensional vector space $V$ over the field $F$. Let $p$ be the minimal polynomial for $T$,
	\[p=p_1^{r_1}p_2^{r_2}\cdots p_k^{r_k}\]
	where $p_i$ are distinct monic prime polynomials over $F$ and $r_i$ are positive integers. Let $W_i$ be the null space of $p_i^{r_i},i=1,\dots,k$. Then
	\begin{enumerate}
		\item [(\romannumeral1)]$V=W_1\oplus\cdots\oplus W_k$
		\item [(\romannumeral2)]each $W_i$ is invariant under $T$
		\item [(\romannumeral3)]if $T_i$ is the operator induced on $W_i$ by $T$, then the minimal polynomial for $T_i$ is $p_i^{r_i}$
	\end{enumerate}
\end{thm}
\begin{proof}
	For each $i$, let
	\[f_i=\frac{p}{p_i^{r_i}}=\prod_{j\neq i}p_{j}^{r_j}\]
	They are relatively prime, thus we have
	\[\sum\limits_{i=1}^kf_ig_i\]
	Obviously, $f_if_j,j\neq i$ is dividable by $p$.Let $E_i(T)=f_i(T)g_i(T)$, then we have
	\[I=E_1+\cdots+E_k,E_iE_j=0\]
	If $\alpha$ is a vector in the range of $E_i$, then $\alpha=E_i\alpha$, and we have
	\[p_i^{r_i}(T)\alpha=p(T)g_i(T)\alpha=0\]
	Conversely, if $\alpha\in W_i$, because $E_j=f_jg_j$ is dividable by $p_i^{r_i}$ if $i\neq j$, we have
	\[E_j\alpha=0,i\neq j\rightarrow\alpha=E_i\alpha\]
	$W_i$ is apparently invariant under $T$ because $E_iT=TE_i$. if $T_i$ is the operator induced on $W_i$ by $T$, then $p_i^{r_i}(T_i)=p_i^{r_i}(T)=0$. If $g$ is the minimal polynomial for $T_i$, then $g(T)f_i(T)$, because $f_i(T)\alpha$ if $\alpha\in W_j,j\neq i$ and $g(T)\alpha=0$ if $\alpha\in W_i$, and we have $V=W_1\oplus\cdots\oplus W_k$. Therefore $p$ as well as $p_i^{r_i}$ divides $gf_i$, which means $p_i^{r_i}$ divides $g$.
\end{proof}
\begin{coro}
	If $U$ commutes with $T$, then $W_i$ is invariant under $U$.
\end{coro}
Now we want to study the specific case of this theorem.
\begin{dde}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$. We say that $T$ is \textbf{diagonalizable} if there exits a basis for $V$ each vector of which is an eigenvector of $T$.
\end{dde}
To comprehend the concept thoroughly, we want first to study it by characteristic space.
\begin{thm}
	The characteristic spaces of $T$ are independent.
\end{thm}
\begin{proof}
	Let $W_i$ be the characteristic space of $T$,
	\[\alpha_1+\cdots+\alpha_k=0,\alpha_i\in W_i\]
	Let $\lambda_i$ be the eigenvalues, then
	\begin{align*}
		\alpha_1+\cdots+\alpha_k&=0\\
		\lambda_1\alpha_1+\cdots+\lambda_k\lambda_k&=0\\
		\vdots\\
		\lambda_1^{k-1}\alpha_1+\cdots+\lambda_k^{k-1}\alpha_k&=0
	\end{align*}
	The coefficient determinant is just the Vandermonde determinant, and thus is nonzero, which means 
	\[A\begin{bmatrix}
		\alpha_1\\
		\alpha_2\\
		\vdots\\
		\alpha_k
	\end{bmatrix}=0,\alpha_1=\cdots=\alpha_k=0\] 
\end{proof}
We can conclude immediately that if $\lambda_i\neq\lambda_j,i\neq j$, then $T$ is obviously diagonalizable, then we have
\[V=W_1\oplus\cdots\oplus W_2\]
But if not? When $T$ is diagonalizable, this is still correct, however, we notice that some characteristic spaces will not be $1$-dimensional. We want to know whether this dimension is relevant to the multiplicity of the eigenvalue.
\begin{thm}
	The dimension of the characteristic space $W$ associated with the eigenvalue $\lambda_0$ is less than or equal to the multiplicity of $\lambda$ in the characteristic polynomial for $T$.
\end{thm}  
\begin{proof}
	Choose an ordered basis for $W$ and extend it to a basis for $V$. Let $r$ be the dimension of $W$ and $A$ be the matrix of $T$ under this basis. we have
	\[A=\begin{bmatrix}
		\lambda_0 I&B\\
		0&C
	\end{bmatrix}\]
	and
	\begin{align*}
		\det(\lambda I-A)&=\begin{vmatrix}
			\lambda I-\lambda_0I&-B\\
			0&\lambda I-C
		\end{vmatrix}\\
		&=\det(\lambda I-\lambda_0I)\det(\lambda I-C)\\
		&=(\lambda-\lambda_0)^r\det(\lambda I-C)
	\end{align*}
	We have finished the proof.
\end{proof} 
Obviously, if $T$ is diagonalizable, then the characteristic polynomial $f$ has the form
\[f(T)=(T-\lambda_1I)^{d_1}\cdots(T-\lambda_kI)^{d_k}\]
and because $d_1+\cdots+d_k=n$, we have $\dim W_i=d_i$.\\
\indent Moreover, we notice that $(T-\lambda_1I)\cdots(T-\lambda_kI)$ annihilate $T$, and thus
\[p=(x-\lambda_1)\cdots(x-\lambda_k)\] 
\begin{thm}
	$T$ is diagonalizable if and only if $p=(x-\lambda_1)\cdots(x-\lambda_k)$.
\end{thm}
\begin{proof}
	Let's prove the ``if" part. If so, $\lambda_i$ is then the eigenvalues of $T$, and use Theorem \ref{PrimeDecomposition}, we have
	\[V=W_1\oplus\cdots\oplus W_k\]
	and $W_i$ is the null space of $T-\lambda_iI$, which is just the characteristic space associated with $\lambda_i$.
\end{proof}
Actually, all the things about eigenvalue decomposition is done. But we want to prepare for the next section.
\begin{lem}
	Let $W$ be an invariant subspace for $T$. The characteristic polynomial for the restriction operator $T_W$ divides the characteristic polynomial for $T$. The minimal polynomial for $T_W$ divides the minimal polynomial for $T$.
\end{lem}
\begin{proof}
	Actually,the first part we have proven before
	\[A=\begin{bmatrix}
		B&C\\
		0&D
	\end{bmatrix}\]
	\begin{align*}
		\det(\lambda I-A)&=\begin{vmatrix}
			\lambda I-B&-C\\
			0&\lambda I-D
		\end{vmatrix}\\
		&=\det(\lambda I-B)\det(\lambda I-D)
	\end{align*}
	and we have
	\[A^k=\begin{bmatrix}
		B^k&C_k\\
		0&D^k
	\end{bmatrix}\]
	which means that any polynomial annihilate $T$ will annihilate $T_W$, then the minimal polynomial for $T_W$ divides the minimal polynomial for $T$. 
\end{proof}
\begin{dde}
	Let $W$ be an invariant subspace for $T$ and let $\alpha$ be a vector in $V$. The $T$-\textbf{conductor of} $\alpha$ \textbf{into} $W$ is the set $S_T(\alpha;W)$, which consists of all polynomials $g$ over $F$ such that $g(T)\alpha\in W$.
\end{dde}
If $W$ is $\{0\}$, we called the conductor the $T$-\textbf{annihilator of} $\alpha$.
\begin{lem}
	$S_T(\alpha;W)$ is an ideal in the polynomial algebra $F[x]$.
\end{lem}
\begin{proof}
	We can prove it by the fact that $W$ is invariant under every polynomial of $T$.  
\end{proof}
The unique monic generator of $S_T(\alpha;W)$ is also called the $T$-\textbf{conductor of} $\alpha$ \textbf{into} $W$. Note that every $S_T(\alpha;W)$ contains the minimal polynomial for $T$, hence, every $T$-conductor divides the minimal polynomial for $T$.
\begin{lem}
	Let $V$ be a finite-dimensional vector space over the field $F$. Let $T$ be a linear operator on $V$ such that the minimal polynomial for $T$ is a product of linear factors
	\[p=(x-\lambda_1)^{r_1}\cdots(x-\lambda_k)^{r_k}\]
	Let $W$ be a proper subspace of $V$ which is invariant under $T$. There exists a vector $\alpha$ in $V$ such that
	\begin{enumerate}
		\item [(a)]$\alpha$ is not in $W$
		\item [(b)]$(T-\lambda I)\alpha$ is in $W$, for some eigenvalue $\lambda$ of $T$.
	\end{enumerate}
\end{lem}
\begin{proof}
	Let $\beta$ be a vector not in $W$. Let $g$ be the $T$-conductor of $\beta$ into $W$, then $g$ divides $p$, and thus has the form
	\[g=(x-\lambda_1)^{e_1}\cdots(x-\lambda_k)^{e_k}\]
	with at least one $e_i$ is nonzero. Choose $e_j>0$, we have
	\[g(T)=(T-\lambda_j I)h(T)\]
	By the definition, $\alpha=h(T)\beta$ is not in $W$, and we have
	\[(T-\lambda_j I)\alpha=g(T)\beta\in W\]
	Thus we have found what we need.
\end{proof}
Not all linear operators can be diagonalized, so we want to loose the requirement. The linear operator $T$ is called \textbf{triangulable} if there is an ordered basis for $V$ in which $T$ is represented by a triangular matrix.
\begin{thm}\label{SchurDecomposition}
	Let $V$ be a finite-dimensional vector space over the field $F$ and let $T$ be a linear operator on $V$. Then $T$ is triangulable if and only if the minimal polynomial for $T$ is the product of linear polynomials over $F$.
\end{thm}
\begin{proof}
	The ``only if" part is obvious by the Theorem \ref{CayleyHamilton}. We shall prove the ``if" part.\\
	Applying the lemma above, we can first let $W$ be $\{0\}$, and then continue the process to find $\alpha_i$ and add it into $W$ until $W=V$. Apparently, each time we have
	\[T\alpha_j=a_{1j}\alpha_1+\cdots+a_{jj}\alpha_j,1\le j\le n\]
	Then $\{\alpha_i\}$ is just the basis that the matrix of $T$ under it is triangular. 
\end{proof} 
\begin{coro}
	Let $F$ be an algebraically closed field. Every linear operator $T$ on $V$ is triangulable.
\end{coro}
Finally we want two operators triangulated or diagonalized simultaneously. One thing you may notice that if two operators are diagonalized simultaneously, they must commute with each other. You will see this is also the sufficient condition for diagonalization and triangulation, however, is not a necessary condition for the latter one.
\begin{lem}
	Let $\mathcal{F}$ be a commuting family of triangulable linear operators on $V$. Let $W$ be a proper subspace of $V$ which is invariant under $\mathcal{F}$. There exists a vector $\alpha$ such that
	\begin{enumerate}
		\item [(a)]$\alpha\notin W$
		\item [(b)]for each $T$ in $\mathcal{F}$, $T\alpha$ is in the subspace spanned by $\alpha$ and $W$.
	\end{enumerate}
\end{lem}
\begin{proof}
	First, $\mathcal{F}$ is finite-dimensional, so we can choose a basis for $\mathcal{F}$, $\{T_1,\dots,T_r\}$. Applying the previous lemma, we find a $\alpha_1$ not in $W$ and a scalar $\lambda_1$ such that $(T_1-\lambda_1 I)\alpha_1\in W$. Denote the collection of $\alpha$ such that $(T_1-\lambda_1 I)\alpha\in W$ by $V_1$. Obviously, $V_1$ is a subspace of $V$ and $W_1$ is a proper subspace of $V_1$. We have
	\[(T_1-\lambda_1 I)T\alpha=T(T_1-\lambda_1 I)\alpha\in W\subset V_1\]
	Thus $V_1$ is invariant under $\mathcal{F}$. Restrict the linear operators in $\mathcal{F}$ to $V_1$ and repeat the process, we will have a $\alpha$ such that
	\begin{enumerate}
		\item [(a)]$\alpha\notin W$
		\item [(b)]$(T_i-\lambda_i I)\alpha\in W,i=1,\dots,r$
	\end{enumerate}
	and then have found what we need.
\end{proof}
\begin{thm}
	Let $V$ be a finite-dimensional vector space over the field $F$. Let $\mathcal{F}$ be a commuting family of triangulable linear operators on $V$. There exists an ordered basis for $V$ such that every operator in $\mathcal{F}$ is represented by a triangular matrix in that basis.
\end{thm}
\begin{proof}
	Use the lemma above and prove like we have done in the proof of Theorem \ref{SchurDecomposition}.
\end{proof}
\begin{thm}
	Use induction. There is nothing to prove when $n=1$. Let $V$ be $n$-dimensional. Let $T$ be a linear operator in $\mathcal{F}$ which is not a scalar multiply the identity, we know that the direct sum of the null space $W_i$ of $T-\lambda_i I$ is just $V$. $W_i$ is obviously invariant under $\mathcal{F}$, and then we can restrict $\mathcal{F}$ to $W_i$. Because $\dim W_i<\dim V$, by induction, we can find a basis $\mathcal{B}_i$ for each $i$ that simultaneously diagonalize $\mathcal{F}_i$, and $\mathcal{B}=\{\mathcal{B}_1,\dots,\mathcal{B}_k\}$ is just the basis we are looking for.
\end{thm}
We want to point out a useful expression of diagonalizable linear operators.
\begin{thm}\label{EigenvalueDecomposition}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$.\\
	If $T$ is diagonalizable and $\lambda_1,\dots,\lambda_k$ are distinct eigenvalues of $T$, then there exists linear operators $E_1,\dots,E_k$ on $V$ such that 
	\begin{enumerate}
		\item [(\romannumeral1)]$T=\lambda_1E_1+\cdots+\lambda_kE_k$
		\item [(\romannumeral2)]$I=E_1+\cdots+E_k$
		\item [(\romannumeral3)]$E_iE_j=0,i\neq j$
		\item [(\romannumeral4)]$E_i^2=E_i$
		\item [(\romannumeral5)]the range of $E_i$ is the characteristic space for $T$ associated with $\lambda_i$
	\end{enumerate}
	Conversely, if there exist distinct scalars $\lambda_1,\dots,\lambda_k$ and nonzero linear operators $E_1,\dots,E_k$ which satisfy (\romannumeral1)(\romannumeral2)(\romannumeral3), then $T$ is diagonalizable and $\lambda_1,\dots,\lambda_k$ are eigenvalues of $T$, and (\romannumeral4)(\romannumeral5) are satisfied as well
\end{thm}
\begin{proof}
	If $T$ is diagonalizable, then $V=W_1\oplus\cdots\oplus W_k$, where $W_i$ is the null space of $T-\lambda_i$. Use Theorem \ref{directsumdecomposition}, we get $E_1,\dots,E_k$, which satisfy (\romannumeral2)(\romannumeral3)(\romannumeral4)(\romannumeral5) automatically. Use Theorem \ref{Tcommuteprojection}, we have
	\[T\alpha=TE_1\alpha+\cdots+TE_k\alpha=\lambda_1E_1\alpha+\cdots+\lambda_kE_k\alpha\]
	Now suppose there exist distinct scalars $\lambda_1,\dots,\lambda_k$ and nonzero linear operators $E_1,\dots,E_k$ which satisfy (\romannumeral1)(\romannumeral2)(\romannumeral3), then (\romannumeral4) will be satisfied immediately. Use Theorem \ref{directsumdecomposition} again, we know that $V=W_1\oplus\cdots\oplus W_k$, where $W_i$ is the range of $E_i$. We have
	\[T-\lambda I=(\lambda_1-\lambda)E_1+\cdots+(\lambda_k-\lambda)E_k\]
	If $(T-\lambda I)\alpha=0$ for a nonzero vector $\alpha$, then there exists some $E_i$ such that $E_i\alpha\neq0$, because $V=W_1\oplus\cdots\oplus W_k$, we have $\lambda=\lambda_i$ for some $i$. Thus $\lambda_i$ is a eigenvalue of $T$. If $\alpha$ in $W_i$, we have
	\[T\alpha=TE_i\alpha=\lambda_i\alpha\]
	Conversely, if $\alpha$ is a vector in the characteristic space for $T$ associated with $\lambda_i$, we have
	\[(T-\lambda_i I)\alpha=\sum\limits_{j=1}^k(\lambda_j-\lambda_i)E_j\alpha=0\]
	which means $E_j\alpha=0,j\neq i$ and $\alpha=E_i\alpha$.
	Then we have finished the proof.  
\end{proof}
\begin{dde}
	Let $N$ be a linear operator on a vector space $V$. We say that $N$ is \textbf{nilpotent} if there is some positive integer $r$ such that $N^r=0$.
\end{dde}
\begin{thm}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$ over the field $F$. Suppose that the minimal polynomial for $T$ is a product of linear polynomials. Then there is a diagonalizable operator $D$ on $V$ and a nilpotent operator $N$ on $V$ such that
	\begin{enumerate}
		\item [(\romannumeral1)]$T=D+N$
		\item [(\romannumeral2)]$DN=ND$
	\end{enumerate}
	The diagonalizable operator $D$ and the nilpotent operator $N$ are uniquely determined by (\romannumeral1)(\romannumeral2), and each of them is a polynomial of $T$. We call $D$ the \textbf{diagonalizable part} of $T$.
\end{thm}
\begin{proof}
	Use Theorem \ref{PrimeDecomposition},\ref{directsumdecomposition} and \ref{EigenvalueDecomposition}, we have $W_i$ is the null space of $(T-\lambda_i I)^r{i}$, and then let
	\[D=\lambda_1E_1+\cdots+\lambda_kE_k\]
	Let $N=T-D$, we have
	\[N^r=(T-\lambda_1 I)^rE_1+\cdots+(T-\lambda_k I)=0\]
	when $r\ge r_i,i=1,2,\dots,k$.\\
	You may notice that in the proof of Theorem \ref{PrimeDecomposition}, each $E_i$ is a polynomial of $T$. This is also correct in the Theorem \ref{EigenvalueDecomposition}.\\
	If there exist a diagonalizable operator $D'$ and a nilpotent operator $N'$ satisfy (\romannumeral1)(\romannumeral2), then we have
	\[D-D'=N'-N\]
	Since $D',N'$ commute with each other, they commute with $T$, as well as $D,N$. Then we know that $D,D'$ can be diagonalized simultaneously and $D-D'$ is diagonalzable. Since $N,N'$ are both nilpotent, we have
	\[(N'-N)^r=\sum\limits_{i=0}^r(N')^i(-N)^{r-i}\]
	when $r$ is large enough, this will be zero. Thus $N'-N$ is nilpotent, which means the minimal polynomial for $D-D'$ divides $x^m$ for some $m$, and then is $x^r$. Because $D-D'$ is diagonalizable, $r=1$, which is just $D-D'=0,N'-N=0$.
\end{proof}
\begin{coro}
	Let $V$ be a finite-dimensional vector space over an algebraically closed field $F$. Then every linear operator $T$ on $V$ can be written as the sum of a diagonalizable operator $D$ and a nilpotent operator $N$ which commute. These operators are unique and each is a polynomial of $T$.
\end{coro}
We will go further in how to decompose a matrix in the next section.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{shinobu2}
\end{figure}
\newpage
\section{The Rational and Jordan Forms}
\end{document}






















