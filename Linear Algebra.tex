\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage{braket}
\usepackage{amssymb}
\usepackage[hidelinks,CJKbookmarks]{hyperref}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\graphicspath{{C:/Users/18967/Desktop/latex/figure/}}
\title{Linear Algebra}
\author{haruki}
\date{\today}
\begin{document}
\maketitle
\tableofcontents
\thispagestyle{empty}
\newpage
\setcounter{page}{1}
\section{Linear Equations}\label{1}
\subsection{What is Fields? What is Vector Spaces?}
\noindent In short, a Field $F$ should satisfy these properties:
\begin{enumerate}
	\item[(A)] Axioms for addition
	\begin{enumerate}
		\item[(A1)] if $x$,$y$ in $F$, then $x+y$ in $F$
	    \item[(A2)] Addition is commutative:\[x+y=y+x\]for all $x$ and $y$ in $F$
	    \item[(A3)] Addition is associative:\[x+(y+z)=(x+y)+z\]for all $x$,$y$ and $z$ in $F$
	    \item[(A4)] There is an element $0$ in $F$ such that $x+0=x$, for all $x$ in $F$
	    \item[(A5)] To every $x$ in $F$,there exists an element $-x$ in $F$ such that \[(-x)+x=0\]
	\end{enumerate}
	\item[(M)] Axioms for multiplication
	\begin{enumerate}
		\item[(M1)] if $x$,$y$ in $F$, then $xy$ in $F$
		\item[(M2)] Multiplication is commutative:\[xy=yx\]for all $x$ and $y$ in $F$
		\item[(M3)] Multiplication is associative:\[x(yz)=(xy)z\]for all $x$,$y$ and $z$ in $F$
		\item[(M4)] There is an element $1\neq 0$ in $F$ such that $1x=x$, for all $x$ in $F$
		\item[(M5)] To every $x\neq 0$ in $F$, there exists an element $\dfrac{1}{x}$ in $F$ such that \[(\frac{1}{x})x=1\]
	\end{enumerate}
	\item[(D)] The distributive law\[x(y+z)=xy+xz\]for all $x$,$y$ and $z$ in $F$
\end{enumerate}
\indent Some familiar propositions like cancellation law and the uniqueness of $0$ and $1$ is omitted here. See the comprehensive proofs in Baby Rudin.\\
\indent In the following context, we assume the field $F$ as a subfield of the complex numbers. Otherwise, it could be possible that $1+1+\dots =0$! You must not want to see that, which arises from the nonzero characteristic of the field. So we assume the field $F$ we discuss is a field of characteristic zero.\\
\indent Our preparation work for vector spaces has done. Let's define it!
\theoremstyle{defination}\newtheorem{dde}{Defination}[section]
\begin{dde}
	A vector space consists of the following:
	\begin{enumerate}
		\item a field $F$ for scalars
		\item a set $V$ of objects, called vectors
		\item an operation called vector addition,which satisfies the axioms for additions (A)
		\item an operation called scalar multiplication, which satisfies:
		\begin{enumerate}
			\item[(a)] $c\alpha$ is in $V$ for every $\alpha$ in $V$
			\item[(b)] $1\alpha=\alpha$
			\item[(c)] $(c_1c_2)\alpha=c_1(c_2\alpha)$
			\item[(d)] $c(\alpha+\beta)=c\alpha+c\beta$
			\item[(e)] $(c_1+c_2)\alpha=c_1\alpha+c_2\alpha$
		\end{enumerate}
	\end{enumerate}
\end{dde}
The scalar multiplication has familiar properties just like the ordinary multiplication. We will dig into this concept more deeply in the following sections.
\subsection{Systems of Linear Equations}\label{systems of le}
\noindent Of Course, the numbers we involved are the elements of a field $F$.\\
A system of linear equations is defined as:
\begin{align}
	A_{11}x_1+A_{12}x_2+\ldots+A_{1n}x_n&=y_1\notag\\
	A_{21}x_1+A_{22}x_2+\ldots+A_{2n}x_n&=y_2\notag\\
	\vdots\:\,\quad\quad\quad\vdots\quad\quad\quad\qquad\quad\vdots\quad&=\,\:\vdots\notag\\
	A_{m1}x_1+A_{m2}x_2+\ldots+A_{mn}x_n&=y_m\notag
\end{align}
\indent Obviously, there's no doubt that all the linear combinations of the equations are still linear equations, and every solution of the previous system of linear equations is also a solution of the new one.\\
\indent However, we want to transform the original system of linear equations to the simplest form, which requires that the solutions should be the same. Therefore, if the original equations are also the linear combinations of the new one, the solutions must be the same, and we've reached our purpose. The two systems of linear equations are called \textbf{equivalent}, if each equation in each system is a linear combination of the equations in the other system. And we have the following apparent theorem.\\
\theoremstyle{plain}\newtheorem{thm}{Theorem}[section]
\begin{thm}
	Equivalent systems of linear equations have exactly the same solutions.
\end{thm}
Then all we have to do is to find the suitable way of linear combinations.
\subsection{Matrices and Elementary Row Operations}
\noindent To simplify our notations, we introduce a concept:
\[A=\begin{bmatrix}
	A_{11}&A_{12}&$\ldots$&A_{1n}\\
	A_{21}&A_{22}&$\ldots$&A_{2n}\\
	\vdots&\vdots&\ddots&\vdots\\
	A_{n1}&A_{n2}&$\ldots$&A_{nn}
\end{bmatrix}\]
We called $A$ a matrix. Sometimes we use the notation $\{A_{ij}\}$ to denote it.\\
\indent By now the matrix is only a pile of numbers, so we endow it some operations:
\begin{enumerate}
	\item matrix addition:$(A+B)_{ij}=A_{ij}+B_{ij}$
	\item scalar multiplication:$(cA)_{ij}=cA_{ij}$
	\item matrix multiplication:$(AB)_{ij}=\sum\limits_{r=1}^pA_{ir}B_{rj}$,where $AB$ is a $m\times n$ matrix, and $A,B$ are $m\times p,p\times n$ matrices respectively. 
\end{enumerate}
The $n\times n$ identity matrix is denoted by $I_n$ with entries $I_{ij}$ that:
\[I_{ij}=
\begin{cases}
	1&\text{if }i=j\\
	0&\text{if }i\neq j
\end{cases}\]
which has a significant property:$IA=AI=A$.\\
\indent The first and second operations make the matrices space become a vector space, and the third operation make the matrices space become a algebra, which will simplify our notations greatly.\\
\indent An important property of matrix multiplication should be pointed out:
\theoremstyle{plain}\newtheorem{pro}{Proposition}[section]
\begin{pro}
	The matrix multiplication is associative.
\end{pro}
\begin{proof}
	The proof is direct. If $A,B,C$ are $m\times p,p\times q,q\times n$ matrices, then
	\begin{align}
		((AB)C)_{ij}&=\sum\limits_{t=1}^q(\sum\limits_{r=1}^pA_{ir}B_{rt})C_{tj}=\sum\limits_{t=1}^q\sum\limits_{r=1}^pA_{ir}B_{rt}C_{tj}\notag\\
		&=\sum\limits_{r=1}^pA_{ir}(\sum\limits_{t=1}^qB_{rt}C_{tj})=(A(BC))_{ij}\notag
	\end{align}
\end{proof}
But it isn't commutative and you can prove it directly as well.\\
\indent We have possessed the matrix, so let's come back to the system of linear equations. You could find an amazing thing we expect:
\[AX=Y\]and where
\[A=\begin{bmatrix}
	A_{11}&A_{12}&$\ldots$&A_{1n}\\
	A_{21}&A_{22}&$\ldots$&A_{2n}\\
	\vdots&\vdots&\ddots&\vdots\\
	A_{n1}&A_{n2}&$\ldots$&A_{nn}
\end{bmatrix}\]
\[X=\begin{bmatrix}
	x_1\\
	x_2\\
	\vdots\\
	x_n\\
\end{bmatrix},
Y=\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n\\
\end{bmatrix}\]This is just the same thing we discussed above!So the objects concerned will be matrices from now on.\\
\indent Don't forget the target we said at the end of the subsection \ref{systems of le}!I will show you that the way we're looking for can be presented as a convenient matrix.\\
\indent We define three elementary row operations of matrix as following:
\begin{enumerate}
	\item multiplication of one row of $A$ by a nonzero scalar $c$
	\item replacement of the $r$th row of $A$ by row $r$ plus $c$ times row $s$, $c$ is any scalar and $r\neq s$
	\item interchange of two rows of $A$
\end{enumerate}
The elementary matrices come after:
\begin{dde}
	An elementary matrix $E_n$ is a matrix which is the consequence of an elementary raw operation on the identity matrix $I_n$. 
\end{dde}
A simple but important conclusion reached now:
\begin{pro}
	The consequence of an elementary raw operation on $m\times n$ matrix $A$ is just $e_mA$.
\end{pro}
Whose proof is direct and I'm lazy to type.\\
\indent By the definition of elementary raw operations, it's apparent that each operation has a \textbf{unique} inverse operation to cancel the effect it cause. 
\indent For a further convenience, we introduce the inverse of a matrix:
\begin{dde}
	Let $A$ be a $n\times n$ matrix. If $BA=I$,we call $B$ a \textbf{left inverse} of $A$. If $AC=I$,we call $C$ a \textbf{right inverse} of $A$. If $BA=AB=I$, we call $B$ a \textbf{two-side inverse} of $A$. You may find that:\[B=BI=BAC=IC=C\]So we use $A^{-1}$ to denote the \textbf{inverse} of $A$, and $A$ is called invertible.
\end{dde}
It's simple to prove:
\begin{thm}
\begin{enumerate}
	\item If $A$ is invertible, then $(A^{-1})^{-1}=A$
	\item If $A,B$ are invertible, then $AB$ is invertible and the inverse is $B^{-1}A^{-1}$ 
\end{enumerate}
\end{thm}
Then we can formalize the statement above:
\begin{thm}
	The elementary matrix is invertible, and its inverse is just the matrix which corresponds to the inverse operation.
\end{thm}
Let's come back to the topic again. Why we choose the elementary raw operations?Because we have the following theorem:
\begin{thm}
	Let $A^*$ be $(A,Y)$ for a system of linear equations which we call the \textbf{augmented matrix} of the system. $B^*,A^*$ are called \textbf{row-equivalent} if they are connected by a series of elementary row operations, then the two system are equivalent as a result.
\end{thm}
We can write $A^*,B^*$ in an equation:\[B^*=E_1E_2\dots E_nA^*\footnote{the foot scripts here don't mean the dimension}\]
and an inverse one:\[A^*=E_n^{-1}E_{n-1}^{-1}\dots E_1^{-1}B^*\]
\indent A more convenient concept comes now:
\begin{dde}
	An $m\times n$ echelon matrix $R$ is called row-reduced if:
	\begin{enumerate}
		\item[(a)] the first nonzero entry in each nonzero row of $R$ is equal to $1$
		\item[(b)] each column of $R$ which contains the leading nonzero entry of some row has all its other entries $0$.
		\item[(c)] if rows$1,2,\dots,r$ are the nonzero rows of $R$, and if the leading nonzero entry of row $i$ occurs in column $k_i$,$i=1,2,\dots,r$,then $k_1<k_2<\dots<k_r$
	\end{enumerate}
\end{dde}
For example, $I_n$ is just a row-reduced echelon matrix.\\
\indent We have a theorem below, whose proof is apparent:
\begin{thm}
	Every $m\times n$ matrix over the field $F$ is row-equivalent to a row-reduced echelon matrix.
\end{thm}
And we have done the most of all things of systems of linear equations. You may find that if we use the elementary raw operations on the augmented matrix $A^*$ to make it become row-reduced echelon matrix $R$, then we get a explicit solution,because from the definition, we have $x_{k_1},x_{k_2},\dots\footnote{the foot scripts label the first nonzero entry of each nonzero row}$ as linear combinations of the rest $x_m$, which by arbitrary values of $x_m$ becomes a solution.\\
\indent Of course we forgot the situation of no solutions, which can be simply demonstrated by the statement:
\begin{pro}
	Let $R$ be the raw-reduced echelon augmented matrix of a system of linear equations, then:
	\begin{itemize}
		\item If there's a first nonzero entry in the last column, there's no solutions
		\item If not the above condition, the solution is unique if except the last column $R$ is $I_n$,otherwise there are infinite number of solutions. 
	\end{itemize} 
\end{pro} 
If you are smart enough, you would find that the uniqueness of solutions means the coefficient matrix $A$ is an invertible matrix,because:
\[E_1E_2\dots E_nA=I_n\]
implies the inverse of $A$ is:
\[A^{-1}=E_n^{-1}E_{n-1}^{-1}\dots E_1^{-1}I_n\]
and the procedure can be taken by hand if we write them together $(A,I_n)$,and use the elementary raw operations till $A$ becomes $I_n$, then the right is just $A^{-1}$.We can go further with this, if you notice that for an invertible matrix $A$, the solution of the systems of linear equations with coefficient matrix $A$ is unique. Then we have a theorem
\begin{thm}\label{determinantandinvertible}
	A square matrix $A$ is invertible if and only if the solution of the systems of linear equations with coefficient matrix $A$ is unique. Moreover, if there exits a square matrix $B$ such that $BA=I$ or $AB=I$, then $A$ is invertible and $B=A^{-1}$. 
\end{thm}
\begin{proof}
	We shall prove the last statement. If $BA=I$, then $AX=Y$ means $X=BY$ that has a unique solution, which implies $B=A^{-1}$.\\Similarly, $AB=I$ means $A=B^{-1}$, and then $B=A^{-1}$.
\end{proof}
\indent The last thing about systems of linear equations you need to know is that the solutions of a system of linear equations is a vector space. Let the rest $x_m$ be a series of values:\[(1,0,\dots,0),(0,1,\dots,0),\dots(0,0,\dots,1)\]and a series of solutions come after. You may notice that each solutions of the system is a linear combination of the $n-r$ solutions, which implies the solutions of the system is a vector space that has a dimension of $n-r$, which we will detail in the next section.\\
\indent We finish this section by some properties of matrices:
\begin{thm}
	\begin{enumerate}
		\item Write $B$ as $(\beta_1,\beta_2,\dots,\beta_n)$, then $AB=(A\beta_1,A\beta_2,\dots,A\beta_n)$
		\item $A$ is called upper triangle matrix if it only have nonzero entries above the diagonal(contain the diagonal), and the product of two upper triangle matrices is still a upper triangle matrix
		\item It will be useful if you notice that $AE_n$ corresponds to an elementary column operation, which is just a copy version of the row one. 
	\end{enumerate}
\end{thm}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{reimu}
\end{figure}
\newpage
\section{Vector Spaces}\label{2}
\subsection{Subspaces,Basis and Dimension}
For a further discussion about linear algebra,we will introduce some useful concepts:
\begin{dde}
	Let $V$ be a vector space over the field $F$.$W$ is a subset of $V$.$W$ is a subspace of $V$ if $W$ is a vector space with vector addition and scalar multiplication on $V$.
\end{dde}
A simple method to check $W$ is make an arbitrary linear combination by two arbitrary vectors in $W$,which means:
\begin{thm}
	If $\alpha$ and $\beta$ are vectors in $W$,then $W$ is a subspace if and only if $c\alpha+\beta$ in $W$ for each pair of $\alpha,\beta$ and each scalar $c$.
\end{thm}
We will introduce the concept ``span" in a new way:
\begin{thm}
	The intersection of any collections of subspaces is also a subspaces.
\end{thm}
The proof is easy,so let's skip it.
\begin{dde}
	Let $S$ be a set of vectors of $V$.The \textbf{subspace spanned} by $S$ is the intersection $W$ of all subspaces that contains $S$.If $S$ is a finite set,then we call $W$ the \textbf{subspace spanned by the vectors} $\{\alpha_1,\alpha_2,\dots,\alpha_n\}$. 
\end{dde}
A more usual form comes form this:
\begin{thm}
	The subspace spanned by a non-empty subset $S$ of a vector space $V$ is the set of all linear combinations of vectors in $S$. 
\end{thm}
There's an operation of addition defined on the subsets:
\begin{dde}
	If $S_1,S_2,\dots,S_k$ is subsets of a vector space $V$,the set of all sums
	\[\alpha_1,\alpha_2,\dots,\alpha_k\]of vectors $\alpha_i$ in $S_i$ is called the \textbf{sum} of the subsets $S_1,S_2,\dots,S_k$ and is denoted by
	\[S_1+S_2+\dots+S_k\]If $S_1,S_2,\dots,S_k$ are subspaces,then the sum of them is also a subspace.
\end{dde}
A further exploration need the concept ``linear dependent":
\begin{dde}
	A subset $S$ of a vector space $V$ is called \textbf{linearly dependent} if there exits distinct vectors $\alpha_1,\alpha_2,\dots,\alpha_n$ in $S$ and scalars $c_1,c_2,\dots,c_n$ in F,not all of which are $0$,such that
	\[c_1\alpha_1+c_2\alpha_2+\dots+c_n\alpha_n=0\]
	A set which is not linear dependent is called \textbf{linearly independent}.If $S$ is only a finite set of vectors,we call these vectors \textbf{dependent} or \textbf{independent} rather than call $S$.
\end{dde}
An important remark is that if $S$ is an infinite set,the concept above didn't mean \textbf{any} infinite sum of vectors.We will see this in the following concept.
\begin{dde}
	A \textbf{basis} for $V$ is a linearly independent set of vectors in $V$,which span the $V$.The \textbf{dimension} of $V$ is the number of elements in the basis.It's easy to see that the linear combination of each vector is unique
\end{dde}
You may notice a theorem is a must to make the definition legitimate.
\theoremstyle{plain}\newtheorem{lem}{Lemma}[section]
\begin{lem}\label{lemma1}
	Let $\alpha_1,\alpha_2,\dots,\alpha_k$ be independent vectors.A vector $\beta$ is a linear combination of $\alpha_1,\alpha_2,\dots,\alpha_k$ if and only if $\beta,\alpha_1,\alpha_2,\dots,\alpha_k$ are dependent vectors.
\end{lem}
\begin{lem}\label{lemma2}
	If a nonzero vector $\beta$ is a linear combination of independent vectors $\alpha_1,\alpha_2,\dots,\alpha_k$,there exits $i$ such that $\beta,\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$ are independent vectors.
\end{lem}
\begin{proof}
	Because $\beta$ is nonzero,there exits $i$ such that the coefficient of $\alpha_i$ is nonzero,which means $\alpha_i$ is a linear combination of $\beta,\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$.\\
	If $\beta,\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$ are dependent,the coefficient of $\beta$ should be nonzero otherwise $\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$ are dependent,which means $\alpha_i$ is a linear combination of $\alpha_1,\alpha_2,\dots,\alpha_{i-1},\alpha_{i+1},\dots,\alpha_k$,and by the Lemma \ref{lemma1},they are dependent.\\Therefore the hypothesis is incorrect.
\end{proof}
\begin{thm}\label{repthm}
	If $\beta_1,\beta_2,\dots,\beta_m$ are independent vectors so as $\alpha_1,\alpha_2,\dots,\alpha_n$,and each of them is a linear combination of $\alpha_1,\alpha_2,\dots,\alpha_n$,then they can replace some of the $\alpha$ to form a new independent vector set.
\end{thm}
\begin{proof}
	We notice that repeat the procedure in Lemma \ref{lemma2} we can change the original set by the new set,and by the independence of $\beta$ we assure the nonzero coefficients happen on $\alpha$,then the proof is over.
\end{proof}
From this theorem,We can prove lots of corollaries.
\theoremstyle{plain}\newtheorem{coro}{Corollary}[section]
\begin{coro}
	If every element of a finite set of independent vectors is a linear combination of the other,then the number of elements of the former $m$ is no more than the one of the latter $n$.  
\end{coro}
\begin{coro}
	The number of every finite basis is equal.
\end{coro}
\begin{coro}
	If there exits an infinite basis,then there's no finite basis.
\end{coro}
Hence definition of dimension is legitimate now.\\
\indent A simple example of basis is below.
Let $V$ be $F^n$\footnote{This means n-triples on field $F$,whose operations defined as usual.},then
\[e_i=(0,0,\dots,1,\dots,0)\qquad\text{The $i$th value is $1$}\]
is a basis.It's natural to popularize this to infinite-dimensional space $F^{\infty}$,but it's wrong.Do you remember the remark above?Yes,the \textbf{``span"} should be \textbf{``finite"} as the \textbf{``finite"} requirement appears in the definition of linear combination!So it's impossible to express $(1,1,\dots)$ by the method above.To ``span" this vector space,we need \textbf{orthogonal basis} rather than usual basis,which appears in functional analysis.\\
\indent How can we form a basis?A useful way to do that is extend a basis of a subspace $W$.The feasibility of this method is assured by Theorem \ref{repthm},which says every basis of $V$ can be replaced partly by a basis of $W$.There's a further theorem:
\begin{thm}
	Let $W_1,W_2$ be finite-dimensional subspaces of $V$,then
	\[\dim W_1+\dim W_2 =\dim(W_1\cap W_2)+\dim(W_1+W_2)\]
\end{thm} 
\begin{proof}
	We can extend the basis of $W_1\cap W_2$ in $W_1,W_2$ respectively,which forms a basis of $W_1,W_2$ and can be proved easily.
\end{proof}
Possessing these theorems,we can learn the topics about ``coordinate" and ``rank",which will be explained in the next subsection.
\subsection{Coordinate and Rank}
\noindent For convenience,the vector space we discuss below is finite-dimensional.
\indent To describe the ``coordinate",we need a basis ordered.having an ordered basis we can define ``coordinate" as following:
\begin{dde}
	A vector $\alpha$ in $V$ can be uniquely presented as a linear combination of an ordered basis $\{\alpha_i\}$,and the coefficient $x_i$ of each $\alpha_i$ is called the $i$th \textbf{coordinate} of $\alpha$ related to the ordered basis $\{\alpha_i\}$.We can write the coordinates in a matrix called \textbf{coordinate matrix}
	\[X=\begin{bmatrix}
		x_1\\
		\vdots\\
		x_n
	\end{bmatrix}\]   
\end{dde}
To do something about coordinate further,we need a theorem about matrix.You may notice we develop the properties of matrix at the time we need it.
\begin{thm}\label{invertindepend}
	Write the square matrix $A$ as column vectors $(\alpha_1,\alpha_2,\dots,\alpha_n)$.$A$ is invertible if and only if $\alpha_1,\alpha_2,\dots,\alpha_n$ are linearly independent. 
\end{thm}
\begin{proof}
	The proof is easy if you notice the fact that a linear combination of $\alpha_1,\alpha_2,\dots,\alpha_n$ is just a product of two matrices corresponded to a system of linear equations:
	\[c_1\alpha_1+c_2\alpha_2+\dots+c_n\alpha_n=A\begin{bmatrix}
		c_1\\
		c_2\\
		\vdots\\
		c_n
	\end{bmatrix}\]
	and immediately we finish the proof.
\end{proof}
Let's come back to the topic.One thing you must notice is the term ``relate",which tells us the arbitrariness of the coordinate because of the arbitrary choice of basis.Therefore a natural question is how to change the coordinate when the basis changed.
\begin{thm}
	Let $\{\alpha_i\},\{\alpha'_i\}$ be two bases.$P$ is a matrix whose entries is
	\[\alpha_i=\sum\limits_{j=1}^nP_{ji}\alpha'_j\]Then the two coordinates are related by
	\[X'=PX\]or\[X=P^{-1}X'\]The invertibility of $P$ is guaranteed by Theorem \ref{invertindepend}.\\
	Inversely,an invertible matrix $P$ relate $\{\alpha_i\}$ to an unique basis $\{\alpha'_i\}$ that 
	\[X'=PX\]and\[X=P^{-1}X\].
\end{thm}
The proof is direct.\\
\indent The rest properties of coordinate are left to the next section.Therefore let's discuss the term ``rank".
\begin{dde}
	The \textbf{raw rank} of a matrix $A$ is the dimension of the vector space\footnote{Obviously a subspace of $F^n$} spanned by the row vectors of $A$.The \textbf{column rank} is defined similarly.
\end{dde}
\begin{thm}
	Row-equivalent matrices have the same raw space. 
\end{thm}
\begin{proof}
	Use Theorem \ref{repthm}.
\end{proof}
The similar conclusion about column-equivalent is established as well.
\begin{thm}
	The nonzero raws of raw-reduced echelon matrix $R$ is a basis of the raw space.So as column-reduced echelon matrix.
\end{thm}
If a matrix $R$ is both a raw-reduced echelon matrix and a column-reduced echelon matrix,it's direct to draw the conclusion:
\begin{thm}
	The raw rank is equal to the column rank.Hence we call them the \textbf{rank} of matrix $A$.
\end{thm}
\begin{proof}
	All we have to do is to prove that elementary raw operations do not change the column rank,which is obviously correct when we analyze the linearly independent column vectors.
\end{proof}
We can also call the finite dimension of a space spanned by a vector set the \textbf{rank} of the vector set.\\
\indent Why we introduce the rank of a matrix?You will find the answer in the next section.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{siji}
\end{figure}
\newpage
\section{Linear Transformations and Forms}\label{3}
\subsection{Linear Transformations and Matrices}
\begin{dde}
	Let $V,W$ be vector spaces over $F$.\textbf{A linear transformation} from $V$ to $W$ is a function $T$ from $V$ to $W$ such that
	\[T(c\alpha+\beta)=c(T\alpha)+T\beta\]
	for all $\alpha,\beta$ in $V$ and $c$ in $F$.
\end{dde}
Obviously,the image 
\begin{dde}
	The \textbf{null space} of $T$ is the set of all vectors $\alpha$ in $V$ such that $T\alpha=0$.\\
	If $V$ is finite-dimensional,the \textbf{rank} of $T$ is the dimension of the range of $T$,and the \textbf{nullity} of $T$ is the dimension of the null space of $T$.
\end{dde}
For convenience,we confined ourselves to finite-dimensional vector spaces.We will have an important theorem.
\begin{thm}\label{dimthm}
	rank($T$)+nullity($T$)=$\dim(T)$
\end{thm}
\begin{proof}
	Let $\{\alpha_i\}$ be an ordered basis of the null space $W$ with dimension $m$.Use theorem \ref{repthm} we can expend it to an ordered basis $\{\alpha_{i}\}$ of $V$.Now we prove $T\alpha_i,i=m+1,\dots,n$ is an ordered basis of the range of $V$.
	\[T\alpha=T(x_1\alpha_1+x_2\alpha_2+\dots+x_n\alpha_n)=x_{m+1}T\alpha_{m+1}+\dots+x_nT\alpha_n\]
	Hence they span the range.
	\[c_1T\alpha_{m+1}+\dots+c_{n-m}T\alpha_n=0\]
	\[T(c_1\alpha_{m+1}+\dots+c_{n-m}\alpha_n)=0\rightarrow c_1\alpha_{m+1}+\dots+c_{n-m}\alpha_n\in W\]
	\[c_1=c_2=\cdots=c_{n-m}=0\]
	Hence they are linearly independent.We have finished the proof.
\end{proof}
We have seen the rank in the previous section.We combine them by the theorem below. 
\begin{thm}\label{matrixrepthm}
	A linear transformation is determined uniquely by its action on an ordered basis of $V$,which is connected to a unique matrix when an ordered basis of $W$ is chosen.
\end{thm}
\begin{proof}
	Let $\{\alpha_i\}$ be an ordered basis of $V$.Let $\{\gamma_i\}$ in $W$ satisfy
	\[T\alpha_i=\gamma_i\]
	Obviously,for all $\alpha$ in $V$,we have
	\[T\alpha=\sum\limits_{i=1}^nx_i\beta_i\]
	where $\{x_i\}$ is the coordinate of $\alpha$.This determines $T$ uniquely.\\
	Let $\{\beta_j\}$ be an ordered basis of $W$,we have
	\[T\alpha=\sum\limits_{i=1}^nx_i\sum\limits_{j=1}^mA_{ji}\beta_j=\sum\limits_{j=1}^m\beta_j\sum\limits_{i=1}^nA_{ji}x_i\]
	Let $\{y_j\}$ be the coordinate of $\beta=T\alpha$,we have
	\[y_j=\sum\limits_{i=1}^nA_{ji}x_i\rightarrow Y=AX\]
	which implies that $T$ is connected to the $m\times n$ matrix $A$.
\end{proof}
Apparently,the rank of matrix is just the rank of the linear transformation,because the nullity of $T$ is just the solution space of $AX=0$ with a connection between coordinate $X$ and $\alpha$.\\
For a further discussion,we should focus on the algebra of linear transformations.
\begin{thm}
	Let $V,W,Z$ be vector spaces.$U,T$ is linear transformations from $W$ to $Z$ and from $V$ to $W$ respectively.Then the function $UT$ is a linear transformation from $V$ to $Z$. If $V$ is invertible,the inverse $T^{-1}$ is also a linear transformation from $W$ to $V$.
\end{thm}
Because the matrices representation of linear transformations,we can define the addition and scalar multiplication on the linear transformations naturally to make a vector space of linear transformations from $V$ to $W$ called $\mathcal{L}(V,W)$,whose dimension is $mn$.Obviously,the addition,scalar multiplication and function composition(a sense of multiplication) of linear transformation are corresponded to the ones of matrix.The linear transformations from $V$ to $V$ is denoted by $\mathcal{L}(V)$.We call $T$ non-singular if $T\alpha=0$ implies $\alpha=0$,which implies that the nullity of $T$ is $0$ and $T$ is one-to-one.
\begin{thm}
	$T$ is non-singular if and only if T carries each linearly independent subset of $W$ onto a linearly independent subset of $W$. 
\end{thm}
The proof is one-step.
\begin{thm}\label{invertiblelineartransformation}
	Let $\dim V=\dim W$.$T$ is invertible if and only if $T$ is non-singular or $T$ is onto.
\end{thm}
Use Theorem \ref{dimthm} and the proof is done.
\begin{dde}
	$V$ is isomorphic to $W$ if there exists an one-to-one linear transformation $T$ of $V$ onto $W$,which is called isomorphism.
\end{dde}
Obviously,isomorphism is an equivalence relation on the class of vector spaces.
\begin{thm}
	$V$ is isomorphic to $W$ if and only if they have the same dimension.
\end{thm}
\begin{proof}
	Let a linear transformation connect the ordered bases of $V,W$,then we have done the proof.
\end{proof}
This theorem tells that the matrix of an invertible $T$ is a square matrix.
\begin{thm}
	The linear transformation is invertible if and only if a matrix of it is invertible,and the matrix of it related to the same basis is just the inverse one.
\end{thm}
The proof is direct.
It's important to notice that the $A$ depends on the choice of ordered bases.You may have found that the $P$ matrix of basis transformation from $\{\alpha_i\}$ to $\{\alpha'_i\}$ is just the matrix of a linear transformation $C$ related to an ordered basis of $V$ such that
\[\alpha'_i=C\alpha_i=\sum\limits_{i=1}^nP^{-1}_{ji}\alpha_j\]
We want to use $C$ on $\alpha$.
\[C\alpha=\sum\limits_{i=1}^nx_i\alpha'_i=\sum\limits_{j=1}^n\sum\limits_{i=1}^nP^{-1}_{ji}x_i\alpha_j\rightarrow X''=P^{-1}X\]
If you look it carefully enough,you may find some difference compared to the previous statement.This denotes a different view of basis transformation.The view before is that basis is changed and vectors is unchanged,which is called passive view.The view above is that vectors is changed and basis is unchanged,which is called active view.We will use passive view below.\\
\begin{thm}
	Let $\{\alpha_i\},\{\alpha'_i\}$ be ordered bases of $V$,$T\in\mathcal{L}(V)$.$A,B$ is the matrices of $T$ related to them respectively.Then we have
	\[B=P^{-1}AP\]
	where $X=PX',\alpha'_i=C\alpha_i=\sum\limits_{i=1}^nP_{ji}\alpha_j$
\end{thm}
\begin{proof}
	\[AX=Y\]
	\[P^{-1}APP^{-1}X=P^{-1}Y\]
	\[P^{-1}APX'=BX'=Y'\]
\end{proof}
\begin{dde}
	Let $A,B$ be square matrices.$B$ is \textbf{similar} to $A$ if there exists an invertible matrix $P$ such that
	\[B=P^{-1}AP\]
\end{dde}
We have
\[C=Q^{-1}BQ=Q^{-1}P^{-1}APQ=(PQ)^{-1}A(PQ)\]
Thus,similarity is an equivalent relation on the set of $n\times n$ square matrices,which implies that $B,A$ are corresponded to the same linear transformation related to different ordered bases.
\subsection{Linear Functionals and Forms}
A linear transformation $f$ from $V$ to $F$ is called \textbf{linear functional}.We use $V^*$ to denote $\mathcal{L(V,F)}$.Obviously we have
\[\dim V^*=\dim V\]
If $\{\alpha_i\}$ is an ordered basis of $V$,we have a matrix representation of $f$
\[\begin{bmatrix}
	a_1&a_2&\dots&a_n
\end{bmatrix}\]
where $a_i=f(\alpha_i)$.\\
\indent A natural basis of $ V^*$ comes after,if we define $f_i(\alpha_j)=\delta_{ij}$,which is guaranteed by Theorem \ref{matrixrepthm}.Then we have the theorem below.
\begin{thm}
	If $V$ is a finite-dimensional vector space and has an ordered basis $\{\alpha_i\}$,then there is a unique \textbf{dual basis} $\{f_i\}$ for $V^*$ such that $f_i(\alpha_j)=\delta_{ij}$.For each linear functional $f$ on $V$ we have
	\[f=\sum\limits_{i=1}^nf(\alpha_i)f_i\]
	and for each vector $\alpha$ in $V$ we have
	\[\alpha=\sum\limits_{i=1}^nf_i(\alpha)\alpha_i\]
\end{thm}
The proof is direct.
\begin{dde}
	If $V$ is a vector space over the field $F$ and $S$ is a subset of $V$,the \textbf{annihilator} of $S$ is the set $S^0$ of linear functionals $f$ on $V$ such that $f(\alpha)=0$ for every $\alpha$ in $S$.
\end{dde}
It's clear that $S^0$ is a subspace of $V^*$.
\begin{thm}\label{dimfunc}
	Let $V$ be a finite-dimensional vector space and let $W$ be a subspace of $V$,we have
	\[\dim W+\dim W^0=\dim V\]
\end{thm}
\begin{proof}
	Let $\{\alpha_i\},i=1,2,\dots,k$ be an ordered basis of $W$ and extend it to an ordered basis of $V$.A linear functional $f$ can be written as a linear combination of the dual basis related to $\{\alpha_i\}$
	\[f=\sum\limits_{i=1}^nf(\alpha_i)f_i\]
	For the functionals in $W^0$ we have
	\[\forall \alpha \in W,f(\alpha)=0\rightarrow\sum\limits_{i=1}^kc_if(\alpha_i)=0\]
	Because $c_i$ is arbitrary,we have
	\[f(\alpha_i)=0,i=1,2,\dots,k\]
	Therefore $\{f_i\},i=k+1,k+2,\dots,n$ is an ordered basis of $W^0$ and we have
	\[\dim W+\dim W^0=\dim V\]
\end{proof}
In a vector space of dimension $n$, a subspace of dimension $n-1$ is called a \textbf{hyperspace}.Obviously a hyperspace is a null space of a linear functional.Use the theorem above we have
\begin{coro}\label{hyperinter}
	If $W$ is a $k$-dimensional subspace of an $n$-dimensional vector space $V$,then $W$ is the intersection of $n-k$ hyperspaces in $V$ 
\end{coro}
\begin{proof}
	From the proof above we learn that $W$ is the intersection of null spaces of $f_i,i=k+1,k+2,\dots,n$,then we done the proof.  
\end{proof}
\begin{coro}\label{uniqueW}
	If $W_1,W_2$ are subspaces of finite-dimensional vector space $V$,then $W_1=W_2$ if and only if $W_1^0=W_2^0$.
\end{coro} 
\begin{proof}
	Use Corollary \ref{hyperinter}
\end{proof}
Another proof of Corollary \ref{hyperinter} is that consider $f(\alpha)=0$ as a system of linear equations,the solution space is $n-k$-dimensional with rank of $k$.\\
You may ask if there exits a basis of $V$ dual to a basis of $V^*$.The answer is to consider $V^{**}$.Let $L_\alpha$ be a linear functional on $V^*$ such that
\[L_\alpha(f)=f(\alpha),\alpha\in V\]
It seems to be a connection to the vector space $V$.
\begin{thm}
	Let $V$ be a finite-dimensional vector space over the field $F$.For each vector $\alpha$ in $V$ define
	\[L_\alpha=f(\alpha),f\in V^*\]
	The mapping $\alpha\rightarrow L_\alpha$ is then an isomorphism of $V$ onto $V^{**}$
\end{thm}
\begin{proof}
	It's easy to show that the mapping is a linear transformation and non-singular.Use Theorem \ref{invertiblelineartransformation}.
\end{proof}
\begin{coro}\label{DualDual}
	Let $V$ be a finite-dimensional vector space over the field $F$.If $L$ is a linear functional on the dual space $V^*$ of $V$, then there is a unique vector $\alpha$ in $V$ such that 
	\[L(f)=f(\alpha)\]
	for every $f$ in $V^*$.
\end{coro}
\begin{coro}
	Let $V$ be a finite-dimensional vector space over the field $F$.Each basis for $V^*$ is the dual of some basis for $V$. 
\end{coro}
\begin{proof}
	Each basis for $V^*$ has a dual basis for $V^{**}$.Use Corollary \ref{DualDual},there exists a basis for $V$ such that
	\[L_i(f_j)=f_j(\alpha_i)=\delta_{ij}\]
	whose dual basis is exactly $\{f_i\}$.
\end{proof}
We usually identify $\alpha$ with $L_\alpha$ and say that $V$ is the dual space of $V^*$
\begin{thm}
	If $S$ is a subset of a finite-dimensional vector space $V$,then $(S^0)^0$ is the subspace spanned by $S$.
\end{thm}
\begin{proof}
	Let $W$ be the subspace spanned by $S$.Clearly $W^0=S^0$.$W$ is uniquely determined by $W^0$ according to Corollary \ref{uniqueW}.You may notice that for $\alpha$ in $(S^0)^0$ we have
	\[L_\alpha(f)=f(\alpha)=0\]
	which means $(W^0)^0=W$,according to Theorem \ref{dimfunc}.
\end{proof}
As for infinite-dimensional vector space,we omitted the proof.\\
\indent It's clear that our definition of hyperspace fails when $V$ is infinite-dimensional,so we need another definition.
\begin{dde}
	If $V$ is a vector space,a \textbf{hyperspace} in $V$ is a \textbf{maximal proper subspace} of $V$,which satisfies
	\begin{enumerate}
		\item $N$ is a proper subspace of $V$.
		\item if $W$ is a subspace of $V$ which contains $N$,then either $W=N$ or $W=V$.
	\end{enumerate}
\end{dde}
\begin{thm}
	If $f$ is a non-zero linear functional on the vector space $V$,then the null space of $f$ is a hyperspace of in $V$.Conversely,every hyperspace in $V$ is the null space of a (not unique) nonzero linear functional on $V$.
\end{thm}
\begin{proof}
	Clearly $N_f$ is a proper subspace of $V$.Let $W$ be a subspace of $V$ which contains $N_f$ and $W\neq N_f$.Choose the vector $\alpha\in W\setminus N_f$ and a vector $\beta$ in $V$ such that $f(\alpha),f(\beta)\neq0$,then we define
	\[c=\frac{f(\beta)}{f(\alpha)}\]
	let $\gamma=\beta-c\alpha$,we have
	\[f(\gamma)=f(\beta)-cf(\alpha)=0\]
	which means $\gamma\in N_f$.Therefore $\beta=\gamma+c\alpha\in W$,$W=V$.\\
	Let $N$ be a hyperspace in $V$.Choose a vector $\alpha\in V\setminus N$,we have
	\[\beta=\gamma+c\alpha,\gamma\in N,\beta\in V\]
	The $\gamma$ and $\alpha$ is determined uniquely by $\beta$.If we have
	\[\beta=\gamma'+c'\alpha\]
	then
	\[(c-c')\alpha=\gamma'-\gamma\]
	If $c-c'\neq0$,then $\alpha\in N$.Hence $c=c',\gamma=\gamma'$.We can define $g(\beta)=c$,which is a linear functional on $V$,and then $N$ is the null space of $g$. 
\end{proof}
\begin{lem}
	If $f,g$ are linear functionals on $V$,then $g=cf,c\in F$ if and only if $N_f\subseteq N_g$.
\end{lem}
\begin{proof}
	The `only if' part is trivial.If $f=0$,then $g=0$.Thus we assume $f\neq0$.The null space $N_f$ is a hyperspace of $V$.Choose $\alpha\in V\setminus N_f$ and define
	\[c=\frac{g(\alpha)}{f(\alpha)}\]
	\[h=g-cf\]
	If $\beta\in V$,similar to the proof above we have
	\[h(\beta)=g(\gamma+c'\alpha)-cf(\gamma+c'\alpha)=c'(g(\alpha)-cf(\alpha))=0\]
	Hence $g=cf$
\end{proof}
\begin{thm}
	Let $g,f_1,f_2,\dots,f_r$ be linear functionals on a vector space $V$ with respective null space $N,N_1,N_2,\dots,N_r$.Then $g$ is a linear combination of $f_1,\dots,f_r$ if and only if $N_1\cap N_2\cap\cdots\cap N_r\subseteq N$.
\end{thm}
\begin{proof}
	The `only if' part is trivial.Suppose the theorem holds for $r=k-1$.If we restrict $g,f_1,\dots,f_{k-1}$ on $N_k$ to be $g',f_1',\dots,f_{k-1}'$,it's clear that $N_i'=N_i\cap N_k,N'=N\cap N_k$,we have
	\[N_1\cap\dots\cap N_k=N_1'\cap\dots\cap N_{k-1}'\subseteq N'\]
	then use induction hypothesis
	\[g'=\sum\limits_{i=1}^{k-1}c_if_i'\]
	Now define $h$ such that
	\[h=g-\sum\limits_{i=1}^{k-1}f_i\]
	Then $h(\alpha)=0$ for every $\alpha\in N_k$.By the proceeding lemma we have $h=c_kf_k$,then
	\[g=\sum\limits_{i=1}^kf_i\] 
\end{proof}
Before we turn to discuss multilinear forms,let's talk something about the transpose of a linear transformation.It's easy to define the transpose of a matrix
\[A^T_{ij}=A_{ji}\]
and obviously has the properties
\begin{enumerate}
	\item $(A^T)^T=A$
	\item $(AB)^T=B^TA^T$
\end{enumerate}
\indent To introduce the transpose of a linear transformation,we need the help of dual space.
\begin{thm}
	Let $V,W$ be vector spaces over the field $F$.For each linear transformation $T$ from $V$ into $W$,there is a unique linear transformation $T^T$ from $W^*$ into $V^*$ such that
	\[(T^Tg)\alpha=g(T\alpha)\]
	for every $g$ in $W^*$ and $\alpha$ in $V$. 
\end{thm}
\begin{proof}
	\[(T^T(cg_1+g_2))(\alpha)=(cg_1+g_2)(T\alpha)=cg_1(T\alpha)+g_2(T\alpha)\]
	\[T^T(cg_1+g_2)=cT^Tg_1+T^Tg_2\]
	The uniqueness is easy to prove.
\end{proof}
We call $T^T$ be the \textbf{transpose} of $T$.$T^T$ is also called the adjoint of $T$.
\begin{thm}
	Let $V,W$ be vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$.The null space of $T^T$ is the annihilator of the range of $T$.If $V,W$ is finite-dimensional,then
	\begin{enumerate}
		\item[\romannumeral1] $rank(T^T)=rank(T)$
		\item[\romannumeral2] the range of $T^T$ is the annihilator of the null space of $T$.
	\end{enumerate}
\end{thm}
\begin{proof}
	Use the definition
	\[(T^Tg)\alpha=g(T\alpha)\]
	It's clear that the null space of $T^T$ is precisely the annihilator of the range of $T$.If $V,W$ are finite-dimensional,use Theorem \ref{dimthm} and Theorem \ref{dimfunc} we have
	\[rank(T^T)=\dim W^*-nullity(T^T)\]
	\[nullity(T^T)=\dim W-rank(T)\]
	\[\dim W=\dim W^*\rightarrow rank(T^T)=rank(T)\] 
	If $\alpha\in N_T$ then we have
	\[(T^Tg)\alpha=g(T\alpha)=0\]
	implies that the range of $T^T$ is a subset of the annihilator of $N_T$.We have
	\[rank(T^T)=rank(T)=\dim V-nullity(T)=\dim N_T^0\]
	Thus the range of $T^T$ is precisely the annihilator of the null space of $T$.
\end{proof}
We know the relation between matrices and linear transformations,thus we have
\begin{thm}
	Let $V,W$ be vector spaces over the field $F$ and let $T$ be a linear transformation from $V$ into $W$.Let $\{\alpha_i\},\{\beta_i\}$ be ordered bases for $V,W$ and let $\{f_i\},\{g_i\}$ be the dual bases of them.Let $A$ be the matrix of $T$ relative to $\{\alpha_i\},\{\beta_i\}$ and let $B$ be the matrix of $T^T$ relative to $\{f_i\},\{g_i\}$.We have
	\[B^T=A\] 
\end{thm}
\begin{proof}
	\[B_{ji}=(T^Tg_i)\alpha_j=g_i(T\alpha_j)=A_{ij}\]
\end{proof}
Use this we can prove a theorem which we have proven by a different method before.
\begin{thm}
	The raw rank is equal to the column rank.
\end{thm}
Let's turn to the topic.
\begin{dde}
	A \textbf{multilinear function} (tensor) is a function from $V^r=V\times V\times\cdots\times V$\footnote{You can define the natural addition and scalar multiplication on $V^r$ like $R^n$} into $F$,which satisfies
	\[L(\alpha_1,\dots,c\alpha_i+\beta_i,\dots,\alpha_r)=cL(\alpha_1,\dots,\alpha_i,\dots,\alpha_r)+L(\alpha_1,\dots,\beta_i,\dots,\alpha_r)\]
	for each $i$ and every $\alpha\in V$.We have the natural definition of addition and scalar multiplication
	\[(L+M)(\alpha_1,\dots,\alpha_r)=L(\alpha_1,\dots,\alpha_r)+M(\alpha_1,\dots,\alpha_r)\]
	\[(cL)(\alpha_1,\dots,\alpha_r)=cL(\alpha_1,\dots,\alpha_r)\]
	Then the collection of all multilinear functions on $V$ is a vector space denoted by $\mathcal{M}^r(V)$. 
\end{dde}
We want to construct an ordered basis of $\mathcal{M}^r(V)$,we need introduce a concept.
\begin{dde}
	The tensor product of $L,M$ is a multilinear function on $V^{r+s}$ such that
	\[L\otimes M(\alpha_1,\dots,\alpha_{r+s})=L(\alpha_1,\dots,\alpha_r)M(\alpha_{r+1},\dots,\alpha_{r+s})\]
\end{dde}
\begin{thm}
	For $L\in\mathcal{M}^r(V),M\in\mathcal{M}^s(V)$,we have
	\begin{enumerate}
		\item $(cL_1+L_2)\otimes M=cL_1\otimes M+L_2\otimes M$
		\item $L\otimes(cM_1+M_2)=cL\otimes M_1+L\otimes M_2$
		\item $L\otimes(M\otimes N)=(L\otimes M)\otimes N$
	\end{enumerate}
\end{thm}
\begin{thm}
	Let $V$ be a finite-dimensional vector space and let $\{\alpha_i\}$ be an ordered basis for $V$.Let $\{f_i\}$ be the dual basis for $V^*$.Then
	\[f_{j_1}\otimes f_{j_2}\otimes\dots\otimes f_{j_r},1\le j_1\le n,\dots,1\le j_r\le n\]
	form a basis for $\mathcal{M}^r(V)$.
\end{thm}
\begin{proof}
	\[L(\alpha)=\sum\limits_{j_1,j_2,\dots,j_r=1}^nA_{1j_1}A_{2j_2}\cdots A_{rj_r}L(\alpha_{j_1},\alpha_{j_2},\dots,\alpha_{j_r})\]
	We shall prove
	\[L=\sum\limits_{j_1,j_2,\dots,j_r=1}^nL(\alpha_{j_1},\alpha_{j_2},\dots,\alpha_{j_r})f_{j_1}\otimes f_{j_2}\otimes\dots\otimes f_{j_r}\]
	which is apparently correct.The linearly independence is easy to prove.
\end{proof}
\begin{dde}
	Let $L$ be a multilinear function on $V^r$.We say that $L$ is alternating if $L(\alpha_1,\dots,\alpha_r)=0$ whenever $\alpha_i=\alpha_j,|i-j|=1$.
\end{dde}
\begin{thm}
	An alternating multilinear function has the properties:
	\begin{enumerate}
		\item $L(\alpha_1,\dots,\alpha_r)=0$ whenever $\alpha_i=\alpha_j,i\neq j$
		\item $L(\alpha_1,\dots,\alpha_i,\dots,\alpha_j,\dots,\alpha_r)=-L(\alpha_1,\dots,\alpha_j,\dots,\alpha_i,\dots,\alpha_r)$
	\end{enumerate}
\end{thm}
\begin{proof}
	Let $i<j$.If $j=i+1$,we have
	\begin{align}
		L(\alpha_1,\dots,\alpha_i+\alpha_j,\alpha_i+\alpha_j,\dots,\alpha_r)&=L(\alpha_1,\dots,\alpha_i,\alpha_i,\dots,\alpha_r)+L(\alpha_1,\dots,\alpha_i,\alpha_j,\dots,\alpha_r)\notag\\
		&+L(\alpha_1,\dots,\alpha_j,\alpha_i,\dots,\alpha_r)+L(\alpha_1,\dots,\alpha_j,\alpha_j,\dots,\alpha_r)=0\notag
	\end{align}
	\[L(\alpha_1,\dots,\alpha_i,\alpha_j,\dots,\alpha_r)=-L(\alpha_1,\dots,\alpha_j,\alpha_i,\dots,\alpha_r)\]
	If $j>i+1$,we need commute one-by-one.Suppose $k=j-i$,then we need commute $2k-1$ times.Thus we have
	\begin{align}
		L(\alpha_1,\dots,\alpha_i,\dots,\alpha_j,\dots,\alpha_r)&=(-1)^{2k-1}L(\alpha_1,\dots,\alpha_j,\dots,\alpha_i,\dots,\alpha_r)\notag\\
		&=-L(\alpha_1,\dots,\alpha_j,\dots,\alpha_i,\dots,\alpha_r)\notag
	\end{align}
	If $\alpha_i=\alpha_j,i\neq j$,we commute to make them adjacent,then we have \[L(A)=-L(B)=0\]
\end{proof}
\begin{dde}
	For an ordered $n$-tuple $a_1a_2\cdots a_n$,each pair of number forms an ordered pair.Let $j=i+1$,if $a_i>a_i$,then we call this pair an \textbf{inversion}.The number of all inversion is called the \textbf{inversion number} of the $n$-tuple.
\end{dde}
\begin{dde}
	A permutation is called an \textbf{odd permutation} if its inversion number is odd,and is called an \textbf{even permutation} if its inversion number is even.
\end{dde}
\begin{thm}
	Transposition change the parity of a permutation.
\end{thm}
The proof is easy so we omitted it.
\begin{thm}
	A permutation is a product of transpositions.
\end{thm}
\begin{proof}
	We can use transposition to change a permutation to $123\cdots n$,and then the inverse is also a product of transpositions. 
\end{proof}
Thus,The parity of the number of transpositions you need to change a permutation to $123\cdots n$ is unique,because the parity of a permutation is unique.Therefore we can define the signal of permutations.
\begin{dde}
	The signal of a permutation is $1$,if it's an even permutation,and is $-1$,if it's an odd permutation.
\end{dde}
This also tells us that $L(\alpha_{\sigma(1)},\alpha_{\sigma(2)},\dots,\alpha_{\sigma(r)})=sgn(\sigma)L(\alpha_1,\dots,\alpha_r)$,and the definition about alternating is legitimate because of the uniqueness of the parity of the number of transpositions the permutation has.\\
\begin{dde}
	A multilinear function $L$ on $V^r$ is called a linear $r$-form if it is alternating.
\end{dde}
The collection of all $r$-forms is obviously a subspace of $\mathcal{M}^r(V)$,we denote it by $\bigwedge^r(V)$.
\begin{thm}
	If $L$ is a non-zero $r$-form,then $r\le n$.The dimension of $\bigwedge^r(V),1\le r\le n$ is $\dbinom{n}{r}$.
\end{thm}
\begin{proof}
	There exist $\dbinom{n}{r}$ $r$-shuffle,which is a $r$-tuple $j_1j_2\cdots j_r$ with $j_1<j_2<\cdots<j_r$.For the coordinate of $f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}$\footnote{We only consider permutations because the alternating property},we have
	\[L(\alpha_{j_{\sigma(1)}},\dots,\alpha_{j_{\sigma(r)}})=sgn(\sigma)L(\alpha_{j_1},\dots,\alpha_{j_r})\]
	and therefore we have
	\[L=\sum\limits_{shuffles-J}L(\alpha_{j_1},\dots,\alpha_{j_r})D_J\]
	\[D_J=\sum\limits_\sigma sgn(\sigma)f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\]
	$D_J$ are obviously linearly independent,hence forms a basis of $\bigwedge^r(V)$.We have
	\[L=\sum\limits_Jc_JD_J\]
\end{proof}
\begin{dde}
	$\pi_r$ is a linear transformation from $\mathcal{M}^r(V)$ into $\bigwedge^r(V)$ such that
	\[\pi_r(L)=\sum\limits_\sigma sgn(\sigma)L_\sigma\]
	\[L_\sigma(\alpha_1,\dots,\alpha_r)=L(\alpha_{\sigma(1)},\dots,\alpha_{\sigma(r)})\]
	If $L\in\bigwedge^r(V)$,$\pi_r(L)=r!L$
\end{dde}
We can write $D_J$ as $\pi_r(f_{j_1}\otimes\cdots\otimes f_{j_r})$\footnote{You may notice that the existence of alternating multilinear functions is proven when we construct the $D_J$}.The proof is easy.
\begin{coro}
	If $V$ is a $n$-dimensional vector space,then $\bigwedge^n(V)$ is a vector space with dimension $1$.If $T$ is a linear transformation on $V$,then there exists a unique $c$ such that
	\[L(T\alpha_1,\dots,T\alpha_n)=cL(\alpha_1,\dots,\alpha_n)\]
	for all $L\in\bigwedge^n(V)$
\end{coro}
\begin{proof}
	Apparently,define $L_T(\alpha_1,\dots,\alpha_n)=L(T\alpha_1,\dots,T\alpha_n)$ and $L_T$ is a $n$-form.For a basis $M$,we have $L=aM$ and $M_T=cM$,then
	\[L_T=(aM)_T=aM_T=c(aM)=cL\]
	You may notice that $D_J$ here is just $\pi_n(f_{1}\otimes\cdots\otimes f_{n})$,and we have
	\[c=\sum\limits_\sigma sgn(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)}\]
	where $A$ is the matrix of $T$ related to the basis $\{\alpha_i\}$.We call $c$ the \textbf{determinant} of the transformation $T$ and denote it as $\det T$.It's clear that the matrix determinant comes after,and is independent of the basis.
\end{proof}
\begin{thm}
	The determinant has some important properties:
	\begin{enumerate}
		\item $\det AB=\det A\det B$
		\item The determinant of upper-triangle matrix is the product of diagonal elements $a_{11}\cdots a_{nn}$
		\item $\det A^T=\det A$
	\end{enumerate}
\end{thm}
\begin{proof}
	The last two statements are easy.We prove the first.
	\begin{align}
		\det AB&=\sum\limits_{\sigma}sgn(\sigma)(\sum\limits_{k_1=1}^na_{1k_1}b_{k_1\sigma(1)})\cdots(\sum\limits_{k_n=1}^na_{nk_n}b_{k_n\sigma(n)})\notag\\
		&=\sum\limits_{\sigma}\sum\limits_{k_1=1}^n\cdots\sum\limits_{k_n=1}^nsgn(\sigma)a_{1k_1}\cdots a_{nk_n}b_{k_1\sigma(1)}\cdots b_{k_n\sigma(n)}\notag
	\end{align}
	Every term has $k_i=k_j$ cancels because of $sgn(\sigma)$,thus $k_1,k_2,\dots,k_n=\tau(1),\dots,\tau(n)$.
	\begin{align}
		\det AB&=\sum\limits_{\sigma}\sum\limits_{k_1=1}^n\cdots\sum\limits_{k_n=1}^nsgn(\sigma)a_{1k_1}\cdots a_{nk_n}b_{k_1\sigma(1)}\cdots b_{k_n\sigma(n)}\notag\\
		&=\sum\limits_{\sigma}\sum\limits_{\tau}sgn(\sigma)a_{1\tau(1)}\cdots a_{n\tau(n)}b_{\sigma^{-1}\tau(1)1}\cdots b_{\sigma^{-1}\tau(n)n}\notag\\
		&=\sum\limits_{\sigma}\sum\limits_{\tau}sgn(\sigma^{-1}\tau)sgn(\tau)a_{1\tau(1)}\cdots a_{n\tau(n)}b_{\sigma^{-1}\tau(1)1}\cdots b_{\sigma^{-1}\tau(n)n}\notag\\
		&=\det A\det B\notag
	\end{align}
\end{proof}
\begin{thm}
	Laplace Expansion
	\[\det A=\sum\limits_{j=1}^n(-1)^{i+j}a_{ij}\det A(i|j)=\sum\limits_{i=1}^n(-1)^{i+j}a_{ij}\det A(i|j)\]
	where $(-1)^{i+j}\det A(i|j)$ is usually called the $i,j$ \textbf{cofactor} of $A$.If we set
	\[C_{ij}=(-1)^{i+j}\det A (i|j)\]
	then we have
	\[\det A=\sum\limits_{j=1}^na_{ij}C_{ij}=\sum\limits_{i=1}^na_{ij}C_{ij}\]
	where the cofactor $C_{ij}$ is $(-1)^{i+j}$ times the determinant of the $(n-1)\times(n-1)$ matrix obtained by deleting the $i$th row and $j$th column of $A$.\\
	we also have
	\[\sum\limits_{j=1}a_{ij}C_{kj}=\delta_{ik}\det A\]  
	\[\sum\limits_{i=1}a_{ij}C_{ik}=\delta_{jk}\det A\]
	If we define the classical adjoint $adj\,A$ of $A$ such that
	\[(adj\,A)_{ij}=C_{ji}\]
	then we have
	\[(adj\,A)A=A(adj\,A)=(\det A)I\]
	and an explicit expression of $A^{-1}$
	\[A^{-1}=\frac{1}{\det A}adj\,A\]
\end{thm}
\begin{proof}
	\begin{align*}
		\det A&=\sum\limits_{\sigma}sgn(\sigma)a_{1\sigma(1)}\cdots a_{n\sigma(n)}\\
		&=\sum\limits_{j=1}^na_{ij}\sum\limits_{\sigma'}sgn(\sigma'(1)\cdots j\cdots\sigma'(n))a_{1\sigma'(1)}\cdots\widehat{a_{ij}}\cdots a_{n\sigma'(n)}\\
		&=\sum\limits_{j=1}^n(-1)^{i-1}a_{ij}\sum\limits_{\sigma'}sgn(j\sigma'(1)\cdots\sigma'(n))a_{1\sigma'(1)}\cdots\widehat{a_{ij}}\cdots a_{n\sigma'(n)}\\
		&=\sum\limits_{j=1}^n(-1)^{i-1+j-1}a_{ij}\sum\limits_{\sigma'}sgn(\sigma')a_{1\sigma'(1)}\cdots\widehat{a_{ij}}\cdots a_{n\sigma'(n)}\\
		&=\sum\limits_{j=1}^n(-1)^{i+j}a_{ij}\det A(i|j)
	\end{align*}
	The second equation can be proven similarly.If we expand the matrix by different raw or column,it's just the same expansion of a matrix with two same raws or columns,and the determinant of it is $0$. 
\end{proof}
This theorem tells us that if and only if $\det A$ is nonzero,$A$ is invertible.In fact,we can reach this conclusion if you notice that $\det A$ is nonzero if and only if $A$ is raw-equivalent with $I$,which implies a unique solution of the system of linear equations with coefficient matrix $A$.Then use Theorem \ref{determinantandinvertible} we reach the conclusion.\\
\indent A simpler proof of the theorem will use the concept we introduce below.
\begin{dde}
	We call ``$\wedge$" wedge product,and write $D_J$ as $f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r}$,which implies that
	\[f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r}=\sum\limits_{\sigma}sgn(\sigma)f_{j_{\sigma(1)}}\otimes f_{j_{\sigma(2)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\] 
\end{dde}
Recall the properties of tensor product,we want the ``wedge product" to have similar properties,namely
\[(f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r})\wedge(f_{k_1}\wedge f_{k_2}\wedge\cdots\wedge f_{k_s})=f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r}\wedge f_{k_1}\wedge f_{k_2}\wedge\cdots\wedge f_{k_s}\]
To reach this,we first expand the RHS:
\begin{align*}
	&f_{j_1}\wedge f_{j_2}\wedge\cdots\wedge f_{j_r}\wedge f_{k_1}\wedge f_{k_2}\wedge\cdots\wedge f_{k_s}\\
	&=\sum\limits_{\sigma}sgn(\sigma)f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\otimes f_{j_\sigma(r+1)}\otimes\cdots\otimes f_{j_{\sigma(r+s)}}\footnotemark[8]
\end{align*}
\footnotetext[8]{For convenience,we denote $k_1,\dots,k_s$ by $j_{r+1},\dots,j_{r+s}$}
Let's divide $\sigma$ into two situation:
\begin{enumerate}
	\item $\sigma=(\sigma_1,\sigma_2)$,where $\sigma_1,\sigma_2$ are permutations of $1,\dots,r$ and $r+1,\dots,r+s$ respectively.
	\item $\sigma$ can't be presented by a product of two permutations of $1,\dots,r$ and $r+1,\dots,r+s$.
\end{enumerate}
For the first situation,we have
\begin{align*}
	&\sum\limits_{some\;\sigma}sgn(\sigma)f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\otimes f_{j_{\sigma(r+1)}}\otimes\cdots\otimes f_{j_{\sigma(r+s)}}\\
	&=\sum\limits_{some\;\sigma}sgn(\sigma_1)sgn(\sigma_2)f_{j_{\sigma_1(1)}}\otimes\cdots\otimes f_{j_{\sigma_1(r)}}\otimes f_{j_{\sigma_2(r+1)}}\otimes\cdots\otimes f_{j_{\sigma_2(r+s)}}\\
	&=f_{j_1}\wedge\cdots\wedge f_{j_r}\otimes f_{k_1}\wedge\cdots\wedge f_{k_s}
\end{align*}
You may notice that the second situation is just the first situation with a combination of $1,2,\dots,r+s$.Thus we can write
\begin{align*}
	&\sum\limits_{\sigma}sgn(\sigma)f_{j_{\sigma(1)}}\otimes\cdots\otimes f_{j_{\sigma(r)}}\otimes f_{j_{\sigma(r+1)}}\otimes\cdots\otimes f_{j_{\sigma(r+s)}}\\
	&=\sum\limits_{shuffles-J_1,J_2}f_{t_1}\wedge\cdots\wedge f_{t_r}\otimes f_{u_1}\wedge\cdots\wedge f_{u_s}
\end{align*} 
where $t_1,\dots,t_r,u_1,\dots,u_s$ are shuffles of $j_1,\dots,j_r,k_1,\dots,k_s$ respectively.\\
Like the definition of tensor product,we can define wedge product as below:
\begin{dde}
	If $L\in\bigwedge^r(V),M\in\bigwedge^s(V)$,then the wedge product of $L,M$ is
	\begin{align*}
		L\wedge M(\alpha_1,\dots,\alpha_{r+s})&=\sum\limits_{shuffles-J_1,J_2}L(\alpha_{j_1},\dots,\alpha_{j_r})M(\alpha_{k_1},\dots,\alpha_{k_s})\\
		&=\frac{1}{r!s!}\sum\limits_{\sigma}sgn(\sigma)L(\alpha_{\sigma(1)},\dots,\alpha_{\sigma(r)})M(\alpha_{\sigma(r+1)},\dots,\alpha_{\sigma(r+s)})
	\end{align*}
\end{dde}
The second equation is derived from $\pi_r(L)=r!L$.This definition is just the above one,if you notice that
\[sgn(\sigma)f_{\sigma(1)}\otimes\cdots\otimes f_{\sigma(r)}(\alpha_1,\dots,\alpha_r)=sgn(\sigma^{-1})f_1\otimes\cdots\otimes f_r(\alpha_{\sigma^{-1}(1)},\dots,\alpha_{\sigma^{-1}(r)})\]
and use the second equation to prove.\\
\indent The wedge product has some properties,which we omitted the proof
\begin{thm}
	If $L\in\bigwedge^r(V),M\in\bigwedge^s(V)$,then
	\begin{enumerate}
		\item $L\wedge M\in\bigwedge^{r+s}(V)$
		\item $(cL_1+L_2)\wedge M=cL_1\wedge M+L_2\wedge M$
		\item $L\wedge(M\wedge N)=(L\wedge M)\wedge N$
		\item $L\wedge M=(-1)^{rs}M\wedge L$
	\end{enumerate}
\end{thm}
The last one implies that
\[f_{\sigma(1)}\wedge\cdots\wedge f_{\sigma(r)}=sgn(\sigma)f_1\wedge\cdots\wedge f_r\]
\indent Having the wedge product,we can calculate determinant in a new way.
\begin{thm}
	If $A$ is a square matrix,define $\mathcal{A}$ such that
	\[\mathcal{A}=\alpha_1\wedge\alpha_2\wedge\cdots\wedge\alpha_n\]
	where $\alpha_i=\sum\limits_{j=1}^na_{ji}f_j$.Then we have
	\[\mathcal{A}=(\det A)f_1\wedge f_2\wedge\cdots\wedge f_n\]
\end{thm}
The proof is easy.Use this expression of determinant,we can reproof previous conclusions.\\
First we prove $\det AB=\det A\det B$.Let $\{e_i\}$ be an ordered basis for $V$.Define $D\in\mathcal(M)(V^n)$ as
\[D(\beta_1,\beta_2,\dots,\beta_n)=\det (A\beta_1,A\beta_2,\dots,A\beta_n)=\det AB\]
where $\beta_i=\sum\limits_{j=1}^nb_{ji}f_j,A\beta_i=\sum\limits_{j=1}^n\sum\limits_{k=1}^na_{jk}b_{ki}f_j$.\\
Obviously,$D\in\bigwedge^n(V)$.We have
\[D=D(e_1,e_2,\dots,e_n)f_1\wedge\cdots\wedge f_n=(\det(\alpha_1,\alpha_2,\dots,\alpha_n))f_1\wedge\cdots\wedge f_n=(\det A)f_1\wedge\cdots\wedge f_n\]
where $\alpha_i$ is the column vector of $A$.Hence
\[D(\beta_1,\dots,\beta_n)=\det AB=(\det A)f_1\wedge\cdots\wedge f_n(\beta_1,\dots,\beta_n)=\det A\det B\]
Second we prove $\det A=\sum\limits_{j=1}^n(-1)^{i+j}a_{ij}\det A(i|j)=\sum\limits_{i=1}^n(-1)^{i+j}a_{ij}\det A(i|j)$.\\
\begin{align*}
	\mathcal{A}&=\alpha_1\wedge\cdots\wedge\alpha_n\\
	&=\sum\limits_{i=1}^na_{ij}\alpha_1\wedge\cdots\wedge f_i\wedge\cdots\wedge\alpha_n\\
	&=\sum\limits_{i=1}^n(-1)^{j-1}a_{ij}f_i\wedge\alpha_1\wedge\cdots\wedge\alpha_n\\
	&=\sum\limits_{i=1}^n(-1)^{j-1}a_{ij}(\det A(i|j))f_i\wedge f_1\wedge\cdots\hat{f_i}\cdots\wedge f_n\\
	&=\sum\limits_{i=1}^n(-1)^{i+j}a_{ij}(\det A(i|j))f_1\wedge\cdots\wedge f_n\\
	&=(\det A)f_1\wedge\cdots\wedge f_n
\end{align*}
The raw expansion can be proven similarly.\\
\indent Actually,we have introduced an strong concept,which can show its strength in the differential forms.\\ 
\indent Next section we will talk about polynomials to prepare the later sections.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{Rubisama1}
\end{figure}
\newpage
\section{Polynomials}\label{4}
\subsection{The Algebra of Polynomials}
\noindent It's tempting to build the polynomials by linear algebra,thus we introduce the concept.
\begin{dde}
	Let $F$ be a field.\textbf{A linear algebra over the field} $F$ is a vector space $\mathcal{A}$ over $F$ with an additional operation called \textbf{multiplication of vectors} which associates with each pair of $\alpha,\beta$ in $\mathcal{A}$ a vector $\alpha\beta$ in $\mathcal{A}$ called the \textbf{product} of $\alpha$ and $\beta$ in such a way that
	\begin{enumerate}
		\item [(a)]multiplication is associative,
		\[\alpha(\beta\gamma)=(\alpha\beta)\gamma\]
		\item [(b)]multiplication is distributive with respect to addition,
		\[\alpha(\beta+\gamma)=\alpha\beta+\alpha\gamma\text{ and }(\alpha+\beta)\gamma=\alpha\gamma+\beta\gamma\]
		\item [(c)]for each scalar $c$ in $F$,
		\[c(\alpha\beta)=(c\alpha)\beta=\alpha(c\beta)\]
	\end{enumerate} 
	If there exists an element $1$ in $\mathcal{A}$ such that $1\alpha=\alpha1=\alpha$ for each $\alpha$ in $\mathcal{A}$,we call $\mathcal{A}$ a \textbf{linear algebra with identity over} $F$,and call $1$ the \textbf{identity} of $\mathcal{A}$.The algebra $\mathcal{A}$ is called commutative if $\alpha\beta=\beta\alpha$ for all $\alpha,\beta$ in $\mathcal{A}$. 
\end{dde}
A linear algebra we concern in this section is $F^\infty$,a vector space we have mentioned in Section \ref{2}.We can write an element of $F^\infty$ by $f=(f_0,f_1,f_2,\dots)$ and define the addition and scalar multiplication as usual
\[af+bg=(af_0+bg_0,af_1+bg_1,af_2+bg_2,\dots)\]
We define a multiplication of vectors on $F^\infty$
\[(fg)_n=\sum\limits_{i=0}^nf_{i}g_{n-i}\]
Thus
\[(gf)_n=\sum\limits_{i=0}^ng_{i}f_{n-i}=\sum\limits_{i=0}^nf_{i}g_{n-i}=(fg)_n\]
\begin{align*}
	(f(gh))_n&=\sum\limits_{j=0}^nf_j\sum\limits_{i=0}^{n-j}g_ih_{n-j-i}\\
	&=\sum\limits_{j=0}^n\sum\limits_{k=0}^{n-j}f_jg_{n-j-k}h_k\\
	&=\sum\limits_{k=0}^nh_k\sum\limits_{j=0}^{n-k}f_jg_{n-j-k}\\
	&=((fg)h)_n
\end{align*}
The distributive law is apparent.We also have
\[1=(1,0,0,\dots)\]
is the identity.Therefore,$F^\infty$ with the operations defined above is a commutative linear algebra with identity over $F$.\\
we shall denote $(0,1,0,\dots)$ as $x$,and you may find
\[x^2=(0,0,1,0,\dots),x^3=(0,0,0,1,0,\dots)\]
Thus we have $(x^k)_n=\delta_{nk}$.Obviously,the set $\{1,x,x^2,\dots\}$ is linearly independent and infinite,which implies that $F^\infty$ is infinite-dimensional.One thing you should remember is that this set is \textbf{not} a basis of $F^\infty$.\\
\indent The algebra $F^\infty$ is sometimes called the \textbf{algebra of formal power series} over $F$.We shall denote $f$ as
\[f=\sum\limits_{n=0}^\infty f_nx^n\]
which is just a formally notation without anything about ``infinite sums". 
\begin{dde}
	Let $F[x]$ be the subspace of $F^\infty$ spanned by the vectors $1,x,x^2,\dots$.An element of $F[x]$ is called a \textbf{polynomials} over $F$.
\end{dde}
Since an element in $F[x]$ is a (finite) linear combination of $1,x,x^2,\dots$,we can find the maximum of $n$ with nonzero $f_n$,and call it the \textbf{degree} of $f$.We denote it by $\deg f$,and do not assign a degree to the $0$-polynomial.We can write $f$ in the manner
\[f=f_01+f_1x+\cdots+f_nx^n,f_n\neq0\]
We call $f_i$ the \textbf{coefficients} of $f$.We shall call $c1$ \textbf{scalar polynomials},and frequently denote it $c$\footnote{We will denote $1$ as $x^0$ in context}.A nonzero $f$ of degree $n$ such that $f_n=1$ is called a \textbf{monic} polynomial.
\begin{thm}
	Let $f,g$ be nonzero polynomials over $F$,then we have
	\begin{enumerate}
		\item [(\rmnum{1})]$fg$ is a nonzero polynomial
		\item [(\rmnum{2})]$\deg (fg)=\deg f+\deg g$
		\item [(\rmnum{3})]$fg$ is a monic polynomial if both $f$ and $g$ are monic polynomials
		\item [(\rmnum{4})]$fg$ is a scalar polynomial if and only if both $f$ and $g$ are scalar polynomials
		\item [(\rmnum{5})]if $f+g\neq0$,
		\[\deg (f+g)\le\max\{\deg f,\deg g\}\]
	\end{enumerate}
\end{thm}
\begin{proof}
	Let $m,n$ be degrees of $f,g$.Obviously,we have
	\[(fg)_{m+n}=f_mg_n\]
	and
	\[(fg)_{m+n+k}=0,k>0\]
	Thus (\rmnum{1}),(\rmnum{2}),(\rmnum{3}),(\rmnum{4}) follow immediately.The proof of (\rmnum{5}) is easy.
\end{proof}
\begin{coro}
	$F[x]$ is a commutative linear algebra with identity over $F$.
\end{coro}
\begin{coro}
	If $f$ is a nonzero polynomial,then $fg=fh$ implies $g=h$.
\end{coro}
We can define the \textbf{division} of two polynomials.
\begin{dde}
	Let $g$ be a nonzero polynomials.Then we define
	\[h=\frac{f}{g}\]
	if $f=hg$.The uniqueness of $h$ is apparent.
\end{dde}
You may notice that we can also write $fg$ as 
\[fg=\sum\limits_{i,j}f_ig_jx^{i+j},0\le i\le m,0\le j\le n\]
\begin{dde}
	Let $\mathcal{A}$ be a linear algebra with identity over $F$.We shall denote the identity of $\mathcal{A}$ by $1$ and make the convention that $\alpha^0=1$ for each $\alpha$ in $\mathcal{A}$.Then to each polynomial $f=\sum\limits_{i=0}^nf_ix^i$ over $F$ and $\alpha$ in $\mathcal{A}$ we associate an element $f(\alpha)$ in $\mathcal{A}$ by the rule
	\[f(\alpha)=\sum\limits_{i=0}^nf_i\alpha^i\]
\end{dde}
\begin{thm}
	Let $F$ be a field and $\mathcal{A}$ be a linear algebra with identity over $F$.Suppose $f,g$ are polynomials over $F$,$\alpha$ is an element of $\mathcal{A}$,and $c$ is a scalar,then we have
	\begin{enumerate}
		\item [(\rmnum{1})](cf+g)($\alpha$)=cf($\alpha$)+g($\alpha$)
		\item [(\rmnum{2})](fg)($\alpha$)=f($\alpha$)g($\alpha$)
	\end{enumerate}
\end{thm}
We notice that $F$ itself is a linear algebra with identity over $F$.Let $V$ be the subspace of $F[x]$ consisting of all polynomials of degree less than or equal to $n$ (together with $0$-polynomial).Obviously,$\{1,x,\dots,x^n\}$ is a basis for $V$.Let $t_0,t_1,\dots,t_n$ be elements of $F$.We define $P_i$ such that
\[P_i=\prod_{j\neq i}\frac{x-t_j}{t_i-t_j}\]
Then we have
\[P_i(t_j)=\delta_{ij}\]
Let $f=\sum\limits_{i=0}^nc_iP_i$,then
\[f(t_j)=c_j\]
Let $f$ be $0$ then we prove the linearly independence of $\{P_i\}$.Since the dimension of $V$ is $n+1$,$\{P_i\}$ is a basis for $V$ and we have
\[f=\sum\limits_{i=0}^nf(t_i)P_i\]
We can define the polynomial function $\tilde{f}$ form $F$ to $F$ by
\[\tilde{f}(x)=f(x)\]
and define similar addition and multiplication like polynomials over $F$
\[(\tilde{f}\tilde{g})(x)=\tilde{f}(x)\tilde{g}(x)\]
Apparently,we have
\[\widetilde{fg}=\tilde{f}\tilde{g}\]
and then the polynomial functions on $F$ is a linear algebra with identity over $F$.
\begin{dde}
	Let $F$ be a field and let $\mathcal{A}$ and $\tilde{\mathcal{A}}$ be linear algebras over $F$.The algebras $\mathcal{A}$ and $\tilde{\mathcal{A}}$ are said to be \textbf{isomorphic} if there is a one-to-one mapping $\alpha\rightarrow\tilde{\alpha}$ of $\mathcal{A}$ onto $\tilde{\mathcal{A}}$ such that
	\begin{enumerate}
		\item [(a)]$\widetilde{(c\alpha+d\beta)}=c\tilde{\alpha}+d\tilde{\beta}$
		\item [(b)]$\widetilde{\alpha\beta}=\tilde{\alpha}\tilde{\beta}$
	\end{enumerate}
	for all $\alpha,\beta$ in $\mathcal{A}$ and all scalars $c,d$ in $F$.The mapping $\alpha\rightarrow\tilde{\alpha}$ is called an \textbf{isomorphism} of $\mathcal{A}$ onto $\tilde{\mathcal{A}}$.
\end{dde}
\begin{thm}
	If $F$ is a field containing an infinite number of distinct elements,the mapping $f\rightarrow\tilde{f}$ is an isomorphism of the algebra of polynomials over $F$ onto the algebra of polynomial functions over $F$. 
\end{thm}
\begin{proof}
	We just need to prove the mapping is one-to-one,which implies we just need to prove $\tilde{f}=0$ means $f=0$.Suppose $f$ is a polynomial of degree $n$ or less (or $0$-polynomial),which obviously is an element of $V$.Choose different $t_i,i=1,2,\dots,n$,and let $\{P_i\}$ be a basis for $V$,then we have
	\[f(t_i)=\tilde{f}(t_i)=0,i=1,2,\dots,n\]
	Thus $f=0$ when $\tilde{f}=0$.
\end{proof}
\subsection{Polynomial Ideals}
\begin{thm}
	Suppose $f,d$ are nonzero polynomials over $F$ such that $\deg d\le\deg f$,then there exists a unique polynomial $g$ such that
	\[f-dg=0\text{ or }\deg(f-dg)<\deg d\]
	We denote $f-dg$ by $r$,so we have
	\[f=dg+r\]
\end{thm}
\begin{proof}
	First we suppose
	\[f=a_nx^n+\sum\limits_{i=0}^{n-1}a_ix^i\]
	\[d=a_mx^m+\sum\limits_{i=0}^{m-1}a_ix^i\]
	Then let $g_1$ be $\dfrac{a_n}{a_m}x^{n-m}$ we have
	\[\deg(f-dg_1)<\deg f\]
	If $\deg(f-dg_1)<\deg d$ or just zero polynomial,we have done.If not,construct $g_2$ similarly,we have
	\[\deg(f-dg_1-d_2)<\deg(f-dg_1)\]
	Obviously this process can be repeated finitely,and we have
	\[g=\sum\limits_{i=1}^kg_i\]
	If there exit another $g',r'$ such that
	\[f=dg+r=dg'+r'\]
	then we have
	\[d(g-g')=r'-r\]
	But if $g\neq g'$,$\deg d(g-g')\ge\deg d>\deg r,\deg r'$.\\
	Hence we must have $g=g',r=r'$.
\end{proof}
\begin{coro}
	$f$ is divisible by $x-c$ if and only if $f(c)=0$.
\end{coro}
\begin{dde}
	Let $F$ be a field.An element $c$ in $F$ is said to be a \textbf{root} or a \textbf{zero} of a polynomial $f$ over $F$ if $f(c)=0$.
\end{dde}
\begin{coro}
	A polynomial $f$ of degree $n$ over $F$ has at most $n$ roots in $F$. 
\end{coro}
\begin{dde}
	Let $f$ be a polynomial over $F$ such that
	\[f=a_0+a_1x+\cdots+a_nx^n\]
	The \textbf{derivative} of the polynomial is
	\[Df=a_1+2a_2x+\cdots+na_nx^{n-1}\]
\end{dde}
\begin{thm}
	Let $F$ be a field and let $c$ be an element in $F$.Let $f$ be a polynomial over $F$,then we have
	\[f=\sum\limits_{k=0}^n\frac{D^kf(c)}{k!}(x-c)^k\]
	where $n\ge\deg f$.
\end{thm}
\begin{proof}
	We notice that
	\begin{align*}
		x^m&=(x-c+c)^m\\
		&=\sum\limits_{k=0}^m\binom{m}{k}(x-c)^kc^{m-k}\\
		&=\sum\limits_{k=0}^m\frac{m!}{(m-k)!k!}(x-c)^kc^{m-k}
	\end{align*}
	\begin{align*}
		f&=\sum\limits_{m=0}^na_mx^m\\
		&=\sum\limits_{m=0}^na_m\sum\limits_{k=0}^m\frac{m!}{(m-k)!k!}(x-c)^kc^{m-k}\\
		&=\sum\limits_{k=0}^n(x-c)^k\sum\limits_{m=k}^na_m\frac{m!}{(m-k)!k!}c^{m-k}\\
		&=\sum\limits_{k=0}^n\frac{D^kf(c)}{k!}(x-c)^k
	\end{align*}
\end{proof}
Obviously $\{(x-c)^k\}$ is linearly independent and is a basis for $F[x]$.\\ 
\indent If $c$ is a root of $f$,the \textbf{multiplicity} of $c$ is the largest positive integer $r$ such that $(x-c)^r$ divides $f$.
\begin{thm}
	Let $f$ be a polynomial over $F$.Then $c$ is a root of $f$ of multiplicity $r$ if and only if
	\[D^kf(c)=0,k=0,1,\dots,r-1\]
	\[D^{r}f(c)\neq0\]  
\end{thm}
\begin{proof}
	First we have
	\[f=(x-c)^rg,g(c)\neq0\]
	Then we expand $g$
	\[f=\sum\limits_{m=0}^{n-r}\frac{D^mg(c)}{m!}(x-c)^{m+r}\]
	which means
	\[D^kf(c)=\begin{cases}
		0&0\le k\le r-1\\[8pt]
		\dfrac{D^{k-r}g(c)}{(k-r)!}&r\le k\le n
	\end{cases}\]
	And we have $g(c)\neq0$,which means $D^rf(c)\neq0$.
\end{proof}
\begin{dde}
	Let $F$ be a field.An \textbf{ideal} in $F[x]$ is a subspace $M$ of $F[x]$ such that $fg$ belongs to $M$ whenever $f$ is in $F[x]$ and $g$ is in $M$. 
\end{dde}
Apparently,$M=dF[x]$ is an ideal of $F[x]$,which is called the \textbf{principal ideal generated by} $d$.\\
\indent Let $d_1,d_2,\dots,d_n$ be a finite number of polynomials in $F[x]$.The sum $M$ of the subspaces $d_iF[x]$ is obviously an ideal,which is called the \textbf{ideal generated by} the polynomials,$d_1,d_2,\dots,d_n$.
\begin{thm}
	Let $F$ be a field and let $M$ be any nonzero ideal in $F[x]$.Then there is a unique monic polynomial $d$ in $F[x]$ such that $M$ is the principal ideal generated by $d$.
\end{thm}
\begin{proof}
	Let $d$ be a monic polynomial in $M$ which has the smallest degree.Then let $g$ be an element in $M$ we have
	\[g=dq+r\]
	Because $M$ is a subspace,then $r=g-dq$ is in $M$.Hence $r$ must be $0$,otherwise $r$ would be the polynomial with the smallest degree.Therefore we have
	\[g=dq,\forall g\in M\]
	which means $M\subseteq dF[x]$.Obviously for all $g\in M$ we have $gF[x]\subseteq M$.Thus $M$ is the principal ideal generated by $d$.\\
	If $d'$ is another monic polynomial in $M$ such that $M=d'F[x]$,then we have
	\[d'=dq=d'pq\]
	which means $p=q=1$. 
\end{proof}
\begin{coro}
	If $p_1,p_2,\dots,p_n$ are polynomials in $F[x]$,not all of which is $0$.Then there exists a unique monic polynomial $d$ such that
	\begin{enumerate}
		\item [(a)]$d$ is in the ideal generated by $p_1,\dots,p_n$
		\item [(b)]$d$ divides each of the polynomials $p_i$
	\end{enumerate}
	Any polynomials satisfying (a) and (b) necessarily satisfies
	\begin{enumerate}
		\item [(c)]$d$ is divided by every polynomial which divides each of the polynomials $p_i$
	\end{enumerate} 
\end{coro}
\begin{proof}
	According to the theorem above,the existence of $d$ is obvious.If there exists another monic polynomial $d'$ satisfies (a)(b),then we have
	\[d'=dq=d'pq\]
	which means $p=q=1,d=d'$.(c) is obviously correct from (a). 
\end{proof}
\begin{dde}
	If $p_1,p_2,\dots,p_n$ are polynomials in $F[x]$,not all of which is $0$,the monic generator $d$ of 
	\[p_1F[x]+p_2F[x]+\cdots+p_nF[x]\]
	is called the \textbf{greatest common divisor} of $p_1,\dots,p_n$.We say $p_1,\dots,p_n$ are \textbf{relatively prime} if their greatest common divisor is $1$,or equivalently if the ideal they generate is all of $F[x]$.
\end{dde}
\begin{dde}
	Let $F$ be a field.A polynomial $f$ in $F[x]$ is called \textbf{reducible over} $F$ if there exit polynomials $g,h$ in $F[x]$ of degree$\le1$ such that $f=gh$,and if not,$f$ is called \textbf{irreducible over} $F$.A non-scalar irreducible polynomial over $F$ is called a \textbf{prime polynomial over} $F$,and we sometimes say it is a \textbf{prime in} $F[x]$.  
\end{dde}
\begin{thm}
	Let $p,f,g$ be polynomials over $F$.Suppose that $p$ is a prime polynomial and that $p$ divides the product $fg$.Then either $p$ divides $f$ or $p$ divides $g$.
\end{thm}
\begin{proof}
	We can always suppose $p$ is a monic polynomial.Then the only monic divisor of $p$ is $1$ and $p$.Therefore the greatest common divisor of $f,p$ is $1$ or $p$.If $(f,p)=1$,then we have
	\[1=ff_0+pp_0\]
	which means
	\[g=fgf_0+gpp_0\]
	and then $p$ divides $g$.
\end{proof}
\begin{coro}
	If $p$ is a prime and divides the product $f_1f_2\cdots f_n$,then $p$ divides one of the polynomials $f_1,\dots,f_n$.
\end{coro}
\begin{thm}
	If $F$ is a field,a non-scalar monic polynomial in $F[x]$ can be factored as a product of monic primes in $F[x]$ in one and,except for order,only one way. 
\end{thm}
\begin{proof}
	There is nothing to do when $\deg f=1$.Suppose $\deg f=n$,we can use induction to prove it easily.If there exits two product such that
	\[p_1\cdots p_n=q_1\cdots q_m\]
	Use the corollary above we have that there exists $q_i$ such that $p_n$ divides $q_i$.Because $p_n,q_i$ are monic primes,we have
	\[p_n=q_i\]
	Rearrange them and cancel them we have
	\[p_1\cdots p_{n-1}=q_1\cdots q_{m-1}\]
	Continue this process we have done the proof.
\end{proof}
Combine the same monic polynomials we have
\[f=p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}\]
which is called the \textbf{primary decomposition} of $f$.It is easy to see that each monic divisor of $f$ has the form
\[p_1^{m_1}p_2^{m_2}\cdots p_r^{m_r},0\le m_i\le n_i\]
\begin{thm}
	Let $f$ be a non-scalar monic polynomials over $F$ and let
	\[f=p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}\]
	\[f_i=\frac{f}{p_i^{n_i}}\]
	Then $f_1,f_2,\dots,f_r$ are relatively prime.
\end{thm}
\begin{thm}
	Let $f$ be an element in $F^\infty$,define $Df$ as
	\[Df=\sum\limits_{i=1}^\infty ia_ix^{i-1}\]
	Then we have
	\[D(fg)=(Df)g+f(Dg)\]
\end{thm}
\begin{proof}
	\begin{align*}
		(D(fg))_n&=(n+1)(fg)_{n+1}\\
		&=(n+1)\sum\limits_{k=0}^{n+1}f_kg_{n+1-k}
	\end{align*}
	\begin{align*}
		((Df)g)_n&=\sum\limits_{k=0}^n(Df)_kg_{n-k}\\
		&=\sum\limits_{k=0}^n(k+1)f_{k+1}g_{n-k}\\
		&=\sum\limits_{k=1}^{n+1}kf_kg_{n+1-k}
	\end{align*}
	\begin{align*}
		(f(Dg))_n&=\sum\limits_{k=0}^nf_{k}(Dg)_{n-k}\\
		&=\sum\limits_{k=0}^n(n+1-k)f_{k}g_{n+1-k}\\
	\end{align*}
	Obviously we have
	\[(D(fg))_n=((Df)g)_n+(f(Dg))_n\]
\end{proof}
\begin{thm}
	Let $f$ be a monic polynomial over $F$ with derivative $Df$.Then $f$ is a product of distinct irreducible polynomials over $F$ if and only if $f$ and $Df$ are relatively prime. 
\end{thm}
\begin{proof}
	If some prime polynomial $p$ is repeated in the product,then $f=p^2h$ and 
	\[Df=p^2(Dh)+2p(Dp)h\]
	which means $Df$ and $f$ are not relatively prime.\\
	If $f$ is a product of distinct irreducible polynomials,then we have
	\[Df=(Dp_1)f_1+(Dp_2)f_2+\cdots+(Dp_r)f_r\]
	If there exists $p_i$ divides $f$ and $Df$,and because $p_i$ divides $f_j$ for every $j\neq i$,we have $p_i$ divides $f_i(Dp_i)$,which means $p_i$ divides $Dp_i$.But this is impossible because the degree of $Dp_i$ is less than the degree of $p_i$.
\end{proof}
\begin{dde}
	The field $F$ is called \textbf{algebraically closed} if every prime polynomials over $F$ has degree $1$.
\end{dde}
The definition means that every polynomials in $F[x]$ can be expressed in the form
\[f=c(x-c_1)^{n_1}\cdots(x-c_r)^{n_r}\]
The field $C$ of complex numbers is algebraically closed,which is called the Fundamental Theorem of Algebra,and this also implies that every prime over $R$ has degree $1$ or $2$.\\
\indent In next section we will introduce an important concept:eigenvalue.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{channel1}
\end{figure}
\section{Elementary Canonical Forms}
\subsection{Partitioned Matrix and Direct Sum of Subspaces}
In this subsection, we will introduce some concepts and prove some theorems about partitioned matrix, preparing for the following content.
\begin{dde}
	We can write a matrix as ``block forms", which is called \textbf{partitioned matrix}.
	\[A=\begin{bmatrix}
		A_{11}&A_{12}&\cdots&A_{1n}\\
		A_{21}&A_{22}&\cdots&A_{2n}\\
		\vdots&\vdots&\ddots&\vdots\\
		A_{m1}&A_{m2}&\cdots&A_{mn}
	\end{bmatrix}\]
\end{dde}
\begin{thm}
	The product of two partitioned matrices $A,B$ satisfies the normal rule of matrix product, while replace the elements with matrices.
	\[A=\begin{bmatrix}
		A_{11}&A_{12}&\cdots&A_{1n}\\
		A_{21}&A_{22}&\cdots&A_{2n}\\
		\vdots&\vdots&\ddots&\vdots\\
		A_{m1}&A_{m2}&\cdots&A_{mn}
	\end{bmatrix},B=\begin{bmatrix}
	B_{11}&B_{12}&\cdots&B_{1k}\\
	B_{21}&B_{22}&\cdots&B_{2k}\\
	\vdots&\vdots&\ddots&\vdots\\
	B_{n1}&B_{m2}&\cdots&B_{mk}
	\end{bmatrix}\]
	\[(AB)_{ij}=\sum\limits_{k=1}^nA_{ik}B_{kj}\]
\end{thm}
Apparently, each product must be legitimate. 
\begin{proof}
	First, It is obviously possible to divide $AB$ as a partitioned matrix that each block has the same size as the sums of $A_{ik}B_{kj}$.
	\begin{align*}
		((AB)_{ij})_{pq}&=\sum\limits_{k'=1}^{n_0}a_{p_0k'}b_{k'q_0}\\
		&=\sum\limits_{k=1}^n(A_{ik}B_{kj})_{pq}\\
	\end{align*}
	Thus we have done the prove.
\end{proof}
\begin{coro}
	If $B=(\beta_1,\beta_2,\dots,\beta_n)$, we have
	\[AB=(A\beta_1,A\beta_2,\dots,A\beta_n)\]
	If $A=\begin{bmatrix}
		\alpha_1\\
		\alpha_2\\
		\vdots\\
		\alpha_m
	\end{bmatrix}$, we have
	\[AB=\begin{bmatrix}
		\alpha_1B\\
		\alpha_2B\\
		\vdots\\
		\alpha_m
	\end{bmatrix}\]
\end{coro}
\begin{dde}
	We can define the elementary raw operations and elementary column operations. However, we will replace the scalar by a matrix $P$.
	\begin{enumerate}
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
		A_{11}&A_{12}\\
		A_{21}+PA_{11}&A_{22}+PA_{12}
		\end{bmatrix}\]
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
		A_{21}&A_{22}\\
		A_{11}&A_{12}
		\end{bmatrix}\]
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
		A_{11}&A_{12}\\
		PA_{21}&PA_{22}
		\end{bmatrix}\]
		where $P$ is an invertible matrix
	\end{enumerate}
	and
	\begin{enumerate}
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
			A_{11}&A_{12}+A_{11}P\\
			A_{21}&A_{22}+A_{21}P
		\end{bmatrix}\]
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
			A_{12}&A_{11}\\
			A_{22}&A_{21}
		\end{bmatrix}\]
		\item \[\begin{bmatrix}
			A_{11}&A_{12}\\
			A_{21}&A_{22}
		\end{bmatrix}\rightarrow\begin{bmatrix}
			A_{11}&A_{12}P\\
			A_{21}&A_{22}P
		\end{bmatrix}\]
		where $P$ is an invertible matrix
	\end{enumerate}
\end{dde}
We can then define the partitioned elementary matrix, which will not be used so we skip it.
\begin{dde}
	Let $W_1,W_2,\dots,W_k$ be subspaces of vector space $V$. We say that $W_1,W_2,\dots,W_k$ are independent if
	\[\alpha_1+\alpha_2+\cdots+\alpha_k=0,\alpha_i\in W_i\]
	implies each $\alpha_i=0$.
\end{dde}
\begin{thm}
	Let $V$ be a finite-dimensional vector space. Let $W_1,W_2,\dots,W_k$ be subspaces of $V$ and let $W=W_1+W_2+\cdots+W_k$. The following are equivalent.
	\begin{enumerate}
		\item [(a)]$W_1,\dots,W_k$ are independent.
		\item [(b)]For each $2\le j\le k$, we have
		\[W_j\cap(W_1+\cdots+W_{j-1})=\{0\}\]
		\item [(c)]If $\mathcal{B}_i$ is an ordered basis for $W_i$, $1\le i\le k$, then the sequence $\mathcal{B}=(\mathcal{B}_1,\mathcal{B}_2,\dots,\mathcal{B}_k)$ is an ordered basis for $W$.
	\end{enumerate}
\end{thm}
\begin{proof}
	Assume (a). Let $\alpha$ be a vector in the intersection $W_j\cap(W_1+\cdots+W_{j-1})$. Then there are $\alpha_1,\dots,\alpha_{j-1},\alpha_i\in W_i$ such that
	\[\alpha_1+\cdots+\alpha_{j-1}+(-\alpha)+0+\cdots+0=0\]
	which means $\alpha_1=\cdots=\alpha_{j-1}=\alpha=0$.\\
	Now assume (b). Let $\alpha_1,\dots,\alpha_k,\alpha_i\in W_i$
	\[\alpha_1+\cdots+\alpha_k=0\]
	If there exists a nonzero $\alpha_i$, denote the largest $i$ by $j$, we have
	\[\alpha_1+\cdots+\alpha_j=0\]
	which means
	\[\alpha_j=-\alpha_1-\cdots-\alpha_{j-1}\]
	then $\alpha_j$ is a nonzero vector in the intersection $W_j\cap(W_1+\cdots+W_{j-1})$.\\
	The equivalence of (c) and (a) is simple to prove and we omit it here.
\end{proof}
One thing you may notice is that the proof of the equivalence of (a) and (b) do not require $V$ be finite-dimensional. We can restate (c) by this:\\
\indent There is one and only one decomposition of $\alpha\in W$ such that
\[\alpha=\alpha_1+\cdots+\alpha_k,\alpha_i\in W_i\]
This also do not need $V$ be finite-dimensional.\\ 
\indent If $W_1,\dots,W_k$ are independent, we call the sum of $W_1,\dots,W_k$ \textbf{direct sum} and we write
\[W=W_1\oplus W_2\oplus\cdots\oplus W_k\] 
\begin{dde}
	If $V$ is a vector space, a \textbf{projection} of $V$ is a linear operator $E$ on $V$ such that $E^2=E$.
\end{dde}
Let $R$ be the range of $E$ and let $N$ be the null space of $E$, we have
\begin{enumerate}
	\item $\beta\in R$ if and only if $E\beta=\beta$. We shall prove the ``only if" part. We have
	\[\beta=E\alpha=E^2\alpha=E\beta\]
	\item $V=R\oplus N$
	\item $\alpha=E\alpha+(\alpha-E\alpha)$
\end{enumerate}
From (3) we know that $V=R+N$, and then
\[\alpha+\beta=0,\alpha\in R,\beta\in N\]
we have
\[E\alpha+E\beta=\alpha=0\rightarrow\beta=0\]
and we prove (2), which implies that the decomposition in (3) is unique.\\
\indent Inversely, It's easy to see that if $V=R\oplus N$, there is one and only one projection operator $E$ with range $R$ and null space $N$. We call it the \textbf{projection on} $R$ \textbf{along} $N$.
\begin{thm}\label{directsumdecomposition}
	If $V=W_1\oplus\cdots\oplus W_k$, then there exists $k$ linear operation $E_1,\dots,E_k$ on $V$ such that
	\begin{enumerate}
		\item [(\romannumeral1)]each $E_i$ is a projection
		\item [(\romannumeral2)]$E_iE_j=0$, if $i\neq j$
		\item [(\romannumeral3)]$I=E_1+\cdots+E_k$
		\item [(\romannumeral4)]the range of $E_i$ is $W_i$
	\end{enumerate}
	Conversely, if $E_1,\dots,E_k$ are linear operators on $V$ which satisfy (\romannumeral2) and (\romannumeral3), and if we let $W_i$ be the range of $E_i$, then $V=W_1\oplus\cdots\oplus W_k$.
\end{thm}
\begin{proof}
	If $V=W_1\oplus\cdots\oplus W_k$, we have
	\[\alpha=\alpha_1+\cdots+\alpha_k,\alpha_i\in W_i\]
	We define $E_i\alpha=\alpha_i$, then (\romannumeral1)(\romannumeral2)(\romannumeral3)(\romannumeral4) are obviously correct.\\
	If $E_1,\dots,E_k$ are linear operators on $V$ which satisfy (\romannumeral2) and (\romannumeral3), we have
	\[E_i=\sum\limits_{j=1}^kE_iE_j=E_i^2\]
	and
	\[\alpha=E_1\alpha+\cdots+E_k\alpha\]
	which implies $V=W_1+\cdots+W_k$. Let $\alpha_i\in W_i$ and
	\[0=\alpha_1+\cdots+\alpha_k=E_1\alpha_1+\cdots+E_k\alpha_k\]
	then
	\[0=\sum\limits_{j=1}^kE_iE_j\alpha_j=E_i^2\alpha_i=\alpha_i\]
	which implies the independence of $W_1,\dots,W_k$.
\end{proof}
\begin{dde}
	Let $V$ be a vector space and $T$ be a linear operator on $T$. If $W$ is a subspace of $V$, we say that $W$ is \textbf{invariant under} $V$ if for each $\alpha$ in $W$ we have $T\alpha$ in $W$
\end{dde}
\begin{thm}\label{Tcommuteprojection}
	Let $T$ be a linear operator on $V$, and let $W_1,\dots,W_k$ and $E_1,\dots,E_k$ be as in Theorem \ref{directsumdecomposition}, then each $W_i$ is invariant under $T$ if and only if
	\[TE_i=E_iT,i=1,\dots,k\]
	If $\mathcal{B}=(\mathcal{B}_1,dots,\mathcal{B}_k)$ is an ordered basis for $V$, then the matrix of $T$ under $\mathcal{B}$ is a partitioned diagonal matrix
	\[A=\begin{bmatrix}
		A_1&0&\cdots&0\\
		0&A_2&\cdots&0\\
		\vdots&\vdots&\ddots&\vdots\\
		0&0&\cdots&A_k
	\end{bmatrix}\]
\end{thm}
\begin{proof}
	First if $\alpha_i\in W_i$ and $TE_i=E_iT$, we have
	\[T\alpha_i=TE_i\alpha_i=E_iT\alpha_i\]
	which means $T\alpha_i\in W_i$.\\
	Conversely, if each $W_i$ is invariant under $T$, we have
	\[T\alpha=TE_1\alpha+\cdots+TE_k\alpha\]
	and we know that if $\alpha\in W_i$, then $E_j\alpha=\delta_{ij}\alpha$.
	\[E_iT\alpha=TE_i\alpha\] 
\end{proof}
\subsection{Primary Decomposition and Eigenvalue Decomposition}
We want to build the theory from general to specific, so we will introduce the annihilating polynomials first.\\
\indent The first thing we notice is that the collection of polynomials $p$ which annihilating $T$ is an ideal\footnote{One thing you must remember is that we regard the polynomial $p$ in $F[x]$ and $p(T)$ in $\mathcal{L}(V)$ as the same thing}, because
\[(qp)(T)=q(T)p(T)=0\]
\indent The existence of annihilating polynomials will be guaranteed by the finite dimension. Let $n$ be the dimension of $V$, then the dimension of $\mathcal{L}(V)$ is $n^2$, which implies that $1,T,T^2,\dots,T^{n^2}$ must bu linearly dependent, and this means 
\[\lambda_0+\lambda T_1+\cdots+\lambda_{n^2}T^{n^2}=0\]
with not all $\lambda_i$ is zero.
\begin{dde}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$ over the field $F$. The \textbf{minimal polynomial} for $T$ is the unique monic generator of the ideal of polynomials over $V$ which annihilate $T$.
\end{dde}
We can define the minimal polynomial of a $n\times n$ matrix $A$ similarly, which is the same as the minimal polynomial of $T$ if $A$ is the matrix of $T$ under some basis.\\
\indent One thing you must know is that if $F_1$ is a subfield of $F$, and $A$ is a matrix over $F_1$, the minimal polynomials over $F_1$ and $F$ are the same. This is because that the minimal polynomial means that
\[A^k+a_{k-1}A^{k-1}+\cdots+a_0I=0\]
the system of $n^2$ linear equations of $a_0,\dots,a_{k-1}$ have a unique solution. For each $k$ we can write the similar equations, however, the system has solutions over $F$ if and only if it has solutions in $F_1$, because the elementary raw operation can happen only in $F_1$, which gives the condition of whether it has solutions, as well as the uniqueness of the solution. Thus the minimal polynomial is the same.\\   
\begin{dde}
	Let $V$ be a vector space over the field $F$ and let $T$ be a linear operator on $V$. An \textbf{eigenvalue} of $V$ is a scalar $\lambda$ in $F$ such that there exists a nonzero vector $\alpha\in V$ with $T\alpha=\lambda\alpha$. If $\lambda$ is an eigenvalue of $T$, then
	\begin{enumerate}
		\item [(a)]any $\alpha$ such that $T\alpha=\lambda\alpha$ is called a \textbf{eigenvector} of $T$ associated with $\lambda$
		\item [(b)]the collection of all eigenvectors associated with $\lambda$ is called \textbf{characteristic space}, which is obviously a subspace of $V$.
	\end{enumerate}
\end{dde}
Apparently, $\lambda$ is an eigenvalue of $T$ if and only if $(T-\lambda I)\alpha=0$ has nonzero solution, which implies $\det(T-\lambda I)=0$.\\
\indent We can define the eigenvalues of an $n\times n$ matrix $A$ similarly, which is the same as eigenvalues of $T$ if $A$ is the matrix of $T$ under some basis.\\ 
\indent $\det (T-\lambda I)$ is a polynomial of $\lambda$. We often want it to be a monic polynomial, thus we call $\det(\lambda I-T)$ the \textbf{characteristic polynomial} of $A$, whose roots are obviously the eigenvalues.
\begin{thm}
	Let $T$ be a linear operator on an $n$-dimensional vector space $V$. The characteristic and minimal polynomials for $T$ have the same roots, except for multiplicities.  
\end{thm}
\begin{proof}
	Let $p$ be the minimal polynomial for $T$. If $\lambda$ is a root of $p$, we have
	\[p(T)=(T-\lambda I)q(T)\]
	Because $q$ is not the annihilating polynomial, we can find a vector $\beta$ such that $q(T)\beta=\alpha\neq0$, then we have
	\[0=(T-\lambda I)\alpha\]
	which implies that $\lambda$ is an eigenvalue of $T$, and thus a root of the characteristic polynomial.\\
	Now suppose $\lambda$ is an eigenvalue of $T$. Let $\alpha$ be an eigenvector associated with $\lambda$, then we have
	\[p(T)\alpha=p(\lambda)\alpha=0\]
	which implies that $\lambda$ is a root of $p$.
\end{proof}
\begin{thm}[Cayley-Hamilton]\label{CayleyHamilton}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$, whose matrix is $A$ under some basis. If $f$ is the characteristic polynomial for $T$, then $f(T)=0$, which implies that the minimal polynomial $p$ divides $f$.
\end{thm}
\begin{proof}
	Let $B=adj\,(\lambda I-A)$. Obviously, each element of $B$ is a polynomial of $\lambda$ with degree less than or equal to $n-1$(or zero polynomial). We have
	\[B(\lambda I-A)=\det(\lambda I-A)I\]
	and we can write $B$ as
	\[B=\lambda^{n-1}B_0+\lambda^{n-2}B_1+\cdots+B_{n-1}\]
	then
	\[B(\lambda I-A)=\lambda^nB_0+\lambda^{n-1}(B_1-B_0A)+\cdots+\lambda(B_{n-1}-B_{n-2}A)-B_{n-1}A\]
	We denote $f$ as
	\[f(\lambda)=\lambda^n+a_1\lambda^{n-1}+\cdots+a_{n}\]
	Then we have
	\begin{align*}
		B_0&=I\\
		B_1-B_0A&=a_1I\\
		\vdots\\
		B_{n-1}-B_{n-2}A&=a_{n-1}I\\
		-B_{n-1}A&=a_n
	\end{align*}
	Multiply $A^n,A^{n-1},\dots,A$, we have
	\begin{align*}
		B_0A^n&=A^n\\
		B_1A^{n-1}-B_0A^n&=a_1A^{n-1}\\
		\vdots\\
		B_{n-1}A-B_{n-2}A^2&=a_{n-1}A\\
		-B_{n-1}A&=a_n
	\end{align*}
	add them up, the left side is zero, the right side is just the characteristic polynomial $f(A)$.Thus we have
	\[f(T)=0\] 
\end{proof}
By this theorem, if we can write $f$ as $(\lambda-\lambda_1)^{d_1}\cdots(\lambda-\lambda_k)^{d_k}$, then the minimal polynomial has the form
\[p(T)=(T-\lambda_1I)^{r_1}\cdots(T-\lambda_kI)^{r_k},1\le r_i\le d_i\]
We can go further by the next theorem.
\begin{thm}[Primary Decomposition Theorem]\label{PrimeDecomposition}
	Let $T$ be a linear operator on a finite dimensional vector space $V$ over the field $F$. Let $p$ be the minimal polynomial for $T$,
	\[p=p_1^{r_1}p_2^{r_2}\cdots p_k^{r_k}\]
	where $p_i$ are distinct monic prime polynomials over $F$ and $r_i$ are positive integers. Let $W_i$ be the null space of $p_i^{r_i},i=1,\dots,k$. Then
	\begin{enumerate}
		\item [(\romannumeral1)]$V=W_1\oplus\cdots\oplus W_k$
		\item [(\romannumeral2)]each $W_i$ is invariant under $T$
		\item [(\romannumeral3)]if $T_i$ is the operator induced on $W_i$ by $T$, then the minimal polynomial for $T_i$ is $p_i^{r_i}$
	\end{enumerate}
\end{thm}
\begin{proof}
	For each $i$, let
	\[f_i=\frac{p}{p_i^{r_i}}=\prod_{j\neq i}p_{j}^{r_j}\]
	They are relatively prime, thus we have
	\[\sum\limits_{i=1}^kf_ig_i\]
	Obviously, $f_if_j,j\neq i$ is dividable by $p$.Let $E_i(T)=f_i(T)g_i(T)$, then we have
	\[I=E_1+\cdots+E_k,E_iE_j=0\]
	If $\alpha$ is a vector in the range of $E_i$, then $\alpha=E_i\alpha$, and we have
	\[p_i^{r_i}(T)\alpha=p(T)g_i(T)\alpha=0\]
	Conversely, if $\alpha\in W_i$, because $E_j=f_jg_j$ is dividable by $p_i^{r_i}$ if $i\neq j$, we have
	\[E_j\alpha=0,i\neq j\rightarrow\alpha=E_i\alpha\]
	$W_i$ is apparently invariant under $T$ because $E_iT=TE_i$. if $T_i$ is the operator induced on $W_i$ by $T$, then $p_i^{r_i}(T_i)=p_i^{r_i}(T)=0$. If $g$ is the minimal polynomial for $T_i$, then $g(T)f_i(T)$, because $f_i(T)\alpha$ if $\alpha\in W_j,j\neq i$ and $g(T)\alpha=0$ if $\alpha\in W_i$, and we have $V=W_1\oplus\cdots\oplus W_k$. Therefore $p$ as well as $p_i^{r_i}$ divides $gf_i$, which means $p_i^{r_i}$ divides $g$.
\end{proof}
\begin{coro}
	If $U$ commutes with $T$, then $W_i$ is invariant under $U$.
\end{coro}
Now we want to study the specific case of this theorem.
\begin{dde}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$. We say that $T$ is \textbf{diagonalizable} if there exits a basis for $V$ each vector of which is an eigenvector of $T$.
\end{dde}
To comprehend the concept thoroughly, we want first to study it by characteristic space.
\begin{thm}
	The characteristic spaces of $T$ are independent.
\end{thm}
\begin{proof}
	Let $W_i$ be the characteristic space of $T$,
	\[\alpha_1+\cdots+\alpha_k=0,\alpha_i\in W_i\]
	Let $\lambda_i$ be the eigenvalues, then
	\begin{align*}
		\alpha_1+\cdots+\alpha_k&=0\\
		\lambda_1\alpha_1+\cdots+\lambda_k\lambda_k&=0\\
		\vdots\\
		\lambda_1^{k-1}\alpha_1+\cdots+\lambda_k^{k-1}\alpha_k&=0
	\end{align*}
	The coefficient determinant is just the Vandermonde determinant, and thus is nonzero, which means 
	\[A\begin{bmatrix}
		\alpha_1\\
		\alpha_2\\
		\vdots\\
		\alpha_k
	\end{bmatrix}=0,\alpha_1=\cdots=\alpha_k=0\] 
\end{proof}
We can conclude immediately that if $\lambda_i\neq\lambda_j,i\neq j$, then $T$ is obviously diagonalizable, then we have
\[V=W_1\oplus\cdots\oplus W_2\]
But if not? When $T$ is diagonalizable, this is still correct, however, we notice that some characteristic spaces will not be $1$-dimensional. We want to know whether this dimension is relevant to the multiplicity of the eigenvalue.
\begin{thm}
	The dimension of the characteristic space $W$ associated with the eigenvalue $\lambda_0$ is less than or equal to the multiplicity of $\lambda$ in the characteristic polynomial for $T$.
\end{thm}  
\begin{proof}
	Choose an ordered basis for $W$ and extend it to a basis for $V$. Let $r$ be the dimension of $W$ and $A$ be the matrix of $T$ under this basis. we have
	\[A=\begin{bmatrix}
		\lambda_0 I&B\\
		0&C
	\end{bmatrix}\]
	and
	\begin{align*}
		\det(\lambda I-A)&=\begin{vmatrix}
			\lambda I-\lambda_0I&-B\\
			0&\lambda I-C
		\end{vmatrix}\\
		&=\det(\lambda I-\lambda_0I)\det(\lambda I-C)\\
		&=(\lambda-\lambda_0)^r\det(\lambda I-C)
	\end{align*}
	We have finished the proof.
\end{proof} 
Obviously, if $T$ is diagonalizable, then the characteristic polynomial $f$ has the form
\[f(T)=(T-\lambda_1I)^{d_1}\cdots(T-\lambda_kI)^{d_k}\]
and because $d_1+\cdots+d_k=n$, we have $\dim W_i=d_i$.\\
\indent Moreover, we notice that $(T-\lambda_1I)\cdots(T-\lambda_kI)$ annihilate $T$, and thus
\[p=(x-\lambda_1)\cdots(x-\lambda_k)\] 
\begin{thm}
	$T$ is diagonalizable if and only if $p=(x-\lambda_1)\cdots(x-\lambda_k)$.
\end{thm}
\begin{proof}
	Let's prove the ``if" part. If so, $\lambda_i$ is then the eigenvalues of $T$, and use Theorem \ref{PrimeDecomposition}, we have
	\[V=W_1\oplus\cdots\oplus W_k\]
	and $W_i$ is the null space of $T-\lambda_iI$, which is just the characteristic space associated with $\lambda_i$.
\end{proof}
By the way, if the minimal polynomial for $T$ is the product of linear factors, use Theorem \ref{PrimeDecomposition} and you may notice that the characteristic polynomial of each $W_i$ must has the form $(x-\lambda_i)^{d_i}$, and therefore $\dim W_i=d_i$.\\ 
\indent Actually, all the things about eigenvalue decomposition is done. But we want to prepare for the next section.
\begin{lem}
	Let $W$ be an invariant subspace for $T$. The characteristic polynomial for the restriction operator $T_W$ divides the characteristic polynomial for $T$. The minimal polynomial for $T_W$ divides the minimal polynomial for $T$.
\end{lem}
\begin{proof}
	Actually,the first part we have proven before
	\[A=\begin{bmatrix}
		B&C\\
		0&D
	\end{bmatrix}\]
	\begin{align*}
		\det(\lambda I-A)&=\begin{vmatrix}
			\lambda I-B&-C\\
			0&\lambda I-D
		\end{vmatrix}\\
		&=\det(\lambda I-B)\det(\lambda I-D)
	\end{align*}
	and we have
	\[A^k=\begin{bmatrix}
		B^k&C_k\\
		0&D^k
	\end{bmatrix}\]
	which means that any polynomial annihilate $T$ will annihilate $T_W$, then the minimal polynomial for $T_W$ divides the minimal polynomial for $T$. 
\end{proof}
\begin{dde}
	Let $W$ be an invariant subspace for $T$ and let $\alpha$ be a vector in $V$. The $T$-\textbf{conductor of} $\alpha$ \textbf{into} $W$ is the set $S_T(\alpha;W)$, which consists of all polynomials $g$ over $F$ such that $g(T)\alpha\in W$.
\end{dde}
If $W$ is $\{0\}$, we called the conductor the $T$-\textbf{annihilator of} $\alpha$.
\begin{lem}
	$S_T(\alpha;W)$ is an ideal in the polynomial algebra $F[x]$.
\end{lem}
\begin{proof}
	We can prove it by the fact that $W$ is invariant under every polynomial of $T$.  
\end{proof}
The unique monic generator of $S_T(\alpha;W)$ is also called the $T$-\textbf{conductor of} $\alpha$ \textbf{into} $W$. Note that every $S_T(\alpha;W)$ contains the minimal polynomial for $T$, hence, every $T$-conductor divides the minimal polynomial for $T$.
\begin{lem}
	Let $V$ be a finite-dimensional vector space over the field $F$. Let $T$ be a linear operator on $V$ such that the minimal polynomial for $T$ is a product of linear factors
	\[p=(x-\lambda_1)^{r_1}\cdots(x-\lambda_k)^{r_k}\]
	Let $W$ be a proper subspace of $V$ which is invariant under $T$. There exists a vector $\alpha$ in $V$ such that
	\begin{enumerate}
		\item [(a)]$\alpha$ is not in $W$
		\item [(b)]$(T-\lambda I)\alpha$ is in $W$, for some eigenvalue $\lambda$ of $T$.
	\end{enumerate}
\end{lem}
\begin{proof}
	Let $\beta$ be a vector not in $W$. Let $g$ be the $T$-conductor of $\beta$ into $W$, then $g$ divides $p$, and thus has the form
	\[g=(x-\lambda_1)^{e_1}\cdots(x-\lambda_k)^{e_k}\]
	with at least one $e_i$ is nonzero. Choose $e_j>0$, we have
	\[g(T)=(T-\lambda_j I)h(T)\]
	By the definition, $\alpha=h(T)\beta$ is not in $W$, and we have
	\[(T-\lambda_j I)\alpha=g(T)\beta\in W\]
	Thus we have found what we need.
\end{proof}
Not all linear operators can be diagonalized, so we want to loose the requirement. The linear operator $T$ is called \textbf{triangulable} if there is an ordered basis for $V$ in which $T$ is represented by a triangular matrix.
\begin{thm}\label{SchurDecomposition}
	Let $V$ be a finite-dimensional vector space over the field $F$ and let $T$ be a linear operator on $V$. Then $T$ is triangulable if and only if the minimal polynomial for $T$ is the product of linear polynomials over $F$.
\end{thm}
\begin{proof}
	The ``only if" part is obvious by the Theorem \ref{CayleyHamilton}. We shall prove the ``if" part.\\
	Applying the lemma above, we can first let $W$ be $\{0\}$, and then continue the process to find $\alpha_i$ and add it into $W$ until $W=V$. Apparently, each time we have
	\[T\alpha_j=a_{1j}\alpha_1+\cdots+a_{jj}\alpha_j,1\le j\le n\]
	Then $\{\alpha_i\}$ is just the basis that the matrix of $T$ under it is triangular. 
\end{proof} 
\begin{coro}
	Let $F$ be an algebraically closed field. Every linear operator $T$ on $V$ is triangulable.
\end{coro}
Finally we want two operators triangulated or diagonalized simultaneously. One thing you may notice that if two operators are diagonalized simultaneously, they must commute with each other. You will see this is also the sufficient condition for diagonalization and triangulation, however, is not a necessary condition for the latter one.
\begin{lem}
	Let $\mathcal{F}$ be a commuting family of triangulable linear operators on $V$. Let $W$ be a proper subspace of $V$ which is invariant under $\mathcal{F}$. There exists a vector $\alpha$ such that
	\begin{enumerate}
		\item [(a)]$\alpha\notin W$
		\item [(b)]for each $T$ in $\mathcal{F}$, $T\alpha$ is in the subspace spanned by $\alpha$ and $W$.
	\end{enumerate}
\end{lem}
\begin{proof}
	First, $\mathcal{F}$ is finite-dimensional, so we can choose a basis for $\mathcal{F}$, $\{T_1,\dots,T_r\}$. Applying the previous lemma, we find a $\alpha_1$ not in $W$ and a scalar $\lambda_1$ such that $(T_1-\lambda_1 I)\alpha_1\in W$. Denote the collection of $\alpha$ such that $(T_1-\lambda_1 I)\alpha\in W$ by $V_1$. Obviously, $V_1$ is a subspace of $V$ and $W_1$ is a proper subspace of $V_1$. We have
	\[(T_1-\lambda_1 I)T\alpha=T(T_1-\lambda_1 I)\alpha\in W\subset V_1\]
	Thus $V_1$ is invariant under $\mathcal{F}$. Restrict the linear operators in $\mathcal{F}$ to $V_1$ and repeat the process, we will have a $\alpha$ such that
	\begin{enumerate}
		\item [(a)]$\alpha\notin W$
		\item [(b)]$(T_i-\lambda_i I)\alpha\in W,i=1,\dots,r$
	\end{enumerate}
	and then have found what we need.
\end{proof}
\begin{thm}
	Let $V$ be a finite-dimensional vector space over the field $F$. Let $\mathcal{F}$ be a commuting family of triangulable linear operators on $V$. There exists an ordered basis for $V$ such that every operator in $\mathcal{F}$ is represented by a triangular matrix in that basis.
\end{thm}
\begin{proof}
	Use the lemma above and prove like we have done in the proof of Theorem \ref{SchurDecomposition}.
\end{proof}
\begin{thm}
	Use induction. There is nothing to prove when $n=1$. Let $V$ be $n$-dimensional. Let $T$ be a linear operator in $\mathcal{F}$ which is not a scalar multiply the identity, we know that the direct sum of the null space $W_i$ of $T-\lambda_i I$ is just $V$. $W_i$ is obviously invariant under $\mathcal{F}$, and then we can restrict $\mathcal{F}$ to $W_i$. Because $\dim W_i<\dim V$, by induction, we can find a basis $\mathcal{B}_i$ for each $i$ that simultaneously diagonalize $\mathcal{F}_i$, and $\mathcal{B}=\{\mathcal{B}_1,\dots,\mathcal{B}_k\}$ is just the basis we are looking for.
\end{thm}
We want to point out a useful expression of diagonalizable linear operators.
\begin{thm}\label{EigenvalueDecomposition}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$.\\
	If $T$ is diagonalizable and $\lambda_1,\dots,\lambda_k$ are distinct eigenvalues of $T$, then there exists linear operators $E_1,\dots,E_k$ on $V$ such that 
	\begin{enumerate}
		\item [(\romannumeral1)]$T=\lambda_1E_1+\cdots+\lambda_kE_k$
		\item [(\romannumeral2)]$I=E_1+\cdots+E_k$
		\item [(\romannumeral3)]$E_iE_j=0,i\neq j$
		\item [(\romannumeral4)]$E_i^2=E_i$
		\item [(\romannumeral5)]the range of $E_i$ is the characteristic space for $T$ associated with $\lambda_i$
	\end{enumerate}
	Conversely, if there exist distinct scalars $\lambda_1,\dots,\lambda_k$ and nonzero linear operators $E_1,\dots,E_k$ which satisfy (\romannumeral1)(\romannumeral2)(\romannumeral3), then $T$ is diagonalizable and $\lambda_1,\dots,\lambda_k$ are eigenvalues of $T$, and (\romannumeral4)(\romannumeral5) are satisfied as well
\end{thm}
\begin{proof}
	If $T$ is diagonalizable, then $V=W_1\oplus\cdots\oplus W_k$, where $W_i$ is the null space of $T-\lambda_i$. Use Theorem \ref{directsumdecomposition}, we get $E_1,\dots,E_k$, which satisfy (\romannumeral2)(\romannumeral3)(\romannumeral4)(\romannumeral5) automatically. Use Theorem \ref{Tcommuteprojection}, we have
	\[T\alpha=TE_1\alpha+\cdots+TE_k\alpha=\lambda_1E_1\alpha+\cdots+\lambda_kE_k\alpha\]
	Now suppose there exist distinct scalars $\lambda_1,\dots,\lambda_k$ and nonzero linear operators $E_1,\dots,E_k$ which satisfy (\romannumeral1)(\romannumeral2)(\romannumeral3), then (\romannumeral4) will be satisfied immediately. Use Theorem \ref{directsumdecomposition} again, we know that $V=W_1\oplus\cdots\oplus W_k$, where $W_i$ is the range of $E_i$. We have
	\[T-\lambda I=(\lambda_1-\lambda)E_1+\cdots+(\lambda_k-\lambda)E_k\]
	If $(T-\lambda I)\alpha=0$ for a nonzero vector $\alpha$, then there exists some $E_i$ such that $E_i\alpha\neq0$, because $V=W_1\oplus\cdots\oplus W_k$, we have $\lambda=\lambda_i$ for some $i$. Thus $\lambda_i$ is a eigenvalue of $T$. If $\alpha$ in $W_i$, we have
	\[T\alpha=TE_i\alpha=\lambda_i\alpha\]
	Conversely, if $\alpha$ is a vector in the characteristic space for $T$ associated with $\lambda_i$, we have
	\[(T-\lambda_i I)\alpha=\sum\limits_{j=1}^k(\lambda_j-\lambda_i)E_j\alpha=0\]
	which means $E_j\alpha=0,j\neq i$ and $\alpha=E_i\alpha$.
	Then we have finished the proof.  
\end{proof}
\begin{dde}
	Let $N$ be a linear operator on a vector space $V$. We say that $N$ is \textbf{nilpotent} if there is some positive integer $r$ such that $N^r=0$.
\end{dde}
\begin{thm}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$ over the field $F$. Suppose that the minimal polynomial for $T$ is a product of linear polynomials. Then there is a diagonalizable operator $D$ on $V$ and a nilpotent operator $N$ on $V$ such that
	\begin{enumerate}
		\item [(\romannumeral1)]$T=D+N$
		\item [(\romannumeral2)]$DN=ND$
	\end{enumerate}
	The diagonalizable operator $D$ and the nilpotent operator $N$ are uniquely determined by (\romannumeral1)(\romannumeral2), and each of them is a polynomial of $T$. We call $D$ the \textbf{diagonalizable part} of $T$.
\end{thm}
\begin{proof}
	Use Theorem \ref{PrimeDecomposition},\ref{directsumdecomposition} and \ref{EigenvalueDecomposition}, we have $W_i$ is the null space of $(T-\lambda_i I)^r{i}$, and then let
	\[D=\lambda_1E_1+\cdots+\lambda_kE_k\]
	Let $N=T-D$, we have
	\[N^r=(T-\lambda_1 I)^rE_1+\cdots+(T-\lambda_k I)=0\]
	when $r\ge r_i,i=1,2,\dots,k$.\\
	You may notice that in the proof of Theorem \ref{PrimeDecomposition}, each $E_i$ is a polynomial of $T$. This is also correct in the Theorem \ref{EigenvalueDecomposition}.\\
	If there exist a diagonalizable operator $D'$ and a nilpotent operator $N'$ satisfy (\romannumeral1)(\romannumeral2), then we have
	\[D-D'=N'-N\]
	Since $D',N'$ commute with each other, they commute with $T$, as well as $D,N$. Then we know that $D,D'$ can be diagonalized simultaneously and $D-D'$ is diagonalzable. Since $N,N'$ are both nilpotent, we have
	\[(N'-N)^r=\sum\limits_{i=0}^r(N')^i(-N)^{r-i}\]
	when $r$ is large enough, this will be zero. Thus $N'-N$ is nilpotent, which means the minimal polynomial for $D-D'$ divides $x^m$ for some $m$, and then is $x^r$. Because $D-D'$ is diagonalizable, $r=1$, which is just $D-D'=0,N'-N=0$.
\end{proof}
\begin{coro}
	Let $V$ be a finite-dimensional vector space over an algebraically closed field $F$. Then every linear operator $T$ on $V$ can be written as the sum of a diagonalizable operator $D$ and a nilpotent operator $N$ which commute. These operators are unique and each is a polynomial of $T$.
\end{coro}
We will go further in how to decompose a matrix in the next section.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{shinobu2}
\end{figure}
\newpage
\section{The Rational and Jordan Forms}
\subsection{Cyclic Decomposition and the Rational Form}
We have found a useful decomposition in the previous section. In this section, we will make the decomposition more delicate.\\
\begin{dde}
	If $\alpha$ is any vector in $V$, the $T$-\textbf{cyclic subspace generated by} $\alpha$ is the subspace $Z(\alpha;T)$ of all vectors of the form $g(T)\alpha$, $g$ in $F[x]$. If $Z(\alpha;T)=V$, then $\alpha$ is called a \textbf{cyclic vector} for $T$.
\end{dde}
\begin{dde}
	If $\alpha$ is any vector in $V$, the $T$-\textbf{annihilator} of $\alpha$ is the ideal $M(\alpha;T)$ in $F[x]$ consisting of all polynomials $g$ over $F$ such that $g(T)\alpha=0$. The unique generator $p_\alpha$ is also called the $T$-\textbf{annihilator} of $\alpha$.
\end{dde}
This concept we have introduced in the previous section.
\begin{thm}
	Let $\alpha$ be any nonzero vector in $V$ and let $p_\alpha$ be the $T$-annihilator of $\alpha$. Then we have
	\begin{enumerate}
		\item [(\romannumeral1)]$\deg p_\alpha=\dim Z(\alpha;T)$
		\item [(\romannumeral2)]If $\deg p_\alpha=k$, then $\alpha,T\alpha,\dots,T^{k-1}\alpha$ form an ordered basis for $Z(\alpha;T)$
		\item [(\romannumeral3)]If $U$ is the linear operator on $Z(\alpha;T)$ induced by $T$, then the minimal polynomial for $U$ is $p_\alpha$
	\end{enumerate} 
\end{thm}
\begin{proof}
	First, let $g$ be any polynomial in $F[x]$, we have
	\[g=p_\alpha q+r\]
	where $r=0$ or $\deg r<k$. Obviously we have
	\[g(T)\alpha=r(T)\alpha\]
	We know that $r(T)\alpha$ is a linear combination of $\alpha,T\alpha,\dots,T^{k-1}\alpha$, which means $Z(\alpha;T)$ is spanned by $\alpha,T\alpha,\dots,T^{k-1}\alpha$. We also know that $\alpha,T\alpha,\dots,T^{k-1}\alpha$ are linearly independent, otherwise there exists another polynomial $g$ with $\deg g<\deg p_\alpha$ such that
	\[g(T)\alpha=(c_0+c_1T+\cdots+c_{k-1}T^{k-1})\alpha=0\]
	which is absurd.\\
	Let $U$ be the linear operator on $Z(\alpha;T)$ induced by $T$. We have
	\[p_\alpha(U)g(T)\alpha=p_\alpha(T)g(T)\alpha=0\]
	and if $\deg h<\deg p_\alpha,h(U)\alpha=h(T)\alpha=0$, then it contradicts to the definition of $p_\alpha$. Thus $p_\alpha$ is the minimal polynomial for $U$.
\end{proof}
One thing you may notice is that if $\alpha$ is a cyclic vector for $T$, then we have $\deg p_\alpha=n$, and then Cayley-Hamilton theorem tells us that the characteristic polynomial for $T$ is just the minimal polynomial.\\
\indent Let's take a look at the matrix of $U$ under the basis above. Let $p_\alpha$ be 
\[c_0+c_1x+\cdots+c_{k-1}x^{k-1}+x^{k}\]
We notice that
\[\alpha_{i+1}=U\alpha_i,i=1,2,\dots,k-1\]
\[U\alpha_k=-c_0\alpha_1-c_1\alpha_2-\cdots-c_{k-1}\alpha_{k}\]
then we have
\[\begin{bmatrix}
	0&0&0&\cdots&0&-c_0\\
	1&0&0&\cdots&0&-c_1\\
	0&1&0&\cdots&0&-c_2\\
	\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
	0&0&0&\cdots&1&-c_{k-1}
\end{bmatrix}\]
The matrix above is called the \textbf{companion matrix} of the monic polynomial $p_\alpha$.
\begin{thm}
	If $U$ is a linear operator on a finite-dimensional vector space $W$, then $U$ has a cyclic vector if and only if there is some ordered basis for $W$ in which $U$ is represented by the companion matrix of the minimal polynomial for $U$.
\end{thm}
The proof is easy.
\begin{coro}
	If $A$ is the companion matrix of a monic polynomial $p$, then $p$ is both the minimal and the characteristic polynomial for $A$.
\end{coro}
If $V=W\oplus W'$, we call $W'$ is \textbf{complementary} to $W$. We want $W,W'$ are $T$-invariant subspaces. If this is true, we have
\[\beta=\gamma+\gamma',\gamma\in W,\gamma'\in W'\]
\[f(T)\beta=f(T)\gamma+f(T)\gamma',f(T)\gamma\in W,f(T)\gamma'\in W'\]
and we know that $f(T)\beta\in W$ if and only if $f(T)\gamma'=0$, and then we have $f(T)\beta=f(T)\gamma,\gamma\in W$.
\begin{dde}
	Let $T$ be a linear operator on $V$ and let $W$ be a subspace of $V$. We say that $W$ is $T$-\textbf{admissible} if
	\begin{enumerate}
		\item [(\romannumeral1)]$W$ is invariant under $T$
		\item [(\romannumeral2)]If $f(T)\beta\in W$, there exists a vector $\gamma\in W$ such that $f(T)\beta=f(T)\gamma$
	\end{enumerate}
\end{dde}
We have known that if $W$ is a invariant space under $T$ and has complementary invariant subspaces, then $W$ is $T$-admissible. Let's see what will happen when $W$ is a $T$-admissible subspace. Let $\beta$ be a vector not in $W$ and let $f=s_T(\beta;T)$, then we have $f(T)\beta\in W$ and there exists some $\gamma\in W$ such that $f(T)\beta=f(T)\gamma$. Let $\alpha=\beta-\gamma$, for any polynomial we have
\[g(T)(\beta-\alpha)=g(T)\gamma\in W\]
which means $S_T(\alpha;W)=S_T(\beta;W)$, and $f=s_T(\alpha;W)$ as well. However we have $f(T)\alpha=0$, which means $g(T)\alpha\in W$ if and only if $g(T)\alpha=0$, and thus we have
\[W\cap Z(\alpha;T)=\{0\}\]
This tells us that we can find a vector not in $W$ whose cyclic subspace is independent with $W$.\\
\begin{thm}[Cyclic Decomposition Theorem]
	Let $T$ be a linear operator on a finite-dimensional vector space $V$ and let $W_0$ be a proper $T$-admissible subspace of $V$. There exist nonzero vectors $\alpha_1,\alpha_2,\dots,\alpha_r$ in $V$ with respective $T$-annihilator $p_1,p_2,\dots,p_r$ such that
	\begin{enumerate}
		\item [(\romannumeral1)]$V=W_0\oplus Z(\alpha_1;T)\oplus\cdots\oplus Z(\alpha_r;T)$
		\item [(\romannumeral2)]$p_k$ divides $p_{k-1},k=2,\dots,r$
	\end{enumerate}
	Furthermore, the integer $r$ and the annihilators $p_1,\dots,p_r$ are uniquely determined by the conditions above.
\end{thm}
\begin{proof}
	The central method is to find $\alpha$ like what we did above. First, obviously we can find $\beta_i$ such that
	\[V=W_0+Z(\beta_1;T)+\cdots+Z(\beta_r;T)\]
	and $p_k=s_T(\beta_k;W_{k-1})$ which satisfies
	\[\deg p_k=\max\limits_{\alpha\in V}\deg s_T(\alpha;W_{k-1})\]
	This is guaranteed by
	\[0<\max_{\alpha\in V}\deg s_T(\alpha;W_{k-1})\le\dim V\]
	if $W_{k-1}$ is a proper invariant subspace. Let $\beta$ be any vector in $V$ and let $f=s_T(\beta;W_{k-1})$. If
	\[f(T)\beta=\beta_0+\sum_{i=1}^{k-1}g_i(T)\beta_i,\beta_i\in W_i\]
	then $f$ divides each polynomial $g_i$ and there exists a $\gamma_0\in W_0$ such that $f(T)\beta_0=f(T)\gamma_0$. This is easy to prove if we let
	\[g_i=fh_i+r_i,r_i=0\text{ or }\deg r_i<\deg f\]
	and 
	\[\gamma=\beta-\sum_{i=1}^{k-1}h_i(T)\beta_i\]
	then
	\[f(T)\gamma=\beta_0+\sum_{i=1}^{k-1}r_i(T)\beta_i\]
	Like we discussed above, $f=s_T(\gamma;W_{k-1})$. If there exists some $r_i\neq0$, then we have
	\[f(T)\gamma=\beta_0+\sum_{i=1}^{j}r_i(T)\beta_i\]
	Let $p=s_T(\gamma;W_{j-1})$, obviously we have
	\[p=fg\]
	and then
	\[p(T)\gamma=gf(T)\gamma=g(T)\beta_0+\sum_{i=1}^{j}gr_i(T)\beta_i\]
	thus $gr_j(T)\beta_j\in W_{j-1}$,. which means
	\[\deg(gr_i)\ge\deg p_j\ge\deg s_T(\gamma;W)=\deg(fg)\]
	and $\deg r_i\ge\deg f$, which is impossible. Thus $f$ divides each $g_i$, then $\beta_0=f(T)\gamma\in W_0$. Because $W_0$ is $T$-admissible, there exits a vector $\gamma_0\in W_0$ such that
	\[f(T)\gamma_0=f(T)\gamma=\beta_0\]
	Then let $\beta=\beta_k$ and $f=p_k$, which is obviously satisfies the condition, we have
	\[p_k(T)\beta_k=p_k(T)\gamma_0+\sum_{i=1}^{k-1}p_kh_i(T)\beta_i\]
	and let
	\[\alpha_k=\beta_k-\gamma_0-\sum_{i=1}^{k-1}h_i(T)\beta_i\]
	We then have $s_T(\alpha_k;W_{k-1})=s_T(\beta_k;W_{k-1})=p_k$ and $p_k$ is the $T$-annihilator of $\alpha_k$, which means
	\[W_{k-1}\cap Z(\alpha_k;T)=\{0\}\]
	We have proven (\romannumeral1)(\romannumeral2).\\
	Before the proof of uniqueness, we notice some easy lemmas. Let $W$ be a subspace of $V$ and let $fW=\{f(T)\alpha|\alpha\in W\}$, we have
	\begin{enumerate}
		\item $fZ(\alpha;T)=Z(f(T)\alpha;T)$
		\item If $V=V_1\oplus\cdots\oplus V_k$ and $V_i$ is $T$-invariant, then $fV=fV_1\oplus\cdots fV_k$
		\item If $\alpha,\gamma$ have the same annihilator, then $f(T)\alpha,f(T)\gamma$ have the same annihilator, and thus
		\[\dim Z(f(T)\alpha;T)=Z(f(T)\gamma;T)\]
	\end{enumerate}
	If we let $S_T(V;W)=\{f\in F[x]|\forall \beta\in V,f(T)\beta\in W\}$ which is obviously an ideal in $F[x]$, then we notice
	\[p_1(T)\beta=p_1(T)\beta_0+\sum_{i=1}^rp_1f_i(T)\beta_i\]
	and because $p_i$ divides $p_1$, $p_1(T)\beta=p_1(T)\beta_0\in W_0$. As $p_1$ is the monic polynomial with the least degree to send $\alpha_1$ to $W_0$, it is just the generator of $S_T(V;W_0)$. If there exist another $s$ and $p_1',\dots,p'_{s}$, we have the same conclusion, which means
	\[p_1=p'_1\]
	Use induction. Suppose we have proven $p_{i}=p'_i,i=1,2,\dots,k-1$, we have
	\[p_kV=p_kW_0\oplus Z(p_k(T)\alpha_1;T)\oplus\cdots\oplus Z(p_k(T)\alpha_{k-1};T)\]
	and
	\[p_kV=p_kW_0\oplus Z(p_k(T)\gamma_1;T)\oplus\cdots\oplus Z(p_k(T)\gamma_s;T)\]
	however for $i=1,\dots,k-1$ we have $p_{i}=p'_{i}$, which means $\dim Z(p_k(T)\alpha_{i};T)=\dim Z(p_k(T)\gamma_{i};T)$, then
	\[\dim Z(p_k(T)\gamma_i;T)=0,i=k,\dots,s\]
	then $p'_k$ divides $p_k$, and $p_k$ divides $p'_k$ can be proven similarly. Thus we have done the rather long proof.
\end{proof}
\begin{coro}
	If $T$ is a linear operator on a finite-dimensional vector space $V$, then every $T$-admissible subspace has a complementary invariant subsspace.
\end{coro}
\begin{proof}
	If $W_0=V$, then this is $\{0\}$. If $W_0\neq V$, then this is $Z(\alpha_1;T)\oplus\cdots\oplus Z(\alpha_r;T)$.
\end{proof}
\begin{coro}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$.
	\begin{enumerate}
		\item [(a)]There exists a vector $\alpha\in V$ such that the $T$-annihilator of $\alpha$ is just the minimal polynomial for $T$
		\item [(b)]$T$ has a cyclic vector if and only if the characteristic and the minimal polynomials for $T$ are identical
	\end{enumerate}
\end{coro}
\begin{proof}
	Let $V=Z(\alpha_1;T)\oplus\cdots\oplus Z(\alpha_r;T)$. Apparently, $p_1$ is just the minimal polynomial for $T$. If the characteristic and the minimal polynomials for $T$ are the same, then $\dim Z(\alpha_1;T)=\dim V$ and $\alpha_1$ is just a cyclic vector of $T$.
\end{proof}
\begin{thm}[Generalized Cayley-Hamilton Theorem]
	Let $T$ be a linear operator on a finite-dimensional vector space $V$. Let $p,f$ be the minimal and the characteristic polynomials for $T$ respectively.
	\begin{enumerate}
		\item [(\romannumeral1)]$p$ divides $f$
		\item [(\romannumeral2)]$p$ and $f$ have the same prime factors, except for multiplicities.
		\item [(\romannumeral3)]If \[p=f_1^{r_1}\cdots f_k^{r_k}\]
		is the prime factorization of $p$, then
		\[f=f_1^{d_1}\cdots f_k^{d_k}\]
		where $d_i$ is the nullity of $f_i(T)^{r_i}$ divided by the degree of $f_i$
	\end{enumerate}
\end{thm}
\begin{proof}
	We know that $p_1=p$. Let $U_i$ be the linear operator on $Z(\alpha_i;T)$ induced by $T$, $\alpha_i$ is a cyclic vector of $U_i$. Thus the minimal polynomial for $U_i$ is $p_i$ as the same as the characteristic polynomial for $U_i$. Obviously, we have
	\[f=p_1p_2\cdots p_r\]
	Thus $p$ divides $f$. A prime factor of $p$ is certainly a prime factor of $f$. Because $p_{i+1}$ divides $p_i$, a prime factor of $f$ is surely a prime factor of $p$ as well.\\
	Employ the primary decomposition theorem, we have
	\[V=V_1\oplus\cdots\oplus V_k\]
	where $V_i$ is the null space of $f_i^{r_i}(T)$. Let $T_i$ be the linear operator on $V_i$ induced by $T$, use (\romannumeral2) we know that the characteristic polynomial for $T_i$ is $f_i^{d_i}$, whose degree is $\dim V_i$. Hence
	\[d_i=\frac{nullity(f_i^{r_i}(T))}{\deg f_i}\]
\end{proof}
\begin{coro}
	If $T$ is a nilpotent linear operator on a vector space with dimension $n$, then the characteristic polynomial for $T$ is $x^n$. 
\end{coro}
Now let's take a look at the matrix of $T$ under a basis formed by the cyclic bases from each cyclic subspace, which just means $\mathcal{B}=\{\mathcal{B}_1,\dots,\mathcal{B_r}\}$, where $\mathcal{B}_i$ is
\[\alpha_i,T\alpha_i,\dots,T^{k_i-1}\alpha_i\]
where $k_i=\deg p_i$. Thus the matrix is
\[\begin{bmatrix}
	A_1&0&\cdots&0\\
	0&A_2&\cdots&0\\
	\vdots&\vdots&\ddots&\vdots\\
	0&0&\cdots&A_r
\end{bmatrix}\]
where $A_i$ is the companion matrix of $p_i$. A matrix like this with $A_i$ is the companion matrix of $p_i$ and $p_{i+1}$ divides $p_i$ is said to be in \textbf{rational form}.
\begin{thm}
	Let $F$ be a field and let $B$ be an $n\times n$ matrix over $F$. Then $B$ is similar over the field $F$ to an one and only one matrix which is in rational form.
\end{thm}
The polynomials $p_1,p_2,\dots,p_r$ are called the \textbf{invariant factors} for the matrix $B$.\\
\indent It is interesting when $T$ is diagonalizable. Then we have
\[V=V_1\oplus\cdots\oplus V_k\]
where $V_i$ is the characteristic space associated with the eigenvalue $\lambda_i$. Then
\[\alpha=\beta_1+\cdots+\beta_k,\beta_i\in V_i\]
and
\[f(T)\alpha=f(\lambda_1)\beta_1+\cdots+f(\lambda_k)\beta_k\]
which means the $T$-annihilator of $\alpha$ is just
\[\prod_{\beta_i\neq0}(x-\lambda_i)\]
Now let $\mathcal{B}_i=\{\beta_{i1},\dots,\beta_{id_i}\}$ and let $r=\max d_i$. We define
\[\alpha_j=\sum_{d_i\ge j}\beta_{ij},1\le j\le r\]
The $T$-annihilator of $\alpha_j$ is
\[p_j=\prod_{d_i\ge j}(x-\lambda_i)\]
and we certainly have
\[V=Z(\alpha_1;T)\oplus\cdots Z(\alpha_r;T)\]
which can be proven easily. 
\subsection{The Jordan Form}
Let $N$ be a nilpotent linear operator on a finite-dimensional vector space $V$. Use the cyclic decomposition theorem and notice that $p_i=x^{k_i}$, we have
\[A_i=\begin{bmatrix}
	0&0&\cdots&0&0\\
	1&0&\cdots&0&0\\
	0&1&\cdots&0&0\\
	\vdots&\vdots&\ddots&\vdots&\vdots\\
	0&0&\cdots&1&0
\end{bmatrix}\]
where $A_i$ is a $k_i\times k_i$ matrix, and $k_1\ge k_2\ge\cdots\ge k_r,k_1+\cdots+k_r=n$.\\
\indent One thing you may notice is that the $r$ vectors $N^{k_i-1}\alpha_i$ form a basis for the null space of $N$.\\
\indent We can write a vector $\alpha$ in the null space as
\[\alpha=f_1(N)\alpha_1+\cdots+f_r(N)\alpha_r\]
where $\deg f_i<k_i$. For each $i$ we have
\[N\alpha=Nf_i(N)\alpha_i=xf_i(N)\alpha_i\]
which means $x^{k_i}$ divides $xf_i$, and thus $\deg(xf_i)\le k_i\rightarrow f_i=c_ix^{k_i-1}$. Then
\[\alpha=c_1N^{k_1-1}\alpha_1+\cdots+c_rN^{k_r-1}\alpha_r\]
and these vectors are linearly independent apparently.\\
\indent Now let's combine this with the primary decomposition theorem. Let $T$ be a linear operator on a finite-dimensional vector space $V$ and let $f$ be the characteristic polynomial for $T$. Suppose $f$ is the product of linear factors
\[f=(x-\lambda_1)^{d_1}\cdots(x-\lambda_k)^{d_k}\]
and then the minimal polynomial $p$ has the form
\[p=(x-\lambda_1)^{r_1}\cdots(x-\lambda_k)^{r_k},1\le r_i\le d_i\]
We have
\[V=W_1\oplus\cdots\oplus W_k\]
where $W_i$ is the null space of $(T-\lambda_iI)^{r_i}$. Let $T_i$ be the linear operator on $W_i$ induced by $T$, then the minimal polynomial for $T_i$ is $(x-\lambda_i)^{r_i}$. Let $N_i=T_i-\lambda_iI$, then $N_i$ is nilpotent and the minimal polynomial for $N_i$ is $x^{r_i}$. Choose a basis for $W_i$ to make the matrix of $N_i$ be in rational form, then we notice that the matrix of $T_i$ is the direct sum of the matrix
\[\begin{bmatrix}
	\lambda_i&0&\cdots&0&0\\
	1&\lambda_i&\cdots&0&0\\
	\vdots&\vdots&\ddots&\vdots&\vdots\\
	0&0&\cdots&\lambda_i&0\\
	0&0&\cdots&1&\lambda_i
\end{bmatrix}\]
A matrix like this is called an \textbf{elementary Jordan matrix with eigenvalue} $\lambda_i$. Then the matrix of $T$ has the form
\[\begin{bmatrix}
	A_1&0&\cdots&0\\
	0&A_2&\cdots&0\\
	\vdots&\vdots&\ddots&\vdots\\
	0&0&\cdots&A_k
\end{bmatrix}\]
Each $A_i$ has the form
\[A_i=\begin{bmatrix}
	J_1(\lambda_i)&0&\cdots&0\\
	0&J_2(\lambda_i)&\cdots&0\\
	\vdots&\vdots&\ddots&\vdots\\
	0&0&\cdots&J_{n_i}(\lambda_i)
\end{bmatrix}\]
where $J_j(\lambda_i)$ is an elementary Jordan matrix with eigenvalue $\lambda_i$. The size of $J_j(\lambda_i)$ decreases as $j$ increases. A matrix like this is said to be in \textbf{Jordan form}.\\
\indent If the characteristic polynomial for a matrix $A$ is a product of linear factors, then it is similar to a unique matrix in Jordan form except for the order of $\lambda_i$. This is easy to prove.\\
\indent Let's take a deeper look at the matrix of $T$ in Jordan form. First we find that $n_i$ is just the dimension of the null space of $N_i=T-\lambda_iI$, and thus we have $n_i\le d_i$, which we have proven before. Obviously $n_i=d_i$ if and only if $T$ is diagonalizable, and then the matrix is just a diagonal matrix.\\
\indent We can also find that $J_1(\lambda_i)$ is an $r_i\times r_i$ matrix, where $r_i$ is the multiplicity of $\lambda_i$ as a root of the minimal polynomial.\\
\indent If $F$ is algebraically closed, then every $n\times n$ matrix is similar to an essentially unique matrix in Jordan form.\\
\indent What we need to do next is to find a way to compute the rational form. An important method is to compute the invariant factors first, then the rational form is easy to write.
\begin{dde}
	A \textbf{ring} is a set $K$, together with two operation $(x,y)\rightarrow x+y$ and $(x,y)\rightarrow xy$ satisfying
	\begin{enumerate}
		\item [(a)]$K$ is a commutative group under addition
		\item [(b)]Multiplication is associative
		\item [(c)]$x(y+z)=xy+xz,(y+z)x=yx+zx$
	\end{enumerate}
	If $xy=yx,\forall x,y\in K$, we say that the ring $K$ is \textbf{commutative}. If there is an element $1$ in $K$ such that $1x=x1=x$ for each $x$, $K$ is said to be a \textbf{ring with identity}, and $1$ is called the \textbf{identity} for $K$.
\end{dde}
Obviously $F[x]$ is a commutative ring with identity $1$. We can define a \textbf{module over} $K$ like a vector space over a field. Because the invariant factors are polynomials in $F[x]$, we need to study the matrix over $F[x]$, denoted by $F[x]^{m\times n}$. We don't want to delve deeper into the module, all we need to remember here is that we can't do any thing about division unless the element in $F[x]$ is a nonzero scalar.\\
\indent Having the caution in mind, we can define the \textbf{elementary row operation} on a matrix $M$ similarly:
\begin{enumerate}
	\item Multiplication of one row of $M$ by a nonzero scalar in $F$
	\item Replacement of the $r$th row of $M$ by row $r$ plus $f$ times row $s$, where $f$ is any polynomial over $F$ and $r\neq s$
	\item Interchange two rows of $M$
\end{enumerate}
Then the operation is surely inversible. We can do the same thing to define \textbf{elementary matrix}, \textbf{row-equivalent} and so on. What we want to do is to transform the matrix into a simple form like row-reduced echelon matrix.
\begin{lem}
	Let $M$ be a matrix in $F[x]^{m\times n}$ which has some nonzero entry in its first column, and let $p$ be the greatest common divisor of the entries in the first column of $M$. Then $M$ is row-equivalent to a matrix $N$ which has
	\[\begin{bmatrix}
		p\\
		0\\
		\vdots\\
		0
	\end{bmatrix}\]
	as its first column
\end{lem}
\begin{proof}
	This is just to do Euclidean algorithm in the first column. Let the first column of $M$ be
	\[\begin{bmatrix}
		f_1\\
		\vdots\\
		f_m
	\end{bmatrix}\]
	First, find the polynomial $f_j$ with the least degree in the first column, then we can use the elementary row operations to transform the first column into
	\[\begin{bmatrix}
		f_j\\
		r_1\\
		\vdots\\
		r_{j-1}\\
		r_{j+1}\\
		\vdots
		r_m
	\end{bmatrix}\]
	where $f_i=f_jg_i+r_i,i\neq j,r_i=0\text{ or }\deg r_i<\deg f_j$. Repeat the process, we will end when some entry in the first column be the greatest common divisor and then we obviously have
	\[\begin{bmatrix}
		p\\
		0\\
		\vdots\\
		0
	\end{bmatrix}\]
\end{proof}
\begin{thm}
	Let $P$ be a matrix in $F[x]^{m\times m}$. The following are equivalent.
	\begin{enumerate}
		\item [(\romannumeral1)]$P$ is invertible
		\item [(\romannumeral2)]$\det P$ is a nonzero scalar in $F$
		\item [(\romannumeral3)]$P$ is row-equivalent to the identity matrix
		\item [(\romannumeral4)]$P$ is a product of elementary matrices
	\end{enumerate}
\end{thm}
\begin{proof}
	From (\romannumeral1) to (\romannumeral2) is easy because $\det P\det P^{-1}=1$ if and only if $\det P$ has a inverse. We can define the classical adjoint of $P$ to prove (\romannumeral2) to (\romannumeral1) easily. It's obviously to prove (\romannumeral3) to (\romannumeral4), (\romannumeral4) to (\romannumeral1). Thus we shall prove (\romannumeral2) to (\romannumeral3).\\
	Because $\det P$ is a nonzero scalar, and we know any common divisor entries in the first column divide $\det P$ as well. Thus we have
	\[\gcd(p_1,p_2,\dots,p_m)=1\]
	and $P$ is row-equivalent to a matrix $Q$ which has the form
	\[\begin{bmatrix}
		1&\alpha\\
		0&B
	\end{bmatrix}\]
	$\det Q$ is also a nonzero scalar because Doing elementary row operations is just multiply a nonzero scalar to the determinant. Then repeat the process we have a matrix $R$ which has the form
	\[\begin{bmatrix}
		1&a_{12}&\cdots&a_{1m}\\
		0&1&\cdots&a_{2m}\\
		\vdots&\vdots&\ddots&\vdots\\
		0&0&\cdots&1
	\end{bmatrix}\]
	which is simply row-equivalent to the identity matrix.
\end{proof}
\begin{coro}
	Let $M,N$ be matrices in $F[x]^{m\times n}$. Then $N$ is row-equivalent to $M$ if and only if 
	\[N=PM\]
	where $P$ is an invertible matrix in $F[x]^{m\times m}$.
\end{coro}
We can define \textbf{elementary column operations} and \textbf{column-equivalent} as well.
\begin{dde}
	Let $M,N$ be matrices in $F[x]^{m\times n}$. We say $N$ is \textbf{equivalent} to $M$ if we can use a series of elementary row and column operations to transform $M$ to $N$. 
\end{dde}
\begin{thm}
	Let $M,N$ be matrices in $F[x]^{m\times n}$. Then $N$ is equivalent to $M$ if and only if 
	\[N=PMQ\]
	where $P$ is an invertible matrix in $F[x]^{m\times m}$ and $Q$ is an invertible matrix in $F[x]^{n\times n}$.
\end{thm}
\begin{thm}
	Let $A$ be a matrix in $F^{n\times n}$ and let $p_1,\dots,p_r$ be the invariant factors for $A$. The matrix $xI-A$ is equivalent to the $n\times n$ diagonal matrix with diagonal entries $p_1,\dots,p_r,1,\dots,1$.
\end{thm}
\begin{proof}
	There exists an invertible matrix $P$ such that
	\[PAP^{-1}=\begin{bmatrix}
		A_1&0&\cdots&0\\
		0&A_2&\cdots&0\\
		\vdots&\vdots&\ddots&\vdots\\
		0&0&\cdots&A_r
	\end{bmatrix}\]
	and thus 
	\[xI-PAP^{-1}=\begin{bmatrix}
		xI-A_1&0&\cdots&0\\
		0&xI-A_2&\cdots&0\\
		\vdots&\vdots&\ddots&\vdots\\
		0&0&\cdots&xI-A_r
	\end{bmatrix}\]
	It's easy to prove that $xI-A_i$ is equivalent to
	\[\begin{bmatrix}
		p_i&0&\cdots&0\\
		0&1&\cdots&0\\
		\vdots&\vdots&\ddots&\vdots\\
		0&0&\cdots&1
	\end{bmatrix}\]
	Then we done the prove.
\end{proof}
\begin{dde}
	Let $N$ be a matrix in $F[x]^{m\times n}$. We say that $N$ is in (Smith) \textbf{normal form} if the nonzero entries are only appear on the main diagonal and $f_k$ divides $f_{k+1},k=1,2,\dots,s-1$. \\The main diagonal of $N$ is the entries $N_{kk},k=1,2,\dots,l,l=\min\{m,n\}$ and we have
	\[N_{kk}=\begin{cases}
		f_k&k\le s\\
		0&k> s
	\end{cases}\]
	where $s\le l$.
\end{dde}
\begin{thm}
	Let $M$ be a matrix in $F[x]^{m\times n}$. Then $M$ is equivalent to a matrix $N$ which is in normal form.
\end{thm}
\begin{proof}
	If $M=0$, then nothing need to prove. If $M\neq0$, we can use the elementary row and column operations to make the first row and column have at least one nonzero entry. Then apply Euclidean algorithm like what we did above, we have
	\[\begin{bmatrix}
		f&0\\
		0&R
	\end{bmatrix}\]
	We can then use the elementary row and column operations to let the nonzero entries in $R$ which can not be divided by $f$ be in the first column or row, and repeat the process. This will end in finite step obviously. Then we have
	\[\begin{bmatrix}
		f_1&0\\
		0&R
	\end{bmatrix}\]
	and each entry in $R$ can be divided by $f_1$. Repeat the process until $R=0$, then we have finished the proof.
\end{proof}
\begin{dde}
	Let $M$ be a matrix in $F[x]^{m\times n}$. If $1\le k\le s$, we define $\delta_k(M)$ to be the greatest common divisor of the determinants of all $k\times k$ submatrices of $M$. $s$ is the largest integer $k$ such that not all determinants of $k\times k$ submatrices of $M$ are $0$.
\end{dde}
\begin{thm}
	If $M,N$ are matrices in $F[x]^{m\times n}$ and are equivalent, then
	\[\delta_k(M)=\delta_k(N),1\le k\le s\]
\end{thm}
The proof is easy and let's skip it.
\begin{coro}
	Let $M$ be a matrix in $F[x]^{m\times n}$. Then $M$ is equivalent to a unique matrix $N$ which is in normal form and 
	\[f_k=\frac{\delta_k(M)}{\delta_{k-1}(M)},1\le k\le s\]
	where we define $\delta_0(M)=1$.
\end{coro}
We call $N$ the \textbf{normal form} of $M$, and call $f_1,\dots,f_s$ the \textbf{invariant factors} of $M$.\\
\indent Now we know that the invariant factors of $xI-A$ is just $1,\dots,1,p_r,\dots,p_1$, and we have
\[p_i=\frac{\delta_{n+1-i}(xI-A)}{\delta_{n-i}(xI-A)}\]
In the end of this section, we will discuss semi-simple operator, which is a general version of diagonalizable operator.
\begin{dde}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$ over the field $F$. We say that $T$ is \textbf{semi-simple} if every $T$-invariant subspace has a complementary $T$-invariant subspace. 
\end{dde}
\begin{lem}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$ and let $V=W_1\oplus\cdots\oplus W_k$ be the primary decomposition of $T$. Let $W$ be any $T$-invariant subspace of $V$, then
	\[W=(W\cap W_1)\oplus\cdots\oplus(W\cap W_k)\]
\end{lem}
\begin{proof}
	Let $E_j$ be the projection of $W_j$, which is a polynomial of $T$. If $\alpha$ is a vector in $W$, then we have
	\[\alpha=\alpha_1+\cdots+\alpha_k,\alpha_i\in W_i\]
	and $\alpha_i=E_i\alpha=h_i(T)\alpha\in W$. Thus $\alpha_i\in W\cap W_i$, and then
	\[W=(W\cap W_1)\oplus\cdots\oplus(W\cap W_k)\] 
\end{proof}
\begin{lem}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$ and suppose the minimal polynomial $p$ for $T$ is irreducible over the field $F$. Then $T$ is semi-simple.
\end{lem}
\begin{proof}
	We just need to prove every invariant subspace $W$ is $T$-admissible. If $f(T)\beta=0$, then let $\alpha=0$. If $f(T)\beta\neq0$, then $f$ can not be divided by $p$. Because $p$ is a prime polynomial, we have
	\[fg+pq=1\]
	As $p(T)=0$, we have $f(T)g(T)=I$, which implies
	\[\beta=g(T)(f(T)\beta)\in W\]
	thus let $\alpha=\beta$.
\end{proof}
\begin{thm}
	Let $T$ be a linear operator on a finite-dimensional vector space $V$. $T$ is semi-simple if and only if the minimal polynomial for $T$ has the form
	\[p=p_1\cdots p_k\]
	where $p_i$ are distinct prime polynomials.
\end{thm}
\begin{proof}
	If there exists a prime polynomial $g$ such that $p=g^2h$. Because $\deg gh<\deg p$, there exists a vector $\beta$ such that $gh(T)\beta\neq0$. Let $W$ be the null space of $g(T)$, we have $g(gh(T))\beta=0$, which implies that $\beta\in W$. However there is no vector $\alpha\in W$ such that $gh(T)\beta=gh(T)\alpha$, because
	\[gh(T)\alpha=hg(T)\alpha=0\neq gh(T)\beta\]
	Thus $W$ is not $T$-admissible, and $T$ is not semi-simple.\\
	If $p=p_1\cdots p_k$, use the prime decomposition we have
	\[V=W_1\oplus\cdots\oplus W_k\]
	Let $W$ be a $T$-invariant subspace. Let $T_i$ be the linear operator on $W_i$ induced by $T$. The minimal polynomial for $T_i$ is $p_i$, which implies that $T_i$ is semi-simple. As $W\cap W_i$ is obviously a $T_i$-invariant subspace, we have
	\[W_i=(W\cap W_i)\oplus V_i\]
	and then
	\begin{align*}
		V&=W_1\oplus\cdots\oplus W_k\\
		&=(W\cap W_1)\oplus\cdots\oplus (W\cap W_k)\oplus V_1\oplus\cdots\oplus V_k\\
		&=W\oplus(V_1\oplus\cdots\oplus V_k)
	\end{align*}
	which means $W$ has a complementary invariant subspace $W'=V_1\oplus\cdots\oplus V_k$. Thus $T$ is semi-simple. 
\end{proof}
\begin{coro}
	If $T$ is a linear operator on a finite-dimensional vector space over an algebraically closed field, then $T$ is semi-simple if and only if $T$ is diagonalizable
\end{coro}
\begin{lem}
	Let $g,h$ be polynomials in $F[x]$. If $f$ is any polynomial in $F[x]$ with $\deg f\le n$, then
	\[f(g)=\sum_{k=0}^n\frac{f^{(k)}(h)}{k!}(g-h)^k\]
\end{lem}
The proof is almost the same as the simple Taylor's formula.
\begin{lem}
	Let $F$ be a subfield of the field of complex numbers. Let $f$ ba a polynomial in $F[x]$ and let $f'$ be the derivative of $f$, then the following are equivalent:
	\begin{enumerate}
		\item [(a)]$f$ is the product of distinct prime polynomials over $F$
		\item [(b)]$f$ and $f'$ are relatively prime
		\item [(c)]As a polynomial with complex coefficients, $f$ has no repeated root.
	\end{enumerate}
\end{lem}
The half of the proof is given before, and the rest of the proof is easy.
\begin{thm}
	Let $F$ be a subfield of the field of the complex numbers. Let $T$ be a linear operator on a finite-dimensional vector space $V$ over $F$. Let $\mathcal{B}$ be an ordered  basis for $V$ and let $A$ be the matrix of $T$ in the basis $\mathcal{B}$. Then $T$ is semi-simple if and only if $A$ is similar over the field of complex numbers to a diagonal matrix. 
\end{thm}
\begin{thm}
	Let $F$ be a subfield of the field of the complex numbers. Let $T$ be a linear operator on a finite-dimensional vector space $V$ over $F$. There is a semi-simple operator $S$ on $V$ and a nilpotent operator $N$ on $V$ such that
	\begin{enumerate}
		\item [(\romannumeral1)]$T=S+N$
		\item [(\romannumeral2)]$SN=NS$
	\end{enumerate} 
	Furthermore, the semi-simple $S$ and nilpotent $N$ satisfying (\romannumeral1)(\romannumeral2) are unique, and each is a polynomial of $T$.
\end{thm}
\begin{proof}
	Let $p=p_1^{r_1}\cdots p_k^{r_k}$ and let $f=p_1\cdots p_k$. Let $r=\max\{r_1,\dots,r_k\}$, we have $f^r(T)=0$. Let construct a series of polynomials $g_0,g_1,g_2$ to let
	\[f(x-\sum_{i=0}^ng_if^i)\]
	can be divided by $f^{n+1}$. Let $g_0=0$, use the induction. Suppose we have chosen $g_0,\dots,g_{n-1}$. Let
	\[h=x-\sum_{i=0}^{n-1}g_if^i\]
	we have
	\[f(h-g_nf^n)=f(h)-f'(h)g_nf^n+f^{n+1}b\]
	where $b$ is a polynomial. Because $f(h)$ can be divided by $f^n$, we have
	\[f(h)=qf^n\]
	we just need to let $q-g_nf'(h)$ to be divided by $f$. Because $f$ is the product of distinct prime polynomials, we have
	\[af+ef'=1\]
	Let $g_n=qe(h)$, we have
	\[q-g_nf'(h)=qa(h)f(h)=q^2a(h)f^n\]
	Let $n=r-1$, we have
	\[f(x-\sum_{i=0}^{r-1}g_if^i)=0\]
	Let
	\[N=\sum_{i=0}^{r-1}g_if^i(T)\]
	Obviously $N$ can be divided by $f$, and thus $N^r=0$, which implies $N$ is nilpotent. Let $S=T-N$, we have $f(S)=f(T-N)=0$, which means $S$ is semi-simple. The uniqueness can be seen when we use the matrix of $T$ and calculate in the field of the complex numbers, and then you will find that the matrix of $S$ is diagonalizable and the matrix of $N$ is nilpotent, whose uniqueness has been proven in the previous section.
\end{proof}
In the next section, we will discuss the inner product, which endows vector spaces abundant beautiful properties.
\newpage
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Marisa}
\end{figure}
\thispagestyle{empty}
\newpage
\section{Inner Product Space}
\subsection{Inner Product}
In this section, we will write a vector as $\ket{v}$, and write a functional as $\bra{v}$. The reason will be explained after.
\begin{dde}
	Let $V$ be a vector space over $F=\mathbb{R}$ or $F=\mathbb{C}$. An \textbf{inner product} on $V$ is a function $\braket{ | }:V\times V\rightarrow F$ with the following properties for all $\ket{u},\ket{v},\ket{w}\in V$ and $c\in F$:
	\begin{enumerate}
		\item [(a)]$\braket{u+v|w}=\braket{u|w}+\braket{v|w}$
		\item [(b)]$\braket{u|cv}=c\braket{u|v}$
		\item [(c)]$\braket{u|v}=\overline{\braket{v|u}}$
		\item [(d)]$\braket{u|u}>0$ if $\ket{u}\neq0$
	\end{enumerate}
	You may notice that we also have $\braket{cu|v}=\bar{c}\braket{u|v}$ and $\braket{u|v+w}=\braket{u|v}+\braket{u|w}$.
\end{dde}
If $V=F^n$, we often define the \textbf{standard inner product}:
\[\braket{x|y}=\sum_{i=1}^{n}\overline{x_i}y_i\]
\indent Let's introduce the \textbf{conjugate transpose} matrix $A^\dagger$ of a matrix $A$, where $A^\dagger_{ij}=\overline{A_{ji}}$. Then we can define an inner product on $F^{n\times n}$:
\[\braket{A|B}=tr(A^\dagger B)=tr(BA^\dagger)\]
\indent Let $V,W$ be vector spaces over the field $F$ and let $\braket{}$ be an inner product on $W$. If $T$ is a non-singular linear transformation from $V$ to $W$, then we can define an inner product on $V$ by
\[\braket{u|v}_T=\braket{Tu|Tv}\]
Then any vector space $V$ with dimension $n$ can define an inner product under an ordered basis, which is just
\[\braket{u|v}=\sum_{i=1}^n\overline{x_i}y_i\]
Obviously, this inner product depends the basis.\\
\indent We call $V$ an \textbf{inner product space} if $V$ is a vector space together with an inner product.
\begin{dde}
	If $V$ is an inner product space, the \textbf{norm} of $\ket{v}\in V$ is 
	\[\Vert v \Vert=\sqrt{\braket{v|v}}\]
\end{dde}
Then we have the following identities
\[\Vert u\pm v\Vert^2=\Vert u\Vert^2\pm2Re(\braket{u|v})+\Vert v\Vert^2\]
and
\[\braket{u|v}=\frac{1}{4}(\Vert u+v\Vert^2-\Vert u-v\Vert^2)\text{ if }F=\mathbb{R}\]
\[\braket{u|v}=\frac{1}{4}(\Vert u+v\Vert^2-\Vert u-v\Vert^2-i(\Vert u+iv\Vert^2-\Vert u-iv\Vert^2))\text{ if }F=\mathbb{C}\]
The two identities above are called the \textbf{polarization identities}.
\begin{thm}
	If $V$ is an inner product space, we have
	\begin{enumerate}
		\item $\Vert cv\Vert=|c|\Vert v\Vert$
		\item $\Vert v\Vert>0$ for $\ket{v}\neq0$
		\item $|\braket{u|v}|\le\Vert u\Vert\Vert v\Vert$
		\item $\Vert u+v\Vert\le \Vert u\Vert+\Vert v\Vert$
	\end{enumerate}
\end{thm}
\begin{proof}
	We just prove the third property. If $\ket{u}=0$, then it is correct automatically. If $\ket{u}\neq0$, let
	\[\ket{w}=\ket{v}-\frac{\braket{u|v}}{\braket{u|u}}\ket{u}\]
	then
	\[0\le\braket{w|w}=\braket{v|v}-\frac{|\braket{u|v}|^2}{\braket{u|u}}\]
	thus
	\[|\braket{u|v}|\le\Vert u\Vert\Vert v\Vert\]
	This is called the \textbf{Cauchy-Schwarz inequality}.
\end{proof}
Before we go further, let's point out two useful lemma.
\begin{lem}
	Let $V$ be an inner product space. If $\braket{u|x}=\braket{v|x}$ for all $\ket{x}\in V$, then $\ket{u}=\ket{v}$.
\end{lem}
\begin{lem}
	Let $V$ be an inner product space and let $T$ be a linear operator on $V$.
	\begin{enumerate}
		\item 
		\[\braket{u|Tv}=0,\forall \ket{u},\ket{v}\in V\Rightarrow T=0\]
		\item If $V$ is a complex inner product space, then
		\[\braket{v|Tv}=0,\forall \ket{v}\in V\Rightarrow T=0\]
	\end{enumerate}
\end{lem}
\begin{proof}
	The first one is obviously right. Let's prove the second one. We have
	\[0=\braket{au+v|T(au+v)}=\overline{a}\braket{u|Tv}+a\braket{v|Tu}\]
	Let $a=1,i$, we have
	\[\braket{u|Tv}=0,\forall \ket{u},\ket{v}\in V\Rightarrow T=0\]
\end{proof}
Let's first study the construction of subspaces of an inner product space.
\begin{dde}
	Let $V$ be an inner product space.\\
	\begin{enumerate}
		\item Let $\ket{u},\ket{v}$ be vectors in $V$, then we call $\ket{u}$ is \textbf{orthogonal} to $\ket{v}$, written $u\bot v$, if $\braket{u|v}=0$.
		\item If $S$ is a subset of $V$, we call $S$ an \textbf{orthogonal set} if
		 \[\braket{u|v}=0,\forall \ket{u},\ket{v}\in S\]
		\item An \textbf{orthonormal set} is an orthogonal set S with $\Vert v\Vert=1,\forall \ket{v}\in S$.
		\item Two subsets$S,U$ are \textbf{orthogonal}, written $S\bot U$, if 
		\[\braket{u|v}=0,\forall \ket{u}\in S,\ket{v}\in U\]
		\item The \textbf{orthogonal complement} of a subset $S$ is the set
		\[S^\bot=\{\ket{v}\in V|\ket{v}\bot S\}\] 
	\end{enumerate}
\end{dde}
\begin{thm}
	Let $V$ be an inner product space.
	\begin{enumerate}
		\item The orthogonal complement $S^\bot$ of any subset $S$ is a subspace
		\item For any subspace $S$ of $V$,
		\[S\cap S^\bot=\{0\}\] 
	\end{enumerate}
\end{thm}
\begin{dde}
	Let $V$ be an inner space and let $V_1,V_2$ be the subspaces of $V$. We call the sum of $V_1,V_2$ the \textbf{orthogonal direct sum} if 
	\[V_1+V_2=V_1\oplus V_2,V_1\bot V_2\]
	and we write
	\[V_1+V_2=V_1\odot V_2\]
	Generally, we call the sum of $V_1,V_2,\dots,V_n$ the \textbf{orthogonal direct sum}, written
	\[V_1+V_2+\cdots+V_n=V_1\odot V_2\cdots\odot V_n\]
	if 
	\[V_1+V_2+\cdots+V_n=V_1\oplus V_2\cdots\oplus V_n,V_i\bot V_j,i\neq j\]
\end{dde}
\begin{thm}
	Any orthogonal set of nonzero vectors in $V$ is linearly independent.
\end{thm}
\begin{proof}
	Let $S$ be an orthogonal set of $V$, we have
	\[\lambda_1\ket{v_1}+\cdots+\lambda_k\ket{v_k}=0,\ket{v_i}\in S\]
	and
	\[\sum_{i=1}^k\lambda_i\braket{v_j|v_i}=\lambda_j\braket{v_j|v_j}=0\Rightarrow\lambda_j=0\]
	Hence, $S$ is linearly independent.
\end{proof}
\begin{thm}[Gram-Schmidt Orthogonalization Process]
	Let $V$ be an inner space and let $\ket{v_1},\dots,\ket{v_n}$ be any independent vectors in $V$. Then one can construct orthogonal vectors $\ket{u_1},\dots,\ket{u_n}$ in $V$ such that for $k=1,2,\dots,n$ the set
	\[\{\ket{u_1},\dots,\ket{u_k}\}\]
	is a basis for the subspace spanned by $\ket{v_1},\dots,\ket{v_k}$.
\end{thm}
\begin{proof}
	Let's use the induction. First let $\ket{u_1}=\ket{v_1}$. Suppose $\ket{u_1},\dots,\ket{u_m}$ have been chosen so that for $k=1,2,\dots,m$ the set
	\[\{\ket{u_1},\dots,\ket{u_k}\}\]
	is an orthogonal basis for the subspace spanned by $\ket{v_1},\dots,\ket{v_k}$.\\
	To construct the next vector, let
	\[\ket{u_{m+1}}=\ket{v_{m+1}}-\sum_{i=1}^m\frac{\braket{u_i|v_{m+1}}}{\braket{u_i|u_i}}\ket{u_i}\]
	Obviously $\braket{u_{m+1}|u_i}=0,1\le i\le m$ and $\ket{u_{m+1}}$ is not zero because the linearly independence. Thus $\ket{u_1},\dots,\ket{u_{m+1}}$ is a basis for the subspace spanned by $\ket{v_1},\dots,\ket{v_{m+1}}$.\\
	You may also notice that if $\ket{v_1},\dots,\ket{v_n}$ are linearly dependent, then we must have $\ket{v_j}$ which is a linear combination of $\ket{v_1},\dots,\ket{v_{j-1}}$, and therefore we have $\ket{u_j}=0$. If you arrange the vectors to let the first $r$ vectors be the maximal linearly independent system, then we know that we will stop our orthogonalization process at $\ket{u_{k+1}}$. 
\end{proof}
\begin{coro}
	Every finite-dimensional vector spaces has an orthonormal basis.
\end{coro}
Once we have found the orthonormal basis, then the calculation of the coordinate and inner product will be simplified if you notice that
\[x_i=\braket{u_i|x},\braket{x|y}=\sum_{i=1}^n\overline{x_i}y_i\]
\begin{lem}
	Let $W$ be a subspace of an inner product space $V$. $\ket{w_0}\in W$ is called the \textbf{best approximation} to a vector $\ket{v}\in V$ if we have
	\[\Vert v-w_0\Vert\le\Vert v-w\Vert,\forall \ket{w}\in W\]
	If the best approximation to $\ket{v}$ from within $W$ exists, then we have
	\begin{enumerate}
		\item $\ket{w_0}$ is a best approximation to $\ket{v}$ if and only if $\ket{v-w_0}\bot W$
		\item $\ket{w_0}$ is unique
	\end{enumerate}
	We also call $\ket{w_0}$ the \textbf{orthogonal projection} of $\ket{v}$ on $W$.
\end{lem}
\begin{proof}
	Let $\ket{w}$ be any vector in $W$, we have
	\[\Vert v-w\Vert^2=\Vert v-w_0\Vert^2+\Vert w_0-w\Vert^2+2Re(\braket{v-w_0|w_0-w})\]
	If $\ket{v-w_0}\bot W$, we have
	\[\Vert v-w\Vert^2=\Vert v-w_0\Vert^2+\Vert w_0-w\Vert^2>\Vert v-w_0\Vert\text{ if }\ket{w}\neq\ket{w_0}\]
	Conversely, suppose $\Vert v-w_0\Vert\le\Vert v-w\Vert$, we have
	\[\Vert w_0-w\Vert^2+2Re(\braket{v-w_0|w_0-w})\ge0,\forall\ket{w}\in W\]
	Notice that $w_0-w$ can be any vector in $W$, then we have
	\[\Vert s\Vert^2+2Re(\braket{v-w_0|s})\ge0,\forall\ket{s}\in W\]
	Suppose $\ket{w}\neq\ket{w_0}$, let
	\[\ket{s}=-\frac{\braket{w_0-w|v-w_0}}{\Vert w_0-w\Vert^2}(\ket{w_0}-\ket{w})\]
	and we have
	\[-\frac{|\braket{v-w_0|w_0-w}|^2}{\Vert w_0-w\Vert^2}\ge0\]
	This implies that $\ket{v-w_0}\bot W$. The uniqueness is transparent.
\end{proof}
\begin{thm}
	Let $W$ be a subspace of an inner product space $V$ with an orthonormal basis $\{\ket{u_1},\dots,\ket{u_m}\}$. Let $\ket{v}$ be a vector in $V$. The \textbf{Fourier expansion} with respect to $W$ of $\ket{v}$ is
	\[\ket{\hat{v}}=\sum_{i=1}^m\braket{u_i|v}\ket{u_i}\]
	where $\braket{u_i|v}$ is called the \textbf{Fourier coefficient} of $\ket{v}$ with respect to $W$. The vector $\ket{\hat{v}}$ can be characterized as follows:
	\begin{enumerate}
		\item $\ket{\hat{v}}$ is the unique vector $\ket{w}\in W$ for which $\ket{v-w}\bot W$.
		\item $\ket{\hat{v}}$ is the best approximation to $\ket{v}$ from within $W$.
		\item The \textbf{Bessel inequality} holds for all $\ket{v}\in V$
		\[\Vert\hat{v}\Vert\le\Vert v\Vert\]
	\end{enumerate}
\end{thm}
\begin{coro}
	Let $\ket{Ev}=\ket{\hat{v}}$, then $E$ is a projection on $V$. We call $E$ the \textbf{orthogonal projection} of $V$ on $W$.
\end{coro}
\begin{coro}
	The mapping $\ket{v}\rightarrow\ket{v-Ev}$ is an orthogonal projection of $V$ on $W^\bot$.
\end{coro}
\begin{proof}
	Obviously we have $\ket{v-Ev}\in W^\bot$, then we have
	\[\Vert v-s\Vert^2=\Vert Ev\Vert^2+\Vert v-Ev-s\Vert^2\ge \Vert v-(v-Ev)\Vert^2,\forall \ket{s}\in W^\bot\]
\end{proof}
\begin{thm}
	If $W$ is a finite-dimensional subspace of an inner product space $V$, then
	\[V=W\odot W^\bot\]
\end{thm}
Actually, $W$ can not be finite-dimensional, as long as there exists the orthogonal projection of every $\ket{v}\in V$ on $W$.\\
\indent Before we turn to study the properties for linear transformations on an inner product space, let's study linear functionals first.
\begin{thm}[The Riesz Representation Theorem]
	Let $V$ be a finite-dimensional inner product space, and let $f$ be a linear functional on $V$. Then there exists a unique vector $\ket{u}\in V$ such that
	\[f\ket{v}=\braket{u|v},\forall \ket{v}\in V\]
	We call $\ket{u}$ the \textbf{Riesz vector} for $f$ and denote $f$ by $\bra{u}$.
\end{thm}
\begin{proof}
	Let $\{v_1,\dots,v_n\}$ be an orthogonal basis for $V$. Let
	\[\ket{u}=\sum_{i=1}^n\overline{f\ket{v_i}}\ket{v_i}\]
	Obviously we have
	\[\braket{u|v}=\sum_{i=1}^nf\ket{v_i}\braket{v_i|v}=f\ket{v}\]
	If $\ket{w}$ is another vector satisfy the property, then $\braket{w|v}=\braket{u|v},\forall \ket{v}\in V$, which implies $\ket{w}=\ket{u}$.
\end{proof}
You may notice that $\ket{u}$ is just a vector in the orthogonal complement of the null space of $f$. Let $W$ be the null space of $f$, we know that the dimension of $W$ is $n-1$, then the orthogonal complement $W^\bot$ has the dimension $1$. Let $P$ be the orthogonal projection of $V$ on $W^\bot$, we have
\[f\ket{v}=f\ket{Pv+v-Pv}=f\ket{Pv}\]
Let $\ket{w}$ be a nonzero vector in $W^\bot$, then we have
\[f\ket{v}=f(\frac{\braket{w|v}}{\Vert w\Vert^2}\ket{w})=\braket{\frac{\overline{f\ket{w}}}{\Vert w\Vert^2}w|v}\]
which implies $\ket{u}=\dfrac{\overline{f\ket{w}}}{\Vert w\Vert^2}\ket{w}$.
\subsection{Linear Operators on Inner Product Space}
For convenience, the field $F$ is either $\mathbb{R}$ or $\mathbb{C}$.
\begin{thm}
	Let $T$ be a linear operator on a finite-dimensional inner product space $V$. Then there exists a unique linear operator $T^\dagger$ such that
	\[\braket{Tu|v}=\braket{u|T^\dagger v},\forall\ket{u},\ket{v}\in V\]
\end{thm}
\begin{proof}
	Fix the vector $\ket{v}$, we notice that $\ket{u}\rightarrow\braket{v|Tu}$ is just a linear functional on $V$, then there exists a unique vector $\ket{v'}$ such that $\braket{v|Tu}=\braket{v'|u}$ for every $\ket{u}$ in $V$. Let $T^\dagger$ denote the mapping $\ket{v}\rightarrow \ket{v'}$:
	\[\ket{v'}=\ket{T^\dagger v}\]
	Obviously, $T^\dagger$ is a linear operator, because we have
	\begin{align*}
		\braket{u|T^\dagger(cv+w)}&=\braket{T(cv+w)|u}\\
		&=c\braket{Tv|u}+\braket{Tw|u}\\
		&=c\braket{u|T^\dagger v}+\braket{u|T^\dagger w}\\
		&=\braket{u|cT^\dagger v+T^\dagger w}
	\end{align*}
	for all $\ket{u}\in V$, which implies $T^\dagger(cv+w)=cT^\dagger v+T^\dagger w$ and $T^\dagger$ is linear.\\
	The uniqueness is clear because we have $\braket{u|T^\dagger v}=\braket{u|Uv}$ for all $\ket{u},\ket{v}$ in $V$, which implies $T^\dagger=U$. 
\end{proof}
\begin{thm}
	Let $T$ be a linear operator on a finite-dimensional inner product space $V$ with an orthonormal basis $\{\ket{v_1},\dots,\ket{v_n}\}$. Let $A$ be the matrix of $T$ under the basis. Then
	\[A_{ij}=\braket{v_i|Tv_j}\]
\end{thm}
\begin{coro}
	Let $T$ be a linear operator on a finite-dimensional inner product space $V$. In any orthonormal basis for $V$, the matrix of $T^\dagger$ is the conjugate transpose of the matrix of $T$.
\end{coro}
\begin{proof}
	Obviously we have
	\[B_{ij}=\braket{v_i|T^\dagger v_j}=\braket{Tv_i|v_j}=\overline{A_{ji}}\]
\end{proof}
You may notice that the orthogonal projection has the properties $E=E^\dagger$, because
\[\braket{Eu|v}=\braket{Eu|(I-E)v}+\braket{Eu|Ev}=\braket{Eu+(I-E)u|Ev}=\braket{u|Ev}\]
\indent Actually, if $E$ is a projection that $E=E^\dagger$, then we have
\[\braket{Eu|v}=\braket{E^2u|v}=\braket{Eu|Ev}\]
and
\[\braket{Eu|v}=\braket{u|Ev}=\braket{E+(I-E)u|Ev}=\braket{Eu|Ev}+\braket{(I-E)u|v}\]
which implies
\[\braket{(I-E)u|v}=0,\forall\ket{u},\ket{v}\in V\]
this just means $E$ is an orthogonal projection. 
\begin{dde}
	Let $T$ be a linear operator on an inner product space $V$. Then we say $T$ \textbf{has an adjoint on} $V$ if there exists a linear operator $T^\dagger$ such that $\braket{Tu|v}=\braket{u|T^\dagger v}$ for all $\ket{u},\ket{v}$ in $V$. We call $T^\dagger$ the \textbf{adjoint} of $T$. 
\end{dde}
\begin{thm}
	Let $T,U$ be linear operators on a finite-dimensional inner product space $V$, then we have
	\begin{enumerate}
		\item $(T+U)^\dagger=T^\dagger+U^\dagger$
		\item $(TU)^\dagger=U^\dagger T^\dagger$
		\item $(T^\dagger)^\dagger=T$
		\item $(cT)^\dagger=\overline{c}T^\dagger$
	\end{enumerate}
\end{thm}
You may notice that we can always write a linear operator $T$ as
\[T=U_1+iU_2\]
where
\[U_1=\frac{T+T^\dagger}{2},U_2=\frac{T-T^\dagger}{2i}\]
which satisfy
\[U_1=U_1^\dagger,U_2=U_2^\dagger\]
\indent A linear operator $T$ such that $T=T^\dagger$ is called \textbf{self-adjoint} (or \textbf{Hermitian}). Numerous properties of Hermitian operators will be discussed soon.
\begin{dde}
	Let $V,W$ be inner product spaces and let $T\in\mathcal{L}(V,W)$.
	\begin{enumerate}
		\item $T$ is \textbf{isometry} if $\braket{Tu|Tv}=\braket{u|v},\forall\ket{u},\ket{v}\in V$
		\item A bijective isometry is called an \textbf{isometric isomorphism}. When $T:V\rightarrow W$ is an isometric isomorphism, we say that $V$ and $W$ are \textbf{isometrically isomorphic}
	\end{enumerate}
\end{dde}
\begin{thm}
	Let $V,W$ be finite-dimensional inner product spaces and let $T\in\mathcal{L}(V,W)$. If $T$ transforms some orthonormal basis for $V$ into an orthonormal basis for $W$, then $T$ is an isometric isomorphism.
\end{thm}
\begin{coro}
	Let $V,W$ be finite-dimensional inner product spaces, then $T$ and $W$ are isometrically isomorphic if and only if they have the same dimension.
\end{coro}
\begin{thm}
	A linear transformation $T\in\mathcal{L}(V,W)$ is an isometric isomorphism if and only if 
	\[\Vert Tv\Vert=\Vert v\Vert,\forall\ket{v}\in V\]
\end{thm}
\begin{proof}
	Use the polarization identities.
\end{proof}
\begin{dde}
	A \textbf{unitary operator} on an inner product space is an isometric isomorphism of the space onto itself.
\end{dde}
\begin{thm}
	Let $U$ be a linear operator on an inner product space $V$. Then $U$ is unitary if and only if the adjoint $U^\dagger$ of $U$ exists and $U^\dagger U=UU^\dagger=I$.
\end{thm}
\begin{proof}
	If $U$ is unitary, then $U$ is invertible and we have
	\[\braket{Ua|b}=\braket{Ua|U^{-1}Ub}=\braket{a|U^{-1}b},\forall\ket{a},\ket{b}\in V\]
	Hence $U^\dagger=U^{-1}$. If there exits $U^\dagger$ and satisfies $U^\dagger U=UU^\dagger=I$, then $U^\dagger=U^{-1}$ and we have
	\[\braket{Ua|Ub}=\braket{a|U^\dagger Ub}=\braket{a|b},\forall\ket{a},\ket{b}\in V\]
\end{proof}
\begin{dde}
	A complex $n\times n$ matrix $A$ is called \textbf{unitary} if $A^\dagger A=I$.
\end{dde}
\begin{thm}
	Let $U$ be a linear operator on a finite-dimensional inner product space $V$. Then $U$ is unitary if and only if the matrix of $U$ under some orthonormal basis for $V$ is unitary.
\end{thm}
You may notice that we have
\[\sum_{k=1}^n\overline{A_{ki}}A_{kj}=\delta_{ij},\sum_{k=1}^nA_{ik}\overline{A_{jk}}=\delta_{ij}\]
which means that the row vectors and column vectors of $A$ are orthogonal to each other. This implies that $A$ is unitary if and only if the rows and columns of $A$ are orthonormal sets.
\begin{dde}
	A real or complex $n\times n$ matrix $A$ is called \textbf{orthogonal} if $A^TA=I$.
\end{dde}
You may notice that if $A$ is unitary, then we have
\[(A^{-1})^{-1}=A=(A^\dagger)^{-1}=(A^{-1})^\dagger\]
and
\[(AB)^{-1}=B^{-1}A^{-1}=B^\dagger A^\dagger=(AB)^\dagger\]
Thus the set $U(n)$ of all $n\times n$ unitary matrices is a group.
\begin{thm}
	Let $A\in F^{m\times n}$. There exists a matrix $Q\in F^{m\times n}$ with orthonormal columns (except some zero vectors) and an upper-triangular matrix $R\in F^{n\times n}$ with non-negative real entries on the diagonal for which
	\[A=QR\]
	Moreover, if $m=n$, then $Q$ is unitary. If $A$ is non-singular, then $R$ can be chosen to have positive entries on the diagonal, in which case the factors $Q,R$ are unique. The factorization is called the \textbf{QR factorization} of the matrix $A$. If $A$ is real, then $Q,R$ may be taken to be real. 
\end{thm}
\begin{proof}
	If we write $A$ as $(\ket{v_1},\ket{v_2},\dots,\ket{v_n})$, then use the Gram-Schmidt orthogonalization process, we can construct $\{\ket{u_i}\}$ such that
	\[\ket{v_i}=\ket{u_i}-\sum_{k=1}^{i-1}r_{ki}\ket{u_k}\]
	where
	\[r_{ki}=\begin{cases}
		0&\text{ if }\ket{u_k}=0\\
		\braket{u_k|v_i}&\text{ if }\ket{u_k}\neq0
	\end{cases}\]
	then we have
	\[A=Q\begin{bmatrix}
		1&r_{12}&r_{13}&\cdots&r_{in}\\
		0&1&r_{22}&\cdots&r_{2n}\\
		0&0&1&\cdots&r_{3n}\\
		\vdots&\vdots&\vdots&\ddots&\vdots\\
		0&0&0&\cdots&1
	\end{bmatrix}\]
	where the columns of $Q$ are orthogonal to each other. Divide each column by the norm of $\ket{u_i}$ and if $\ket{u_i}=0$ then divide $1$, we have
	\[A=Q\begin{bmatrix}
		a_1&a_1r_{12}&a_1r_{13}&\cdots&a_1r_{in}\\
		0&a_2a_2&r_{22}&\cdots&a_2r_{2n}\\
		0&0&a_3&\cdots&a_3r_{3n}\\
		\vdots&\vdots&\vdots&\ddots&\vdots\\
		0&0&0&\cdots&a_n
	\end{bmatrix}\]
	Obviously, if $A$ is not invertible, there exist some $\ket{u_i}=0$, we just let $a_i=0$, then we can replace $\ket{u_i}$ by any vectors we want. If $m=n$, we can then let $Q$ be unitary.\\
	If $A$ is invertible, we have proven the existence of $Q,R$. As for the uniqueness, suppose we have $A=QR=Q'R'$, then we have
	\[QQ'^{-1}=R'R^{-1}\]
	because an upper-triangular matrix with positive entries on the diagonal is invertible, and the inverse is still an upper-triangular matrix. We know that $QQ'^{-1}$ is still unitary, which implies that the upper-triangular matrix $R'R^{-1}$ is also unitary, and therefore is the identity. Finally, if $A$ is real, then all the operations can only happen in the real field, and then $Q,R$ are real.
\end{proof}
The set of all the invertible $n\times n$ matrices is a group as well, and is called the \textbf{general linear group}, written $GL(n)$. This theorem then tells us that any matrix in $GL(n)$ can be written as a product of two matrices in $U(n)$ and $T^+(n)$ respectively, where $T^+(n)$ is the set of all upper-triangular matrix with positive entries on the diagonal and is also a group.\\
\indent Before we turn to the topic of normal operator, let's study the change of orthonormal basis first. Let $\{\ket{u_i}\},\{\ket{v_i}\}$ be two orthonormal bases for $V$. We write
\[\ket{v_j}=\sum_{i=1}^nP_{ij}\ket{u_i}\]
then $P$ is just a matrix of a linear operator transform an orthonormal basis to another, which is thus unitary. Then we have the familiar result
\[A'=P^{-1}AP\]
However, this time we have $P^{-1}=P^\dagger$, then
\[A'=P^\dagger AP\]
\begin{dde}
	Let $A$ and $B$ be complex $n\times n$ matrices. We say that $B$ is \textbf{unitarily equivalent to} $A$ if there exits an $n\times n$ unitary matrix $P$ such that $B=P^{-1}AP$. We say that $B$ is \textbf{orthogonally equivalent to} $A$ if there exits an $n\times n$ orthogonal matrix $P$ such that $B=P^{-1}AP$
\end{dde}
\begin{dde}
	A linear operator $T\in\mathcal{L}(V)$ is \textbf{unitarily diagonalizable} (when $V$ is complex) and \textbf{orthogonally diagonalizable} (when $V$ is real) if there is an ordered orthonormal basis $\{\ket{u_i}\}$ for $V$ for which the matrix of $T$ is diagonal.
\end{dde}
\begin{thm}
	Let $V$ be a finite-dimensional inner product space and let $T$ be a linear operator on $V$. The following are equivalent:
	\begin{enumerate}
		\item $T$ is unitarily(orthogonally) diagonalizable
		\item $V$ has an orthonormal basis that consists entirely of eigenvectors of $T$.
		\item $V$ has the form
		\[V=W_1\odot W_2\odot\cdots\odot W_k\]
		where $W_i$ is the null space of $T-\lambda_iI$
	\end{enumerate}
\end{thm}
An important thing is that if $T$ is unitarily\footnote{We won't write orthogonally anymore.} diagonalizable, then we have
\[TT^\dagger=T^\dagger T\]
Then we can define this kind of linear operators.
\begin{dde}
	A linear operator $T$ on a finite-dimensional inner product space is called \textbf{normal}, if it commutes with its adjoint.
\end{dde}
The matrix version can be defined similarly.
\begin{thm}
	Let $T$ be a linear operator on a finite-dimensional inner product space $V$. Suppose $W$ is a $T$-invariant subspace of $V$, then the orthogonal complement of $W$ is invariant under $T^\dagger$.
\end{thm}
\begin{proof}
	We have
	\[\braket{Tu|v}=\braket{u|T^\dagger v}=0,\forall\ket{u}\in W,\ket{v}\in W^\bot\]
	which implies that $T^\dagger\ket{v}$ is a vector in $W^\bot$.
\end{proof}
\begin{coro}
	If $T$ is a linear operator on a finite-dimensional inner product space $V$ and suppose $W,W^\bot$ are $T$-invariant subspaces of $V$. Then $W,W^\bot$ are also invariant under $T^\dagger$.
\end{coro}
\begin{thm}\label{ThepropertyofNormalOperator}
	Let $T$ be a normal operator on a finite-dimensional inner product space $V$.
	\begin{enumerate}
		\item The following are also normal:
		\begin{enumerate}
			\item $T|_W$, when $T$ reduces $W,W^\bot$, where $T|_W$ is the operator induced by $T$ on $W$
			\item $T^\dagger$
			\item $T^{-1}$, if $T$ is invertible
			\item $p(T)$, for any $p\in F[x]$
		\end{enumerate}
		\item For any $\ket{u},\ket{v}\in V$,
		\[\braket{Tu|Tv}=\braket{T^\dagger u|T^\dagger v}\]
		in particular,
		\[\Vert Tu\Vert=\Vert T^\dagger u\Vert\]
		and so,
		\[\ker T=\ker T^\dagger\footnotemark[11]\]
		\footnotetext[11]{Sometimes we denote the null space of a linear operator by $\ker$}
		\item For any integer $k\ge1$,
		\[\ker T^k=\ker T\]
		\item The minimal polynomial $p$ for $T$ is a product of distinct prime polynomials
		\item $T\ket{v}=\lambda\ket{v}\Leftrightarrow T^\dagger\ket{v}=\overline{\lambda}\ket{v}$
		\item Let $p,q$ be the annihilators of $W_1,W_2$. If $p,q$ are relatively prime, then $W_1\bot W_2$
		\item If $\lambda_1,\lambda_2$ are distinct eigenvalues of $T$, then $W_1\bot W_2$, where $W_i$ is the characteristic space of $\lambda_i$
	\end{enumerate}
\end{thm}
\begin{proof}
	(1) is easy to prove.\\
	(2): we have
	\[\braket{Tu|Tv}=\braket{u|T^\dagger Tv}=\braket{u|TT^\dagger v}=\braket{T^\dagger u|T^\dagger v}\]
	(3): Let $U=T^\dagger T$, we have
	\[U^\dagger=(T^\dagger T)^\dagger=T^\dagger T=U\]
	which implies that $U$ is self-adjoint. If $U^k\ket{v}=0$, we have
	\[0=\braket{U^kv|U^{k-2}v}=\braket{U^{k-1}v|U^{k-1}v}\]
	which means $U^{k-1}\ket{v}=0$. Continue this process, we have $U\ket{v}=0$. Now suppose $T^k\ket{v}=0$, we have
	\[U^k\ket{v}=(T^\dagger T)^k\ket{v}=(T^{\dagger})^kT^k\ket{v}=0\]
	then $U\ket{v}=0$, hence
	\[0=\braket{Uv|v}=\braket{T^\dagger Tv|v}=\braket{Tv|Tv}\]
	and then $T\ket{v}=0$. Therefore, $\ker T^k=\ker T$.\\
	(4): If $p=p_1^rq$, then we have
	\[p_1^r(T)[q(T)\ket{v}]=0,\forall\ket{v}\in V\]
	because $p_1(T)$ is also a normal operaor, use (3) we know that
	\[p_1(T)[q(T)\ket{v}]=0,\forall \ket{v}\in V\]
	then according to the definition of $p$ we have $r=1$.\\
	(5): \[\ker(T-\lambda I)=\ker(T-\lambda I)^\dagger=\ker(T^\dagger-\overline{\lambda}I)\]
	(6): We have $ap+bq=1$ for some $a,b\in F[x]$. Because $ap(T),bq(T)$ are also normal operators, we have
	\[\braket{u|v}=\braket{(ap(T)+bq(T))u|v}=\braket{bq(T)u|v}=\braket{u|(bq(T))^\dagger v}=0\]
	for all $\ket{u}\in W_1,\ket{v}\in W_2$. Then $W_1\bot W_2$.\\
	(7): Let $\ket{u}\in W_1,\ket{v}\in W_2$, we have
	\[\lambda_1\braket{u|v}=\braket{T^\dagger u|v}=\braket{u|Tv}=\lambda_2\braket{u|v}\]
	which implies $\braket{u|v}=0$ for all $\ket{u}\in W_1,\ket{v}\in W_2$.
\end{proof}
\begin{coro}
	If $T$ is a normal operator on a finite-dimensional inner product space $V$ and suppose $W$ is a $T$-invariant subspace of $V$. Then $W$ is also invariant under $T^\dagger$, so as $W^\bot$.
\end{coro}
\begin{proof}
	Let $\ket{u_i}$ be an orthonormal basis for $W$ and extend it to a basis for $V$. Then we know the matrix of $T$ under this basis has the form
	\[\begin{bmatrix}
		A&B\\
		0&C
	\end{bmatrix}\]
	And the matrix of $T^\dagger$ has the form
	\[\begin{bmatrix}
		A^\dagger&0\\
		B^\dagger&C^\dagger
	\end{bmatrix}\]
	Because $\sum\limits_{i=1}^m\Vert Tu_i\Vert$ is just the sum of the square of the absolute value of each entries of the first $m$ columns, so as $\sum\limits_{i=1}^m\Vert T^\dagger u_i\Vert$, and we have
	\[\Vert Tu\Vert=\Vert T^\dagger u\Vert\]
	then we know that $B$ must be zero.
\end{proof}
According to (4)(7), we now know that a normal operator is unitarily diagonalizable over the complex field.
\begin{thm}[Complex Spectral Theorem]
	Let $T$ be a linear operator on a finite-dimensional inner product space $V$. The followings are equivalent:
	\begin{enumerate}
		\item $T$ is normal
		\item $T$ is unitarily diagonalizable
		\item $T$ has an \textbf{orthogonal spectral resolution}
		\[T=\lambda_1E_1+\cdots+\lambda_kE_k\]
		where $E_1+\cdots+E_k=I$ and $E_i$ is orthogonal projection.
	\end{enumerate}
\end{thm}
Before we discuss the real version of the spectral theorem, we will first introduce the complexification of a linear operator.\\
\indent Recall that we have said long ago that the minimal polynomial do not change in the complex field and real field. However, we have only proven the matrix version, which is easily to be generalized from the real field to complex field. Hence, we need the concept of the complexification of a vector space.\\
\begin{dde}
	A \textbf{complexification} of a real vector space $V$, denoted $V_C$, is just $V\times V$ equipped with the addition and scalar multiplication defined following:
	\begin{enumerate}
		\item Addition:
		\[(\ket{u_1},\ket{v_1})+(\ket{u_2},\ket{v_2})=\ket{u_1+u_2},\ket{v_1+v_2}\]
		\item Scalar multiplication:
		\[(a+ib)(\ket{u},\ket{v})=(\ket{au-bv},\ket{bu+av})\]
		where $a,b\in \mathbb{R}$
	\end{enumerate}
	Therefore, $V_C$ is a vector space over $\mathbb{C}$.
\end{dde}
We will denote $(\ket{u},\ket{v})$ as $\ket{u+iv}$. A vector $\ket{u}$ in $V$ is just $(\ket{v},0)$, written $\ket{v}$ for convenience.
\begin{thm}
	The basis for $V$ is also a basis for $V_C$, and therefore $\dim V=\dim V_C$.
\end{thm}
\begin{proof}
	Let $\{\ket{v_i}\}$ be a basis for $V$. First we know that $\{\ket{v_i}\}$ obviously span $V_C$. We have
	\[\lambda_1\ket{v_1}+\cdots+\lambda_n\ket{v_n}=0\]
	The real part and imaginary part of the left side of the equation is zero respectively, according to the linearly independence of $\{\ket{v_i}\}$ in $V$ we know that $\lambda_i=0$, and thus $\{\ket{v_i}\}$ are also linearly independent in $V_C$.
\end{proof}
\begin{dde}
	Let $T$ be a linear operator on a real vector space $V$. The \textbf{complexification} of $T$, denoted $T_C$, is a linear operator on $V_C$ defined by
	\[T_C(\ket{u+iv})=T\ket{u}+iT\ket{v}\]
\end{dde}
\begin{dde}
	Let $V$ be a real inner product space and let $V_C$ be the complexification of $V$. Then we can define the inner product on $V_C$ by 
	\[\braket{u_1+iv_1|u_2+iv_2}=\braket{u_1|u_2}_V+\braket{v_1|v_2}_V+i\braket{u_1|v_2}_V-i\braket{v_1|u_2}_V\]
	We find that if $v_1=v_2=0$, we have the same result. So we won't add $V$ after.
\end{dde}
Apparently we have $T_C^n\ket{u+iv}=T^n\ket{u}+iT^n\ket{v}$. Then we have the theorem we want.
\begin{thm}
	Let $T$ be a linear operator on a real vector space $V$. Then the minimal polynomial for $T_C$ equals the minimal polynomial for $T$.
\end{thm}
\begin{proof}
	Let $p$ be the minimal polynomial for $T$, use the identity above, we have $p(T_C)=0$.\\
	Now suppose $q$ is the minimal polynomial for $T_C$. Denote the real part of the polynomial by $r$, then we have
	\[q(T)=0\rightarrow r(T)=0\]
	and we have $\deg q=\deg r\ge \deg p$. Thus $p=q$.
\end{proof}
You may notice that this theorem implies that the minimal polynomial for $T_C$ will have conjugate roots. Then we can prove the real version of the spectral theorem
\begin{thm}[Real Spectral Theorem]
	Let $T$ be a linear operator on a finite-dimensional real inner product space $v$, then $T$ is normal if and only if 
	\[V=V_1\odot\cdots\odot V_k\odot W_1\odot\cdots W_r\]
	where $V_1$ is the characteristic space of $\lambda_i$ and $W_i$ is the null space of $T^2+\alpha_iT+\beta_i$. We can find an orthonormal basis for $V$ in which the matrix of $T|_{W_i}$ has the form
	\[\begin{bmatrix}
		a_i&-b_i\\
		b_i&a_i
	\end{bmatrix}\]
\end{thm}
\begin{proof}
	The first part of the theorem can easily be proven by (4)(6) of Theorem \ref{ThepropertyofNormalOperator}. We just need to prove that the matrix of a normal operator $T$ on two-dimension vector space $V$ can be
	\[\begin{bmatrix}
		a&-b\\
		b&a
	\end{bmatrix}\]
	under an orthonormal basis. First we know that $T_C$ is also a normal operator on $V_C$, which can be justified simply. Then there exists an orthonormal basis for $V$ such that the matrix of $T_C$ under this basis is
	\[\begin{bmatrix}
		ce^{i\theta}&0\\
		0&ce^{-i\theta}
	\end{bmatrix}\]
	Suppose $\ket{e_1}=\ket{u+iv}$, then we have
	\[T\ket{u-iv}=ae^{-i\theta}\]
	which means $\ket{u-iv}$ is a eigenvalue of $ae^{-i\theta}$. We know that $\Vert u+iv\Vert^2=\Vert u\Vert^2+\Vert v\Vert^2$, therefore $\ket{u-iv}$ is just $\ket{e_2}$. Now we have
	\[\braket{e_1|e_2}=\braket{u|u}-\braket{v|v}-2i\braket{u|v}=0\]
	which implies $\Vert u\Vert=\Vert v\Vert,\braket{u|v}=0$. Normalizing $\ket{u},\ket{v}$ we have an orthonormal basis for $V$, and then
	\[T\ket{u}=\frac{T_C\ket{e_1}+T_C\ket{e_2}}{2}=c\cos\theta\ket{u}-c\sin\theta\ket{v}\]
	\[T\ket{v}=\frac{T_C\ket{e_1}-T_C\ket{e_2}}{2i}=c\sin\theta\ket{u}+c\cos\theta\ket{v}\]
	Therefore the matrix of $T$ under the orthonormal basis will have the form
	\[\begin{bmatrix}
		a&-b\\
		b&a
	\end{bmatrix}\]
	In fact we can calculate $a,b$ from the discussion above.
	\[a=-\frac{\alpha}{2},b=-\frac{\sqrt{4\beta-\alpha^2}}{2},c=\sqrt{\beta}\]
	If we have
	\[V=V_1\odot\cdots\odot V_k\odot W_1\odot\cdots W_r\]
	then we know $T$ is normal, because
	\[AA^\dagger=\begin{bmatrix}
		a&-b\\
		b&a
	\end{bmatrix}\begin{bmatrix}
	a&b\\
	-b&a
	\end{bmatrix}=(a^2+b^2)I=A^\dagger A\]
\end{proof}
Now we can delve into some special normal operator.
\begin{dde}
	Let $T$ be a linear operator on an inner product space $V$. The \textbf{quadratic form} associated with $T$ is a function $Q_T:V\rightarrow F$ defined by
	\[Q_T(\ket{v})=\braket{v|Tv}\]
\end{dde}
\begin{thm}
	Let $T,U$ be linear operators on a finite-dimensional inner product space $V$.
	\begin{enumerate}
		\item If $T,U$ are self-adjoint, then so are the following:
		\begin{enumerate}
			\item $T+U$
			\item $T^{-1}$, if $T$ is invertible
			\item $p(T)$, for any real polynomial $p\in\mathbb{R}[x]$
		\end{enumerate}
		\item A complex operator $T$ is Hermitian if and only if $Q_T(\ket{v})$ is real for all $\ket{v}\in V$
		\item If $T$ is a complex operator or a real symmetric operator, then
		\[T=0\Leftrightarrow Q_T=0\]
		\item All complex roots of the characteristic polynomial for a self-adjoint operator are real. Hence, the minimal polynomial for $T$ is a product of distinct linear factors over $\mathbb{R}$
	\end{enumerate}
\end{thm}
\begin{proof}
	(1) is simple to prove.\\
	(2): If $T$ is Hermitian, we have
	\[Q_T(\ket{v})=\braket{v|Tv}=\overline{\braket{Tv|v}}=\overline{\braket{v|Tv}}=\overline{Q_T(\Ket{v})}\]
	Conversely, if $\braket{Tv|v}=\braket{Tv|v}$, we have
	\[\braket{v|T^\dagger v}=\braket{Tv|v}=\braket{v|Tv}\]
	and so $T=T^\dagger$.\\
	(3): We need only prove $Q_T=0$ implies $T=0$ for $F=\mathbb{R}$.
	\begin{align*}
		0&=\braket{x+y|T(x+y)}\\
		&=\braket{x|Ty}+\braket{y|Tx}\\
		&=\braket{x|Ty}+\braket{Tx|y}\\
		&=\braket{x|Ty}+\braket{x|Ty}\\
		&=2\braket{x|Ty}
	\end{align*}
	and therefore $T=0$.\\
	(4): If $T$ is Hermitian($F=\mathbb{C}$) and $T\ket{v}=\lambda\ket{v}$, we have
	\[\lambda\ket{v}=T\ket{v}=T^\dagger\ket{v}=\overline{\lambda}\ket{v}\]
	and so $\lambda$ is real. If $F=\mathbb{R}$, then we consider the matrix of $T$ under an orthonormal basis and then it can be seen as a complex Hermitian matrix. Use the conclusion above, we know that the characteristic polynomial for $T$ has only real roots as well. 
\end{proof}
Then let's turn to unitary operators.
\begin{thm}
	If $T$ is a unitary operator on a finite-dimensional inner product space, then the absolute values of the eigenvalues are $1$. Therefore, we can find an orthonormal basis for $V$ in which the matrix of $T$ has the form
	\[\begin{bmatrix}
		1& & & & &\\
		 &\ddots& & & &\\
		 & &-1& & &\\
		 & & &\ddots& &\\
		 & & & &\begin{bmatrix}
		 	\cos\theta_1&-\sin\theta_1\\
		 	\sin\theta_1&\cos\theta_1
		 \end{bmatrix}&\\
		 & & & & &\ddots
	\end{bmatrix}\]
\end{thm}
\begin{proof}
	\[\braket{Tv|Tv}=\lambda\overline{\lambda}\braket{v|v}=\braket{v|v}\rightarrow|\lambda|=1\]
\end{proof}
The following defines a special type of unitary operator.
\begin{dde}
	For a nonzero vector $\ket{v}\in V$, the unique operator $H_v$ for which
	\[H_v\ket{v}=-\ket{v},H_v\ket{u}=\ket{u},\forall\ket{u}\in\{\ket{v}\}^\bot\]
	is called a \textbf{reflection} or a \textbf{Householder transformation}.
\end{dde}
Obviously the unique operator is 
\[H_v\ket{w}=\ket{w}-\frac{2\braket{v|w}}{\braket{v|v}}\ket{v}\]
\indent Extend $\ket{v}$ to an orthonormal basis for $V$, then the matrix of $H_v$ has the form
\[\begin{bmatrix}
	-1&&&\\
	&1&&\\
	&&\ddots&\\
	&&&1
\end{bmatrix}\]
Hence $H_v$ is both unitary and Hermitian,
\[H_v^\dagger=H_v^{-1}=H_v\]
\indent Moreover, $H_v\ket{u}=-\ket{u}$ if and only if $\ket{u}=c\ket{v}$, and we know that $H_v=H_{cv}$ for any nonzero vector $\ket{v}\in V$
\begin{thm}
	Let $\ket{v},\ket{w}\in V$ be distinct nonzero vector with equal length. Then $H_{v-w}$ is the unique reflection sending $\ket{v}$ to $\ket{w}$ and $\ket{w}$ to $\ket{v}$.
\end{thm}
\begin{proof}
	If $\Vert v\Vert=\Vert w\Vert$, then $\ket{v-w}\bot\ket{v+w}$, hence
	\[H_{v-w}\ket{v-w}=\ket{w-v}\]
	\[H_{v-w}\ket{v+w}=\ket{v+w}\]
	and therefore $H_{v-w}\ket{v}=\ket{w}$ and $H_{v-w}\ket{w}=\ket{v}$. If $H_x\ket{v}=\ket{w}$, because $H_x=H_x^{-1}$, we have $H_x\ket{w}=\ket{v}$, therefore 
	\[H_x\ket{v-w}=-\ket{v-w}\]
	and we have $H_x=H_{v-w}$.
\end{proof}
\begin{thm}
	Let $V$ be a finite-dimensional inner product space. The following are equivalent for a linear operator $T$ on $V$:
	\begin{enumerate}
		\item $T$ is unitary/orthogonal
		\item $T$ is a product of reflections
	\end{enumerate}
\end{thm}
\begin{proof}
	(2) to (1) is obvious. Suppose $T$ is unitary/orthogonal. Let $\{\ket{u_i}\}$ be an orthonormal basis for $V$, we have
	\[H_{Tu_1-u_1}\ket{Tu_1}=\ket{u_1}\]
	then let $T_1=H_{Tu_1-u_1}T$, we have $T_1\ket{u_1}=\ket{u_1}$. Suppose we have found reflections $H_{x_1},\dots,H_{x_{k-1}}$ for which $T_{k-1}=H_{x_{k-1}}\cdots H_{x_{1}}T$ that $T_{k-1}\ket{u_{i}}=\ket{u_{i}}$, for all $i<k$. Then
	\[H_{T_{k-1}u_k-u_k}\ket{T_{k-1}u_k}=\ket{u_k}\]
	and for $i<k$, we have
	\[\braket{T_{k-1}u_k-u_k|u_i}=\braket{H_{x_{k-1}}\cdots H_{x_{1}}Tu_k|u_i}=\braket{Tu_k|H_{x_{1}}\cdots H_{x_{k-1}}u_i}=\braket{Tu_k|Tu_i}=0\] 
	Hence let $\ket{x_k}=\ket{T_{k-1}u_k-u_k}$ and let $T_k=H_{x_k}\cdots H_{x_1}T$, we have $T_k\ket{u_i}=\ket{u_i}$, for all $i<k+1$. Continue this process until $k=n$, we then have
	\[H_{x_n}\cdots H_{x_1}T\ket{u_i}=\ket{u_i}\]
	for all $\ket{u_i}$, and therefore is an identity. Thus we have
	\[T=H_{x_1}\cdots H_{x_n}\]
\end{proof}
Before we study positive operators, let's study the function of operators first.
\begin{dde}
	Let $T$ be a normal operator on a finite-dimensional inner product space. Let $f$ be a function form $\{\lambda_i\}$ to $F$, then we can define the function of $T$ as
	\[f(T)=f(\lambda_1)E_1+\cdots+f(\lambda_k)E_k\]
	where $\lambda_i$ is the eigenvalue of $T$ and $E_i$ is the orthogonal projection.
\end{dde}
In fact, because this expression is just a finite sum, any function of $T$ is just a polynomial of $T$, as we can always find a polynomial satisfies $p(\lambda_i)=f(\lambda_i)$.
\begin{dde}
	A self-adjoint $T\in\mathcal{L}(V)$ is
	\begin{enumerate}
		\item \textbf{positive} if $Q_T(\ket{v})\ge0,\forall\ket{v}\in V$
		\item \textbf{positive definite} if $Q_T(\ket{v})>0,\forall\ket{v}\neq0$
	\end{enumerate}
\end{dde}
\begin{thm}
	A self-adjoint $T$ on a finite-dimensional inner product space is
	\begin{enumerate}
		\item positive if and only if all its eigenvalues are non-negative
		\item positive definite if and only if all its eigenvalues are positive
	\end{enumerate}
\end{thm}
\begin{proof}
	If $T$ is positive, then for $\ket{Tv}=\lambda\ket{v}$, we have
	\[Q_T(\ket{v})=\braket{v|Tv}=\lambda\braket{v|v}\ge0\rightarrow\lambda\ge0\]
	If $\lambda_i=0$, then
	\[Q_T(\ket{v})=\sum_{i=1}^k\lambda_i\braket{E_iv|E_iv}\ge0\]
	(2) is proved similarly.
\end{proof}
Then we can define the \textbf{positive square root} of a positive operator $T$:
\[\sqrt{T}=\sqrt{\lambda_1}E_1+\cdots+\sqrt{\lambda_k}E_k\]
It is clear that $(\sqrt{T})^2=T$, and it is not hard to see that $\sqrt{T}$ is the unique positive operator whose square is $T$. Hence, an operator $T$ is positive if and only if it has a positive square root.\\
\indent If $T$ is positive, then $\sqrt{T}$ is self-adjoint, and we have
\[(\sqrt{T})^\dagger\sqrt{T}=T\]
Conversely, if $T=U^\dagger U$ for some operator $U$, then
\[Q_T(\ket{v})=\braket{U^\dagger Uv|v}=\braket{Uv|Uv}\ge0\]
Hence, $T$ is positive if and only if $T=U^\dagger U$ for some operator $U$.
\begin{thm}
	If $T,U$ are positive operators on a finite-dimensional inner product space and $TU=UT$, then $TU$ is also positive.
\end{thm}
\begin{proof}
	As we know that a function of $T$ is a polynomial of $T$, $\sqrt{T},\sqrt{U}$ are polynomials of $T,U$ respectively, and therefore are commutative. Then
	\[(\sqrt{T}\sqrt{U})^2=(\sqrt{T})^2(\sqrt{U})^2=TU\]
	Since $\sqrt{T},\sqrt{U}$ are self-adjoint and commutative, then $\sqrt{T}\sqrt{U}$ is also self-adjoint, and so $TU$ is positive.
\end{proof}
Let's now prove the final decomposition of the basic part of this article.
\begin{thm}
	Let $T$ be a nonzero linear operator on a finite-dimensional inner product space $V$.
	\begin{enumerate}
		\item There exist a positive operator $R$ and a unitary operator $U$ for which $T=UR$. Moreover $R$ is unique and if $T$ is invertible, then $U$ is unique
		\item There exist a positive operator $R'$ and a unitary operator $U'$ for which $T=R'U'$. Moreover $R'$ is unique and if $T$ is invertible, then $U'$ is unique
	\end{enumerate}
\end{thm}
\begin{proof}
	First we know $T^\dagger T$ is a positive operator, and then we define
	\[R=\sqrt{T^\dagger T}\]
	and we have
	\[\Vert Rv\Vert^2=\braket{Rv|Rv}=\braket{R^2v|v}=\braket{Tv|Tv}=\Vert Tv\Vert^2\]
	Define $U$ on the range of $R$ by
	\[U(R\ket{v})=T\ket{v},\forall\ket{v}\in V\]
	This definition is legitimate, because we have $R\ket{x}=R\ket{y}\rightarrow T\ket{x}=T\ket{y}$.\\
	Moreover, we have
	\[\Vert URv\Vert=\Vert Tv\Vert=\Vert Rv\Vert\]
	which implies that $U$ is a isometry on the range of $R$. Thus, if $\{\ket{v_i}\}$ is an orthonormal basis for the range of $R$, then $\{\ket{Uv_i}\}$ is an orthonormal basis for the range of $T$. Extend both orthonormal bases to orthonormal bases for $V$ and extend the definition of $U$.\\
	Obviously $U$ is not unique if the range of $R\neq V$, because the way we extend the basis is not unique. If $T$ is invertible, then $R$ is invertible because now the range of $R$ is just $V$, and we have $U=TR^{-1}$, which is unique.\\
	(2) can be proven when we apply (1) to $T^\dagger$.
	\[T=(T^\dagger)^\dagger=(UR')^\dagger=R'U^{-1}=R'U'\]
	and we have $R'=TT^\dagger$. 
\end{proof}
\begin{thm}
	A unitary operator $U$ on a finite-dimensional complex inner product space $V$ has the form $U=e^{iH}$, where $H$ is an Hermitian operator on $V$ and is unique in some sense.
\end{thm}
\begin{proof}
	We know the spectral decomposition of $U$ is
	\[U=E^{i\theta_1}E_1+\cdots+e^{i\theta_k}E_k,\theta_i\in\mathbb{R}\]
	and we define
	\[H=\theta_1E_1+\cdots+\theta_kE_k\]
	which is obviously self-adjoint. We know that $\theta_i$ can be changed by $2\pi$, hence $H$ is unique only when we choose $-\pi<\theta_i\le\pi$.
\end{proof}
\begin{thm}[Polar Decomposition]
	Let $T$ be a nonzero linear operator on a finite-dimensional complex inner product space. Then there is a positive operator $R$ and a self-adjoint operator $H$ for which $T$ has the \textbf{polar decomposition}
	\[T=Re^{iH}\]
	Moreover, $R$ is unique and if $T$ is invertible, then $H$ is unique.
\end{thm}
\begin{thm}
	Let $T=Re^{iH}$ be a polar decomposition of a nonzero linear operator $T$. Then $T$ is normal if and only if $RH=HR$.
\end{thm}
\begin{proof}
	\[TT^\dagger=R^2\]
	\[T^\dagger T=e^{-iH}R^2e^{iH}\]
	Therefore $T$ is normal if and only if 
	\[R^2e^{iH}=R^2e^{iH}\]
	Because $R^2,e^{iH}$ are polynomials of $R,H$ respectively, this equals
	\[RH=HR\]
	where we use a simple conclusion that two normal operators are commutative if and only if some functions of them are commutative.
\end{proof}
\newpage
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{hiten6}
\end{figure}
\newpage
\section{Advanced Topics}
\end{document}





























